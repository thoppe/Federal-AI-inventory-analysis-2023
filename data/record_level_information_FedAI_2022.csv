Department,Agency,Office,Title,Summary,Lifecycle,Contact Name,Contact Email
Agriculture,USDA,APHIS,Predictive modeling of invasive pest species and category at the port of entry using machine learning algorithms,Macine learning algorithms are used to develop with inspection data and improve prediction ability of detecting invasive/quarantine significant pests at the port of entry.,Operation and Management,,
Agriculture,USDA,APHIS,Detection of pre-symptomatic HLB infected citrus,Identify pixels with HLB infection signature in multispectral and thermal imagery,Operation and Management,,
Agriculture,USDA,APHIS,High throughput phenotyping in citrus orchards,"Locate, count, and categorize citrus trees in an orchard to monitor orchard health",Operation and Management,,
Agriculture,USDA,APHIS,Detection of aquatic weeds,Identify and locate aquatic weeds,Operation and Management,,
Agriculture,USDA,APHIS,Automated Detection & Mapping of Host Plants from Ground Level Imagery,Generate maps of target trees from ground-level (streetview) imagery,Development and Acquisition,,
Agriculture,USDA,APHIS,Standardization of cut flower business names for message set data,"Natural language processing technique. Data are cleaned (e.g., remove punctuation) to facilitate matching. Cosine similarity is calculated, similar terms are matched, and the results are output.",Implementation,,
Agriculture,USDA,APHIS,"Approximate string or fuzzy matching, used to automate matching similar, but not identical, text in administrative documents","The algorithm computes a string similarity metric which can be used to classify similar strings into a single category, reducing information duplication and onerous, manual error-checking",Operation and Management,,
Agriculture,USDA,APHIS,Training machine learning models to automatically read file attachments and save information into a more convenient Excel format. ,"Artificial intelligence used to automate document processing and information extraction. Program managers often need information from specific form fields that are sent as PDF email attachments. Many emailed documents are received each day, making manually opening each attachment and copying the needed information too time-consuming. ",Operation and Management,,
Agriculture,USDA,APHIS,Artificial Intelligence for correlative statistical analysis,"AI-type statistical techniques are used to model predictive relationships between variables. We routinely use modeling approaches such as random forest, artificial neural networks, k-nearest neighbor clustering, and support vector machines, for statistical prediction. ",Operation and Management,,
Agriculture,USDA,ARS,4% Repair Dashboard,"The model reviews the descriptions of expenses tagged to repairs and maintenance and classifies expenses as ""repair"" or ""not repair"" based on keywords in context.",Operation and Management,,
Agriculture,USDA,ARS,ARS Project Mapping,NLP of research project plans including term analysis and clustering enables national program leaders to work with an interactive dashboard to find synergies and patterns within and across the various ARS research program portfolios.,Operation and Management,,
Agriculture,USDA,ARS,NAL Automated indexing,"Cogito (vendor) software, uses AI for automated subject indexing to annotate peer reviewed journal articles (~500,000 annually) using the National Ag Library Thesaurus concept space (NALT). Only NALT concepts are annotated as metadata to content in the Library's bibliographic citation database, AGRICOLA, PubAg, and Ag Data Commons.",Operation and Management,,
Agriculture,USDA,ERS,Democratizing Data,"The purpose of this project is to use AI tools, machine learning and natural language processing to understand how publicly-funded data and evidence are used to serve science and society.",Implementation,,
Agriculture,USDA,ERS,Westat,"A competition to find automated, yet effective, ways of linking USDA nutrition information to 750K food items in a proprietary data set of food purchases and acquisitions. Competing teams used a number of  AI methods including Natural Language Processing (NLP), random forest, and semantic matching.",Operation and Management,,
Agriculture,USDA,Federal CDO Council,OCIO/CDO Council Comment Analysis Tool,"The Comment Analysis pilot has shown that a toolset leveraging recent advances in Natural Language Processing (NLP) can aid the regulatory comment analysis process. We developed tools that help comment reviewers identify the topics and themes of comments, as well as group comments that are semantically similar. Tools like these offer significant value by creating efficiencies through novel insights and streamlined processing of comments, reducing duplicative, upfront development efforts across government, and ultimately realizing cost savings for agencies and the USG. 
",Development and Acquisition,,
Agriculture,USDA,FNS,Retailer Receipt Analysis,"The Retailer Receipt Analysis is a Proof of Concept (POC) that uses Optical Character Recognition (OCR), an application of artificial intelligence on a sample (no more than 1000) of FNS receipt and invoice data. Consultants will use this data to demonstrate how the existing manual process can be automated, saving staff time, ensuring accurate review, and detecting difficult patterns. The goal of this POC will pave the way for a review system that (1) has an automated workflow and learns from analyst feedback (2) can incorporate know SNAP fraud patterns, look for new patterns, and visualize alerts on these patterns on retailer invoices and receipts.",Development and Acquisition,,
Agriculture,USDA,FNS,Nutrition Education & Local Access Dashboard,"The goal of the this Dashboard is to provide a county-level visualization of FNS nutrition support, specifically nutrition education and local food access, alongside other metrics related to hunger and nutritional health. As part of this dashboard, the team developed a K-means clustering script to group States by 7 different clustering options:  Farm to School Intensity & Size, Program Activity Intensity, Ethnicity & Race, Fresh Food Access, School Size, and Program Participation. This allows users to find like-minded, or similar, States based on any of these characteristics, opening up avenues for partnerships with States that they otherwise may not have considered.",Operation and Management,,
Agriculture,USDA,Forest Service,Ecosystem Management Decision Support System (EMDS),"EMDS is a spatial decision support system for landscape analysis and planning that runs as a component of ArcGIS and QGIS. Users develop applications for their specific problem that may use any combination of four AI engines for 1) logic processing, 2) multi-criteria decision analysis, 3) Bayesian networks, and Prolog-based decision trees.",Operation and Management,,
Agriculture,USDA,Forest Service,Wildland Urban Interface - Mapping Wildfire Loss,"This is a proof-of-concept study to investigate the use of machine learning (deep learning / convolutional neural networks) and object-based image classification techniques to identify buildings, building loss, and defensible space around buildings before and after a wildfire event in wildland-urban interface settings.",Development and Acquisition,,
Agriculture,USDA,Forest Service,CLT Knowledge Database,"The CLT knowledge database catalogs cross-laminated timber information in an interface that helps users find relevant information. The information system uses data aggregator bots that search the internet for relevant information. These bots search for hundreds of keywords and use machine learning to determine if what is found is relevant. The search engine uses intelligent software to locate and update pertinent CLT references, as well as categorize information with respect to common application and interest areas. As of 2/24/2022, the CLT knowledge database has cataloged >3,600 publications on various aspects of CLT. This system fosters growth of mass timber markets by disseminating knowledge and facilitating collaboration among stakeholders, and by reducing the risk of duplication of efforts. Manufacturers, researchers, design professionals, code officials, government agencies, and other stakeholders directly benefit from the tool, thereby supporting the increasing use of mass timber, which benefits forest health by increasing the economic value of forests.",Operation and Management,,
Agriculture,USDA,Forest Service,RMRS Raster Utility,"RMRS Raster Utility is a .NET object oriented library that simplifies data acquisition, raster sampling, and statistical and spatial modeling while reducing the processing time and storage space associated with raster analysis. It includes machine learning techniques.",Operation and Management,,
Agriculture,USDA,Forest Service,TreeMap 2016,"TreeMap 2016 provides a tree-level model of the forests of the conterminous United States. It matches forest plot data from Forest Inventory and Analysis (FIA) to a 30x30 meter (m) grid. TreeMap 2016 is being used in both the private and public sectors for projects including fuel treatment planning, snag hazard mapping, and estimation of terrestrial carbon resources. A random forests machine-learning algorithm was used to impute the forest plot data to a set of target rasters provided by Landscape Fire and Resource Management Planning Tools (LANDFIRE: https://landfire.gov). Predictor variables consisted of percent forest cover, height, and vegetation type, as well as topography (slope, elevation, and aspect), location (latitude and longitude), biophysical variables (photosynthetically active radiation, precipitation, maximum temperature, minimum temperature, relative humidity, and vapour pressure deficit), and disturbance history (time since disturbance and disturbance type) for the landscape circa 2016.",Operation and Management,,
Agriculture,USDA,Forest Service,Landscape Change Monitoring System (LCMS),"The Landscape Change Monitoring System (LCMS) is a National landsat/sentinal remote sensing-based data produced by the USDA Forest Service for mapping and monitoring changes related to vegetation canopy cover, as well as land cover and land use. The process utilizes temporal change classifications together with training data in a supervised classification process for vegetation gain, and loss as well as land cover and use.",Development and Acquisition,,
Agriculture,USDA,Forest Service,Geospatial and Remote Sensing Training Courses,"Several courses are offered which teach the use of software and scripting which allow for machine learning.  The courses change, but current topics include Intro and Advanced Change Detection, eCognition (software package), Geospatial Scripting for Google Earth Engine.  Some of the courses show how to use Collect Earth Online.",Operation and Management,,
Agriculture,USDA,Forest Service,Forest Health Detection Monitoring,"Machine learning models are used to (1) upscale training data, using Sentinel-2, Landsat,  MODIS, and lidar imagery, that was collected from both the field and high-resolution imagery to map and monitor stages of forest mortality and defoliation across the United States, and (2) to post-process raster outputs to vector polygons.",Operation and Management,,
Agriculture,USDA,FPAC,Land Change Analysis Tool (LCAT),We employ a random forest machine learning classifier to produce high resolution land cover maps from aerial and/or satellite imagery.  Training data is generate from a custom-built web application.  We built and operate a 192-node docker cluster to parallize CPU-intensive processing tasks.  We are publishing results through a publicly available  Image service.  To date we have mapped over 600 million acres and have generated over 700 thousand traiing samples.,Operation and Management,,
Agriculture,USDA,NASS,Cropland Data Layer,"A machine learning algorithm is used to interpret readings from satellite-based sensors and CLASSIFY the type of crop or activity that falls in each 30 square meter pixel (a box of fixed size) on the ground.  The algorithms are trained on USDA&%2339;s Farm Services Agency data and other sources of data as sources of &quot;ground truth&quot;.  It allows us to not only produce a classification, but to assess the accuracy of the classification as well.  For commodities, like corn and soybeans, the CDL is highly accurate.  The CDL has been produced for national coverage since 2008.  Some summary and background about the CDL is available in a number of peer reviewed research papers and presentations
https://www.nass.usda.gov/Research_and_Science/Cropland/othercitations/index.php",Operation and Management,,
Agriculture,USDA,NASS,List Frame Deadwood Identification,"The deadwood model leverages boosted regression trees with inputs such as  administrative linkage data, frame data, and historical response information as inputs, to produce a propensity score representing a relative likelihood of a farm operation being out of business.  Common tree splits were identified using the model and combined with expert knowledge to develop a recurring process for deadwood clean up.",Operation and Management,,
Agriculture,USDA,NASS,Census of Agricuilture Response Propensity Scores,"The response propensity scores to the COA are derived from random forest models that use historical data, control data, and other survey data. These scores are used to help target more effective data collection.",Operation and Management,,
Agriculture,USDA,NIFA,Climate Change Classification NLP,"The model classifies NIFA funded projects as climate change related or not climate related through natural language processing techniques. The model input features include text fields containing the project's title, non-technical summary, objectives and keywords. The target is a dummy variable classification of projects as climate change related or not climate change related.",Development and Acquisition,,
Agriculture,USDA,NRCS,Operational water supply forecasting for western US rivers,"Western US water management is underpinned by forecasts of spring-summer river flow volumes made using operational hydrologic models. The USDA Natural Resources Conservation Service (NRCS) National Water and Climate Center operates the largest such forecast system regionally, carrying on a nearly century-old tradition. The NWCC recently developed a next-generation prototype for generating such operational water supply forecasts (WSFs), the multi-model machine-learning metasystem (M4), which integrates a variety of AI and other data-science technologies carefully chosen or developed to satisfy specific user needs. Required inputs are data around snow and precipitation from the NRCS Snow Survey and Water Supply Forecast program SNOTEL environmental monitoring network, but are flexible.  In hindcasting test-cases spanning diverse environments across the western US and Alaska, out-of-sample accuracy improved markedly over current benchmarks. Various technical design elements, including multi-model ensemble modeling, autonomous machine learning (AutoML), hyperparameter pre-calibration, and theory-guided data science, collectively permitted automated training and operation.  Live operational testing at a subset of sites additionally demonstrated logistical feasibility of workflows, as well as geophysical explainability of results in terms of known hydroclimatic processes, belying the black-box reputation of machine learning and enabling relatable forecast storylines for NRCS customers.",Development and Acquisition,,
Agriculture,USDA,NRCS,Ecological Site Descriptions (machine learning),"Analysis of over 20 million records of soils data and 20,000 text documents of ecological state and transition information. ",Development and Acquisition,,
Agriculture,USDA,NRCS,Conservation Effects Assessment Project,"The goal is to predict conservation benefits at the field level. The model uses farmer survey data, APEX modeling results and environmental data.",Development and Acquisition,,
Agriculture,USDA,NRCS,Digital Imagery (no-change) for NRI program,Using neural networks and other AI technologies to detect no-changes in digital imagery for the NRI (national resources inventory) program ,Initiation,,
Agriculture,USDA,OASCR,Artificial Intelligence SPAM Mitigation Project,"The AI Solution invoves Robotic Process Automation + AI/ML model solution to automatically classify and remove spam and marketing emails that appear in civil rights complaints email channels. A significant portion of incoming OASCR emails are spam, marketing and phishing emails. 
",Development and Acquisition,,
Agriculture,USDA,OCIO,Acquisition Approval Request Compliance Tool,"A natural language processing (NLP) model was developed to utilize the text in procurement header and line descriptions within USDA's Integrated Acquisition System (IAS) to determine the likelihood that an award is IT-related, and therefore might require an AAR. The model uses the text characteristics for awards that have an AAR number entered into IAS and then calculates the probability of being IT-related for those procurements that did not have an AAR Number entered in IAS.",Operation and Management,,
Agriculture,USDA,OCIO,Intelligent Ticket Routing,"Routes BMC Remedy tickets to proper work group automatically utilizing python, jupyterhub, scikit learn, gitlab, flask, gunicorn, nginx, erms.",Operation and Management,,
Agriculture,USDA,OCIO,Predictive Maintenance Impacts,"Predict impacts of DISC maintenance on infrastructure items.  Utilizes: einblick, mysql, python, linux, tableau",Operation and Management,,
Agriculture,USDA,OSSP,Video Surveillance System,"The Video Surveillance System: the VSS system design will include a video management system, NVRs, DVRs, encoders, fixed cameras, Pan and Tilt cameras, network switches, routers, IP cables, equipment racks and mounting hardware. The Video Surveillance System (VSS)- shall control multiple sources of video surveillance subsystems to collect, manage, and present video clearly and concisely. VMS shall integrate the capabilities of each subsystem across single or multiple sites, allowing video management of any compatible analog or digital video device through a unified configuration platform and viewer. Disparate video systems are normalized and funneled through a shared video experience. Drag and drop cameras from the Security Management System hardware tree into VMS views and leverage Security Management System alarm integration and advanced features that help the operator track a target through a set of sequential cameras with a simplified method to select a new central camera and surrounding camera views.",Operation and Management,,
Commerce,"International Trade 
Administration (ITA)",,B2B Matchmaking,"The system's algorithms and AI technology qualifies data and makes B2B matches with 
event participants according to their specific needs and available opportunities.  The 
systems inputs are data related to event participants and the outputs are suggested B2B 
matches between participants and a match strength scorecard.",,"Russell, Ryan",Ryan.Russell@trade.gov
Commerce,"International Trade 
Administration (ITA)",,Chatbot Pilot,"Chatbot embedded into trade.gov to assist ITA clients with FAQs, locating information and 
content, suggesting events and services.  ITA clients would enter input into the chatbot in 
the form of questions or responses to prompts.  The chatbot would scan ITA content 
libraries and input from ITA staff and return answers and suggestions based on client 
persona (exporter, foreign buyer, investor).",,"Howard, Ed",Ed.Howard@trade.gov
Commerce,"International Trade 
Administration (ITA)",,Consolidated Screening List,"The Consolidated Screening List (CSL) is a list of parties for which the United States 
Government maintains restrictions on certain exports, reexports, or transfers of items. It 
consists of the consolidation of 13 export screening lists of the Departments of 
Commerce, State, and Treasury.  The CSL search engine has “Fuzzy Name Search” 
capabilities, allowing a search without knowing the exact spelling of an entity’s name. In 
Fuzzy Name mode, the CSL returns a “score” for results that exactly or nearly match the 
searched name. This is particularly helpful when searching on CSL for names that have 
been translated into English from non-Latin alphabet languages.",,"Kirwin, Patrick",patrick.kirwin@trade.gov
Commerce,"International Trade 
Administration (ITA)",,AD/CVD Self Initiation,"The ADCVD program investigates allegations of dumping and/or countervailing of duties.  
Investigations are initiated when a harmed US entity files a petition identifying the alleged 
offence and the specific harm inflicted.  Self-Initiation will allow ITA to monitor trade 
patterns for this activity and preemptively initiate investigations by identifying harmed US 
entities, often before these entities are aware of the harm.",,"Kennedy, Brooke",brook.kennedy@trade.gov
Commerce,"International Trade 
Administration (ITA)",,Market Diversification Toolkit,"The Market Diversification Tool identifies potential new export markets using current 
trade patterns. A user enters what products they make and the markets they currently 
export to.  The Market Diversification Tool applies a ML algorithm to identify and compare 
markets that should be considered.  The tool brings together product-specific trade and 
tariff data and economy-level macroeconomic and governance data to provide a picture 
of which markets make sense for further market research. Users can limit the markets in 
the results to only the ones they want to consider and modify how each of the eleven 
indicators in the tool contributes to a country’s overall score. Users can export all the data 
to a spreadsheet for further analysis.",,"Barr, Morgan",morgan.barr@trade.gov
Commerce,"Minority Business Development 
Administration (MBDA)",,Azure Chatbot,"Azure Chatbot is being leveraged to automate and streamline the user response to 
potential questions for MBDA users while interacting with the external facing MBDA 
website. The solution leverages AI based chatbot response coupled with Machine 
Learning and Natural Language Processing capabilities.",,"Bajwa, Prabhjot",pbajwa@doc.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"Fisheries Electronic Monitoring 
Image Library","The Fisheries Electronic Monitoring Library (FEML) will be the central repository for 
electronic monitoring (EM) data related to marine life.",,"Alger, Brett",brett.alger@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"Passive acoustic analysis using ML 
in Cook Inlet, AK","Passive acoustic data is analyzed for detection of beluga whales and classification of the 
different signals emitted by these species. Detection and classification are done with an 
ensemble of 4 CNN models and weighted scoring developed in collaboration with 
Microsoft. Results are being used to inform seasonal distribution, habitat use, and impact 
from anthropogenic disturbance within Cook Inlet beluga critical habitat. The project is 
aimed to expand to other cetacean species as well as anthropogenic noise.",,"Angliss, Robyn",robyn.angliss@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"AI-based automation of acoustic 
detection of marine mammals","Timely processing of these data is critical for adapting mitigation measures as climate 
change continues to impact Arctic marine mammals.  Infrastructure for Noise and 
Soundscape Tolerant Investigation of Nonspecific Call Types (INSTINCT) is command line 
software which was developed in-house for model training, evaluation, and deployment 
of machine learning models for the purpose of marine mammal detection in passive 
acoustic data. It also includes annotation workflows for labeling and validation. INSTINCT 
has been successfully deployed in several analyses, and further development of detectors 
within INSTINCT is desired for future novel studies and automation. Continued integration 
of AI methods into existing processes of the CAEP acoustics group requires a skilled 
operator familiar with INSTINCT, machine learning, and acoustic repertoire of Alaska 
region marine mammals.",,"Berchok, Catherine",cathering.bderchok@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"Developing automation to 
determine species and count 
using optical survey data in the 
Gulf of Mexico","VIAME - This project focuses on optical survey collected in the Gulf of Mexico: 1) develop 
an image library of landed catch, 2) develop of automated image processing (ML/DL) to 
identify and enumerate species from underwater imagery and 3) develop automated 
algorithms to process imagery in near real time and download information to central 
database.",,"Campbell, Matthew",matthew.d.campbell@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"Fast tracking the use of VIAME for 
automated identification of reef 
fish","We've been compiling image libraries for use in creating automated detection and 
classification models for use in automating the annotation process for the SEAMAP Reef 
Fish Video survey of the Gulf of Mexico. This work is being conducted in VIAME but we're 
looking at several other paths forward in the project to identify best performing models. 
Current status is that models are performing well enough that we will incorporate 
automated analysis in video reads this spring as part of a supervised annotation-qa/qc 
process.",,"Campbell, Matthew",matthew.d.campbell@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"A Hybrid Statistical-Dynamical 
System for the Seamless 
Prediction of Daily Extremes and 
Subseasonal to Seasonal Climate 
Variability","Demonstrate the skill and suitability for operations of a statistical- dynamical prediction 
system that yields seamless probabilistic forecasts of daily extremes and sub seasonal-to-
seasonal temperature and precipitation. We recently demonstrated a Bayesian statistical 
method for post-processing seasonal forecasts of mean temperature and precipitation 
from the North American Multi-Model Ensemble (NMME). We now seek to test the utility 
of an updated hybrid statistical-dynamical prediction system that facilitates seamless sub 
seasonal and seasonal forecasting. Importantly, this method allows for the representation 
of daily extremes consistent with climate conditions. This project explores the use of 
machine learning.",,"Collins, Dan",dan.collins@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,FathomNet,"FathomNet provides much-needed training data (e.g., annotated, and localized imagery) 
for developing machine learning algorithms that will enable fast, sophisticated analysis of 
visual data. We've utilized interns and college class curriculums to localize annotations on 
NOAA video data for inclusion in FathomNet and to begin training our own algorithms.",,"Cromwell, Megan",megan.cromwell@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"ANN to improve CFS T and P 
outlooks","Fan Y., Krasnopolsky, V., van den Dool H., Wu, C. , and Gottschalck J. (2021). Using 
Artificial Neural Networks to Improve CFS Week 3-4 Precipitation and Temperature 
Forecasts.",,"Fan, Yun",yun.fan@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"Drought outlooks by using ML 
techniques","Drought outlooks by using ML techniques with NCEP models. Simple NN and Deep 
Learning techniques used for GEFSv12 to predict Week 1-5 Prcp & T2m over CONUS",,"Fan, Yun",yun.fan@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"EcoCast: A dynamic ocean 
management tool to reduce 
bycatch and support sustainable 
fisheries","Operational tool that uses boosted regression trees to model the distribution of swordfish 
and bycatch species in the California Current",,"Hazen, Elliott",elliot.hazen@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"Coastal Change Analysis Program 
(C-CAP)","Beginning in 2015, C-CAP embarked on operational high resolution land cover 
development effort that utilized geographic object-based image analysis and ML 
algorithms such as Random Forest to classify coastal land cover from 1m multispectral 
imagery. More recently, C-CAP has been relying on a CNN approach for the deriving the 
impervious surface component of their land cover products. The majority of the work is 
accomplished through external contracts. Prior to the high-res effort, C-CAP focused on 
developing Landsat based moderate resolution multi-date land cover for the coastal U.S. 
In 2002, C-CAP adopted a methodology that employed Classification and Regression Trees 
for land cover data development.",,"Herold, Nate",nate.herold@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"Deep learning algorithms to 
automate right whale photo id","AI for right whale photo id began with a Kaggle competition and has since expanded to 
include several algorithms to match right whales from different viewpoints (aerial, lateral) 
and body part (head, fluke, peduncle). The system is now live and operational on the 
Flukebook platform for both North Atlantic and southern right whales. We have a paper in 
review at Mammalian Biology.",,"Khan, Christin",christin.khan@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,NN Radiation,"Developing fast and accurate NN LW- and SW radiations for GFS and GEFS.  NN LW- and 
SW radiations have been successfully developed for previous version of GFS, see: doi: 
10.1175/2009MWR3149.1 and the stability and robustness of the approach used was 
demonstrated, see:  https://arxiv.org/ftp/arxiv/papers/2103/2103.07024.pdf  NN LW- and 
SW radiations will be developed for the current versions of for GFS and GEFS.",,"Krasnopolosky, Vladimir",vladimir.krasnopolsky@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"NN training software for the new 
generation of NCEP models","Optimize NCEP EMC Training and Validation System for efficient handling of high spatial 
resolution model data produced by the new generation of NCEP's operational models",,"Krasnopolosky, Vladimir",vladimir.krasnopolsky@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,Coral Reef Watch,"For more than 20 years, NOAA Coral Reef Watch (CRW) has been using remote sensing, 
modeled, and in situ data to operate a Decision Support System (DSS) to help resource 
managers (our target audience), researchers, decision makers, and other stakeholders 
around the world prepare for and respond to coral reef ecosystem stressors, 
predominantly resulting from climate change and warming of the Earth's oceans. Offering 
the world's only global early-warning system of coral reef ecosystem physical 
environmental changes, CRW remotely monitors conditions that can cause coral 
bleaching, disease, and death; delivers information and early warnings in near real-time to 
our user community; and uses operational climate forecasts to provide outlooks of 
stressful environmental conditions at targeted reef locations worldwide. CRW products 
are primarily sea surface temperature (SST)-based but also incorporate light and ocean 
color, among other variables.",,"Manzello, Derek",derek.manzello@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"Robotic microscopes and machine 
learning algorithms remotely and 
autonomously track lower trophic 
levels for improved ecosystem 
monitoring and assessment","Phytoplankton are the foundation of marine food webs supporting fisheries and coastal 
communities. They respond rapidly to physical and chemical oceanography, and changes 
in phytoplankton communities can impact the structure and functioning of food webs. We 
use a robotic microscope called an Imaging Flow Cytobot (IFCB) to continuously collect 
images of phytoplankton from seawater. Automated taxonomic identification of imaged 
phytoplankton uses a supervised machine learning approach (random forest algorithm). 
We deploy the IFCB on fixed (docks) and roving (aboard survey ships) platforms to 
autonomously monitor phytoplankton communities in aquaculture areas in Puget Sound 
and in the California Current System. We map the distribution and abundance of 
phytoplankton functional groups and their relative food value to support fisheries and 
aquaculture and describe their changes in relation to ocean and climate variability and 
change.",,"Moore, Stephanie",stephanie.moore@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"Edge AI survey payload 
development","Continued support of multispectral aerial imaging payload running detection model 
pipelines in real-time. This is a nine camera (color, infrared, ultraviolet) payload controlled 
by dedicated on-board computers with GPUs. YOLO detection models run at a rate faster 
than image collection, allowing real-time processing of imagery as it comes off the 
cameras. Goals of effort are to reduce overall data burden (by TBs) and reduce the data 
processing timeline, expediting analysis and population assessment for arctic mammals.",,"Moreland, Erin",erin.moreland@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"Ice seal detection and species 
classification in multispectral 
aerial imagery","Refine and improve detection and classification pipelines with the goal of reducing false 
positive rates (to < 50%) while maintaining > 90% accuracy and significantly reducing or 
eliminating the labor intensive, post survey review process.",,"Moreland, Erin",erin.moreland@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"First Guess Excessive Rainfall 
Outlook","Machine Learning Product that is a first guess for the WPC Excessive Rainfall Outlook - It is 
learned from the ERO with atmospheric variables. It is for the Day 4-7 products",,"Nelson, James",james.a.nelson@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"First Guess Excessive Rainfall 
Outlook","Machine Learning Product that is a first guess for the WPC Excessive Rainfall Outlook - It is 
learned from the ERO with atmospheric variables. It is for the Day 1, 2, 3 products",,"Nelson, James",james.a.nelson@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"CoralNet: Ongoing operational 
use, improvement, and 
development, of machine vision 
point classification","CoralNet is our operational point annotation software for benthic photo quadrat 
annotation. Our development of our classifiers has allowed us to significantly reduce our 
human annotation, and we continue to co-develop (and co-fund) new developments in 
CoralNet,",,"Oliver, Thomas",thomas.oliver@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"Automated detection of 
hazardous low clouds in support 
of safe and efficient 
transportation","This is a maintenance and sustainment project for the operational GOES-R fog/low stratus 
(FLS) products. The FLS products are derived from the combination of GOES-R satellite 
imagery and NWP data using machine learning. The FLS products, which are available in 
AWIPS, are routinely used by the NWS Aviation Weather Center and Weather Forecast 
Offices.",,"Pavolonis, Mike",mike.pavolonis@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"The Development of ProbSevere 
v3 - An improved nowcasting 
model in support of severe 
weather warning operations","ProbSevere is a ML model that utilizes NWP, satellite, radar, and lightning data to nowcast 
severe wind, severe hail, and tornadoes. ProbSevere, which was transitioned to NWS 
operations in October 2020, is a proven tool that enhances operational severe weather 
warnings. This project aims to develop the next version of ProbSevere, ProbSevere v3. 
ProbSevere v3 utilizes additional data sets and improved machine learning techniques to 
improve upon the operational version of ProbSevere. ProbSevere v3 was successfully 
demonstrated in the 2021 Hazardous Weather Testbed and a JTTI proposal was recently 
submitted to facilitate an operational update. The development is funded by GOES-R.",,"Pavolonis, Mike",mike.pavolonis@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"The VOLcanic Cloud Analysis 
Toolkit (VOLCAT): An application 
system for detecting, tracking, 
characterizing, and forecasting 
hazardous volcanic events","Volcanic ash is a major aviation hazard. The VOLcanic Cloud Analysis Toolkit (VOLCAT) 
consists of several AI powered satellite applications including: eruption detection, 
alerting, and volcanic cloud tracking. These applications are routinely utilized by Volcanic 
Ash Advisory Centers to issue volcanic ash advisories. Under this project, the VOLCAT 
products will be further developed, and subsequently transitioned to the NESDIS Common 
Cloud Framework, to help ensure adherence to new International Civil Aviation 
Organization requirements.",,"Pavolonis, Mike",mike.pavolonis@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,SUVI Thematic Maps,"The GOES-16 Solar Ultraviolet Imager (SUVI) is NOAA's operational solar extreme-
ultraviolet imager. The SUVI Level 2 Thematic Map files in these directories are produced 
by NOAA's National Centers for Environmental Information in Boulder, Colorado. These 
data have been processed from Level 2 High Dynamic Range (HDR) composite SUVI 
images. The FITS file headers are populated with metadata to facilitate interpretation by 
users of these observations. Please note that these files are considered to be 
experimental and thus will be improved in future releases. Users requiring assistance with 
these files can contact the NCEI SUVI team by emailing goesr.suvi@noaa.gov. The SUVI 
Thematic Maps product is a Level 2 data product that (presently) uses a machine learning 
classifier to generate a pixel-by-pixel map of important solar features digested from all six 
SUVI spectral channels.",,"Rachmeler, Laurel",laurel.rachmeler@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"BANTER, a machine learning 
acoustic event classifier",A supervised machine learning acoustic event classifier using hierarchical random forests,,"Rankin, Shannon",shannon.rankin@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"ProbSR (probability of subfreezing 
roads",A machine-learned algorithm that provides a 0-100% probability roads are subfreezing,,"Reeves, Heather",heather.reeves@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"VIAME: Video and Image Analysis 
for the Marine Environment 
Software Toolkit","The Video and Image Analysis for the Marine Environment Software Toolkit, commonly 
known as VIAME, is an open-source, modular software toolkit that allows users to employ 
high-level, deep-learning algorithms for automated annotation of imagery using a low 
code/no code graphical user interface. VIAME is available free of charge to all NOAA 
users.  The NOAA Fisheries Office of Science and Technology supports an annual 
maintenance contract covering technical and customer support by the developer, routine 
software updates, bug fixes, and development efforts that support broad, cross center 
application needs.",,"Richards, Benjamin",benjamin.richards@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"ENSO Outlooks using 
observed/analyzed fields","LSTM model that uses ocean and atmospheric predictors throughout the tropical Pacific 
to forecast ONI values up to 1 year in advance. An extension of this was submitted to the 
cloud portfolio with the intent of adding a CNN layer that that uses reforecast data to 
improve the ONI forecasts.",,"Rosencrans, Matthew",matthew.rosencrans@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"Using community-sourced 
underwater photography and 
image recognition software to 
study green sea turtle distribution 
and ecology in southern California","The goal of this project is to study green turtles in and around La Jolla Cove in the San 
Diego Region-a highly populated site with ecotourism-by engaging with local 
photographers to collect green turtle underwater images.  The project uses publicly 
available facial recognition software (HotSpotter) to identify individual turtles, from which 
we determine population size, residency patterns, and foraging ecology",,"Seminoff, Jeffrey",jeffrey.seminoff@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"An Interactive Machine Learning Signals in Passive Acoustic  Recordings
Toolkit for Classifying Species 
Identity of Cetacean Echolocation","Develop robust automated machine learning detection and classification tools for acoustic 
species identification of toothed whale and dolphin echolocation clicks for up to 20 
species found in the Gulf of Mexico. Tool development project funded from June 2018 to May 2021. Tool will be used for automated analyses of long-term recordings from Gulf- wide passive acoustic moored instruments deployed from 2010-2025 to look at  environmental processes driving trends in marine mammal density and distribution.",,"Soldevilla, Melissa",melissa.soldevilla@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"Steller sea lion automated count 
program","NOAA Fisheries Alaska Fisheries Science Center's Marine Mammal Laboratory (MML) is 
mandated to monitor the endangered western Steller sea lion population in Alaska. MML 
conducts annual aerial surveys of known Steller sea lion sites across the southern 
Alaska coastline to capture visual imagery. It requires two full-time, independent counters 
to process overlapping imagery manually (to avoid double counting sea lions in multiple 
frames), and count and classify individuals by age and sex class. These counts are vital 
for population and ecosystem-based modeling to better understand the species and 
ecosystem, to inform sustainable fishery management decisions, and are eagerly 
anticipated by stakeholders like the NOAA Alaska Regional Office, industry, and 
environmental groups. MML worked with Kitware to develop detection and image 
registration pipelines with VIAME (updates to the DIVE program to support updated 
interface needs). MML is now working to assess the algorithms efficacy and develop a 
workflow to augment the traditional counting method (to RL 9).",,"Sweeney, Katie",katie.sweeney@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,Steller sea lion brand sighting,"Detection and identification of branded steller sea lions from remote camera images in 
the western Aleutian Islands, AK. The goal is to help streamline photo processing to 
reduce the effort required to review images.",,"Sweeney, Katie",katie.sweeney@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"Replacing unstructured WW3 in 
the Great Lakes with a Recurrent 
neural network and a boosted 
ensemble decision tree","Investigated replacing unstructured WW3 in the Great Lakes with (i) a Recurrent Neural 
Network (RNN, especially an LSTM) developed by EMC and (ii) a boosted ensemble 
decision tree (XGBoost) developed by GLERL. These two AI models were trained on two 
decades of wave observations in Lake Erie and compared to the operational Great Lakes 
unstructured WW3.",,"Van Der Westhuysen, Andre",andre.vanderwesthuysen@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,"Using k-means clustering to 
identify spatially and temporally 
consistent wave systems","Postprocessing that uses k-means clustering to identify spatially and temporally consistent 
wave systems from the output of NWPS v1.3. Has been successfully evaluated in the field 
by NWS marine forecasters nationwide and has been implemented into operations on 
February 3, 2021.",,"Van Der Westhuysen, Andre",andre.vanderwesthuysen@noaa.gov
Commerce,"National Oceanic and 
Atmospheric Administration 
(NOAA)",,Picky,"Using CNN to pick out objects of a particular size from sides scan imagery.  Presents users 
with a probability that allows for automation of contact picking in the field.  Side scan 
imagery is simple one channel intensity image which lends itself well to basic CNN 
techniques.",,"Zhang, Chen",chen.zhang@noaa.gov
Commerce,"National Telecommunications and 
Information Administration (NTIA)",,Data Science: Clutter,"NTIA’s Institute for Telecommunication Sciences (ITS) is investigating the use of AI to 
automatically identify and classify clutter obstructed radio frequency propagation paths. 
Clutter is vegetation, buildings, and other structures that cause radio signal loss through 
dispersion, reflection, and diffraction. It does not include terrain effects. The classifier is a 
convolutional neural network (CNN) trained using lidar data coinciding with radio 
frequency propagation measurements made by ITS. This trained CNN can be fed new 
radio path lidar data and a clutter classification label is predicted.",,"Eales, Bradley",beales@ntia.gov
Commerce,"National Telecommunications and 
Information Administration (NTIA)",,WAWENETS,"The algorithm produces estimates of telecommunications speech quality and speech 
intelligibility.  The input is a recording of speech from a telecommunications system in 
digital file format.  The output is a single number that indicates speech quality (typically 
on a 1 to 5 scale) or speech intelligibility (typically on a 0 to 1 scale).",,"Voran, Steve",svoran@ntia.gov
Commerce,"United States Patent and Trade 
Office (USPTO)",,AI retrieval for patent search,"Augmentation for next generation patent search tool to assist examiners identify relevant 
documents and additional areas to search.  System takes input from published or 
unpublished applications and provides recommendations on further prior art areas to 
search, giving the user the ability to sort by similarity to concepts of their choosing.",,"Horner, Jon",jonathan.horner@uspto.gov
Commerce,"United States Patent and Trade 
Office (USPTO)",,AI use for CPC classification,"System that classifies incoming patent application based on the cooperative patent 
classification scheme for operational assignment of work and symbol recommendation for 
aI search.  Backoffice processing system that uses incoming patent applications as input 
and outputs the resulting classification symbols.",,"Yang, Nelson",nelson.yang@uspto.gov
Commerce,"United States Patent and Trade 
Office (USPTO)",,"AI retrieval for TM design coding 
and Image search","Clarivate COTS solution to assist examiner identification of similar trademark images, to 
suggest the correct assignment of mark image design codes, and to determine the 
potential acceptability of the identifications of goods and services.  System is anticipated 
to use both incoming trademark images and registered trademark images and output 
design codes and/or other related images.",,"Doninger, Chris",chris.doninger@uspto.gov
Commerce,"United States Patent and Trade 
Office (USPTO)",,Enriched Citation,"Data dissemination system that identifies which references, or prior art, were cited in 
specific patent application office actions, including: bibliographic information of the 
reference, the claims that the prior art was cited against, and the relevant sections that 
the examiner relied upon.  System extracts information from unstructured office actions 
and provides the information through a structured public facing API.",,"Yang, Nelson",nelson.yang@uspto.gov
Commerce,"United States Patent and Trade 
Office (USPTO)",,Inventor Search Assistant (iSAT),"Service to help inventors ""get started"" identifying relevant documents, figures, and 
classification codes used to conduct a novelty search.  System takes a user entered short 
description of invention and provides a user selectable set of recommended documents, 
figures, and classification areas.",,"Beliveau, Scott",scott.beliveau@uspto.gov
Education,Education,Office of Federal Student Aid (FSA),Aidan,"FSA’s virtual assistant uses natural language processing to answer common financial aid questions and help customers get information about their federal aid on StudentAid.gov. In just over two years, Aidan has interacted with over 2.6 million unique customers, resulting in more than 11 million user messages. ",Continuous Monitoring,,
Energy,,Brookhaven National Laboratory,"Objective-Driven Data Reduction for 
Scientific Workflows","This project aims to develop theories and algorithms for objective-driven reduction of 
scientific data in workflows that are composed of various models, including data-
driven AI models",Problem Scoping,"Byung-Jun, Yoon",byoon@bnl.gov
Energy,,Idaho National Laboratory,"Advances in Nuclear Fuel Cycle 
Nonproliferation, Safeguards, and 
Security Using an Integrated Data 
Science Approach","This research will develop a digital twin of a centrifugal contactor system that 
receives data from traditional and real time sensors, constructs a digital 
representation or simulation of the chemical separations component within the nuclear 
fuel cycle, and performs data analysis through machine learning to determine 
anomalies, failures, and trends. The research will include the identification and 
implementation of advanced artificial intelligence, machine learning, and data analysis 
techniques advised by a team of nuclear safeguards experts.",,"Kerman, Mitchell C.",mitchell.kerman@inl.gov
Energy,,Idaho National Laboratory,"Development of a multi-sensor data 
science system used for signature 
development on solvent extraction 
processes conducted within Beartooth 
facility","This project will develop a system that utilizes non-traditional measurement sources 
such as vibration, acoustics, current, and light, and traditional sources such as flow, 
and temperature in conjunction with data-based, machine learning techniques that will 
allow for signal discovery. The goal is to characterize stages within a solvent 
extraction process can increase target metals recovery, indicate process faults, 
account for special nuclear material, and inform near real-time decision making.",,"Kerman, Mitchell C.",mitchell.kerman@inl.gov
Energy,,Idaho National Laboratory,"Scalable Framework of Hybrid Modeling 
with Anticipatory Control Strategy for 
Autonomous Operation of Modular and 
Microreactors","The goal this research is to develop and validate novel and scalable models to 
achieve faster-than-real-time prediction and decision-making capabilities. To achieve 
the project goal of autonomous operation of microreactors, a novel hybrid modeling 
approach combining both physics-based and artificial intelligence techniques will be 
developed at the component or sub-system level, integrated with anticipatory control 
techniques, and scaled. A novel distributed anticipatory control strategy will be 
developed as part of the scalability analysis to understand the risk of cascading 
failures when emerging reactors are deployed as part of a full feeder microgrid.",,"Kerman, Mitchell C.",mitchell.kerman@inl.gov
Energy,,Idaho National Laboratory,"Accelerating and Improving the 
Reliability of Low Failure Probability 
Computations to Support the Efficient 
Safety Evaluation and Deployment of 
Advanced Reactor Technologies","This project will research artificial intelligence enabled Monte Carlo algorithms to 
significantly reduce the computational burden by reducing the number of finite element 
evaluations when estimating low failure probabilities. These will be implemented in the 
Multiphysics Object-Oriented Simulation Environment, which will help the nuclear 
engineering community to efficiently conduct probabilistic failure analyses and 
uncertainty quantification studies for the design and optimization of advanced reactor 
technologies.",,"Kerman, Mitchell C.",mitchell.kerman@inl.gov
Energy,,Idaho National Laboratory,"Accelerating deployment of nuclear 
fuels through reduced-order thermo-
physical property models and machine 
learning","This project will develop a novel physics-based tool that combines 1) reduced-order 
models, 2) machine learning algorithms, 3) fuel performance methods, and 4) state-of-
the-art thermal property characterization equipment and irradiated nuclear fuel data 
sets to accelerate nuclear fuel discovery, development, and deployment. The models 
will describe thermal conductivity, specific heat, thermal expansion, and self-diffusion 
coefficients as a function of temperature and irradiation.",,"Kerman, Mitchell C.",mitchell.kerman@inl.gov
Energy,,Idaho National Laboratory,"Promoting Optimal Sparse Sensing and 
Sparse Learning for Nuclear Digital 
Twins","This project will address the efficient use of limited experimental data available for 
nuclear digital twin (NDT) training and demonstration. This involves developing sparse 
data reconstruction methods and using NDT models to define sensor requirements 
(location, number, accuracy) for the design of demonstration experiments. NDTs 
should leverage 1) sparse sensing for identifying optimal locations and the minimal set 
of required sensors and 2) sparse learning and recovery of full maps of responses of 
interest for stronger prediction, diagnostics, and prognostics capabilities.",,"Kerman, Mitchell C.",mitchell.kerman@inl.gov
Energy,,Idaho National Laboratory,"Artificial Intelligence Enhanced 
Advanced Post Irradiation Examination","This project uses post irradiation examination of uranium-10wt.% zirconium (UZr) 
metallic fuel as a case study to show how artificial intelligence (AI)-based technology 
can facilitate and accelerate nuclear fuel development. The approach will 1) revisit the 
microstructural image and local thermal conductivity data collected from UZr, 2) build a 
benchmark dataset for the microstructural patterns of irradiated UZr, and 3) train the 
machine learning and deep learning models to uncover the relationships between 
micro/nanoscale structure, zirconium phase redistribution, local thermal conductivity, 
and engineering scale fuel properties.",,"Kerman, Mitchell C.",mitchell.kerman@inl.gov
Energy,,Idaho National Laboratory,"Secure Millimeter Wave Spectrum 
Sharing with Autonomous Beam 
Scheduling","This approach exploits the millimeter wave beam directionality and utilizes the beam 
sensing capabilities at end devices to prove that an autonomous radio frequency 
beam scheduler can support secure 5G spectrum sharing and guarantee optimality for 
base stations. Measurements and predictive analytics are used to develop the 
autonomous beam scheduling algorithms. These improvements will benefit mission 
critical communications and emergency response operations as well as enable 
secure communication for critical infrastructure without expensive and competitive 
licensed bands.",,"Kerman, Mitchell C.",mitchell.kerman@inl.gov
Energy,,Idaho National Laboratory,"Artificial Intelligence Based Process 
Control and Optimization for Advanced 
Manufacturing","This project will develop the capability to intelligently control and optimize advanced 
manufacturing processes instead of the existing trial and error approach. To achieve 
this goal, artificial intelligence (AI) based control algorithms will be developed by 
employing deep reinforcement learning. To reduce the computational expense with 
advanced manufacturing models, physics-informed reduced order models (ROMs) will 
be developed. The AI-based control algorithms will employ the ROMs’ predictions to 
adaptively inform processing decisions in a simulation environment.",,"Kerman, Mitchell C.",mitchell.kerman@inl.gov
Energy,,Idaho National Laboratory,"Smart Contingency Analysis Neural 
Network for in-depth Power Grid 
Vulnerability Analyses","Typical contingency analysis for a power utility is limited to n-1 due to computational 
complexity and cost. A machine learning framework and resilience-chaos plots are 
leveraged to reduce computational expense required to discover, with 90% accuracy, 
n-2 contingencies by 50%.",,"Kerman, Mitchell C.",mitchell.kerman@inl.gov
Energy,,Idaho National Laboratory,"Resilient Attack Interceptor for 
Intelligent Devices","The Resilient Attack Interceptor for Intelligent Devices approach focuses on 
developing external monitoring methods to protect industrial internet of things devices 
by correlating observable physical aspects that are produced naturally and 
involuntarily during the operational lifecycle with anomalous functionality.",,"Kerman, Mitchell C.",mitchell.kerman@inl.gov
Energy,,Idaho National Laboratory,Infrastructure eXpression,"The project developed a framework and process to translate industrial control system 
features to a machine-readable format for use with automated cyber tools. This 
research also examined other current and evolving standards for usability with diverse 
grid architectures that represent a set of variable conditions to establish the 
foundation for determining where future research should focus and to support 
improvements to industry standards and architecture designs for machine-learning 
cyber defense solutions. This project’s success can serve as the foundation for 
prioritizing the next research steps to realize automated threat response, improving 
the timeliness and fidelity of cyber incident consequence models, and enriching 
national capabilities to share actionable threat intelligence at machine speed.",,"Kerman, Mitchell C.",mitchell.kerman@inl.gov
Energy,,Idaho National Laboratory,"Protocol Analytics to enable Forensics 
of Industrial Control Systems","The goal of this research is to discover methods and technologies to bridge gaps 
between the various industrial control systems (ICS) communication protocols and 
standard Ethernet to enable existing cybersecurity tools defend ICS networks and 
empower cybersecurity analysts to detect compromise before threat actors can 
disrupt infrastructure, damage property, and inflict harm. Research focuses on 
electronic signal analysis of captured communication to determine the protocol, using 
use machine learning to identify unknown protocols. Findings will be incorporated into 
a prototype device.",,"Kerman, Mitchell C.",mitchell.kerman@inl.gov
Energy,,Idaho National Laboratory,"Automated Type and Data Structure 
Resolution","This research identified and labeled type and structure data in an automated and 
scalable way such that the information can be used in other tools and other Reverse 
Engineering at Scale research areas such as symbolic execution. This was done 
initially by utilizing heuristic methods and then scaled by adopting a machine learning 
approach.",,"Kerman, Mitchell C.",mitchell.kerman@inl.gov
Energy,,Idaho National Laboratory,"Signal Decomposition for Intrusion 
Detection in Reliability Assessment in 
Cyber Resilience","The objective of this project is to research, assess, and implement machine learning 
and artificial intelligence and physics-based algorithms for signal decomposition and 
provide a straightforward framework wherein an anomaly detection algorithm can be 
trained on existing expected data and then used for false data injection detection. An 
advanced library for signal decomposition and analysis will be developed that allows 
combining machine learning and artificial intelligence algorithms and high-fidelity model 
comparisons for greatly improved false data injection detection. This library will 
facilitate online and posteriori analysis of digital signals for the purpose of detecting 
potential malicious tampering in physical processes.",,"Kerman, Mitchell C.",mitchell.kerman@inl.gov
Energy,,Idaho National Laboratory,"Advanced Machine Learning-based 
Fifth Generation Network Attack 
Detection System","The project goal is to prove that enhancing attack detection via innovative machine 
learning and artificial intelligence techniques into the fifth generation (5G) cellular 
network can help to secure mission-critical applications, such as automated vehicles 
and drones, connected health, emergency response operations, and other mission-
critical devices that either are or will be connected to the 5G cellular network.",,"Kerman, Mitchell C.",mitchell.kerman@inl.gov
Energy,,Idaho National Laboratory,Red Teaming Artificial Intelligence,"This research will advance the state of the art for red team security assessment of 
machine learning and artificial intelligence systems by providing methods for the 
reverse engineering, exploitation, risk assessment and vulnerability remediation. The 
insights gained from the explorations into vulnerability assessment research will 
proactively address critical gaps in the cybersecurity community’s understanding of 
these systems and can be used to create appropriate risk evaluation metrics and 
provide best practices for inclusion into consequence-driven cyber-informed 
engineering.",,"Kerman, Mitchell C.",mitchell.kerman@inl.gov
Energy,,Idaho National Laboratory,"Unattended Operation through Digital 
Twin Innovations","The team hypothesizes that artificial intelligence can predict events using the 
integrated data from test bed sensors and physics-based models. A second 
hypothesis is that integrating software and artificial intelligence with sensor data from 
a test bed will lead to a framework for future digital twins. The team will train artificial 
intelligence models to determine what attributes are most important for enabling 
intelligent autonomous control and will determine best practices for digital twin 
cybersecurity.",,"Kerman, Mitchell C.",mitchell.kerman@inl.gov
Energy,,Idaho National Laboratory,"Secure and Resilient Machine Learning 
System for Detecting Fifth Generation 
(5G) Attacks including Zero-Day Attacks","This project will implement an advanced machine learning based 5G attack detection 
system that can achieve high classification speed (10k packets per second) with high 
accuracy (90% or greater) as well as address a vulnerability to zero-day attacks 
(90% accuracy against real zero-day attacks recorded by Amazon Web Services) 
using field programmable gate array based deep autoencoders.",,"Kerman, Mitchell C.",mitchell.kerman@inl.gov
Energy,,Idaho National Laboratory,"Automated Malware Analysis Via 
Dynamic Sandboxes","The goal of this project is to develop an analysis framework enabled by dynamic 
sandboxes that allows for automated analysis, provides non-existing core capabilities 
to analyze industrial control system malware, and outputs to a format that is machine 
readable and an industry standard in sharing threat information. This will enable further 
analysis efforts via machine learning and provide a foundational platform that would 
allow for timely, automated analysis of malware samples.",,"Kerman, Mitchell C.",mitchell.kerman@inl.gov
Energy,,Idaho National Laboratory,"Interdependent Infrastructure Systems 
Resilience Analysis for Enhanced 
Microreactor Power Grid Penetration","This project will develop machine learning enabled integrated resource planning 
methodologies to help quantify key resilience elements across integrated energy 
systems and their vulnerabilities to threats and hazards. This includes the ability to 
accurately analyze and visualize a region’s critical infrastructure systems ability to 
sustain impacts, maintain critical functionality, recover from disruptive events. This 
advanced decision support capability can improve our understanding of these complex 
relationships and help predict the potential impacts that microreactors and distributed 
energy resources have on the reliability and resiliency of our energy systems.",,"Kerman, Mitchell C.",mitchell.kerman@inl.gov
Energy,,Idaho National Laboratory,"Adaptive Fingerprinting of Control 
System Devices through Generative 
Adversarial Networks","This project focuses on the reduction of manual labor and operational cost required 
for training an electromagnetic (EM)-based anomaly detection system for legacy 
industrial control systems devices and Industrial Internet of Things. This research 
would enable EM-based intrusion detection systems to be deployed to protect legacy 
control systems.","Kerman, Mitchell C.",mitchell.kerman@inl.gov,
Energy,,Idaho National Laboratory,"Support Vector Analysis for 
Computational Risk Assessment, 
Decision-Making, and Vulnerability 
Discovery in Complex Systems","This project addressed limitations in current probabilistic risk assessment (PRA) by 
combining a support vector machine and PRA software to auto-detect system design 
vulnerabilities and find previously unseen issues, reduce human error, and reduce 
human costs. This method does not require training data that would only be available 
in the event of system or subsystem failures.","Kerman, Mitchell C.",mitchell.kerman@inl.gov,
Energy,,Idaho National Laboratory,"Deep Reinforcement Learning and 
Decision Analytics for Integrated 
Energy Systems","This project will develop a novel deep reinforcement learning approach that can 
manage distributed or tightly coupled multi-agent systems utilizing deep neural 
networks for automatic system representation, modeling, and end-to-end learning. 
This new control method will enable complex, nonlinear system optimization over 
timescales from milliseconds to months.","Kerman, Mitchell C.",mitchell.kerman@inl.gov,
Energy,,Idaho National Laboratory,"Nuclear-Renewable-Storage Digital 
Twin: Enhancing Design, Dispatch, and 
Cyber Response of Integrated Energy 
Systems","This project will develop a learning-based and digital twin enabled modeling and 
simulation framework for economic and resilient real-time decision-making of physics-
informed integrated energy systems (IES) operation. High-fidelity physics models will 
be linked with large-scale grid monitoring data to provide real-time updates of IES 
states, predictive control systems, and optimized power dispatch solutions. Learning-
based algorithms will make real-time decisions upon detection of component 
contingencies caused by climate-induced or man-made extreme events, such as 
cyber-attacks or extreme weather, thereby mitigating their impacts through 
appropriate counter measures.","Kerman, Mitchell C.",mitchell.kerman@inl.gov,
Energy,,Idaho National Laboratory,"Automated Infrastructure & Dependency 
Detection via Satellite Imagery and 
Dependency Profiles","Computer vision, a broad set of techniques for training statistical models and neural 
networks to process images, has advanced substantially in recent years. Applying 
these capabilities to satellite imagery can improve critical infrastructure analysis and 
interdependency data build-outs. Combining advanced computer vision techniques, a 
functional taxonomic approach to critical infrastructure, and the unique geo-spatial and 
dependency datasets the research team developed can produce innovative and state-
of-the-art image processing results that advance abilities to secure and defend 
national critical infrastructure.","Kerman, Mitchell C.",mitchell.kerman@inl.gov,
Energy,,Idaho National Laboratory,"Accelerated Nuclear Materials and Fuel 
Qualification by Adopting a First to 
Failure Approach","Physics-based multi-scale modeling was coupled with deep, recursive, and transfer 
learning approaches to accelerate nuclear materials research and qualification of high-
entropy alloys. Applying AI to combinatorial-based materials research enables 
subsequent analysis to focus on a limited number of candidates predicted to have the 
necessary materials properties for the application.","Kerman, Mitchell C.",mitchell.kerman@inl.gov,
Energy,,Idaho National Laboratory,"Evaluating thermal properties of 
advanced materials","The standard thermal diffusivity measurement technique laser flash is enhanced by 
modifying the traditional experimental set up and analyzing results with a machine 
learning based tool that includes a finite element model, a least-squares fitting 
algorithm and experimental data treatment algorithms. This tool helps elucidate thermo-
physical properties of a material from a single laser flash measurement.","Kerman, Mitchell C.",mitchell.kerman@inl.gov,
Energy,,Idaho National Laboratory,"Spectral Observation Convolutional 
Neural Network","This project developed method to analyze collected radiation spectra using advanced, 
scalable deep learning by combining spectroscopic expertise with high performance 
computing. Sophisticated deep learning can overcome the weaknesses of existing 
spectroscopic techniques and enhance the value of difficult measurements. This 
method was trained, tested, and operated on the International Space Station’s 
Spaceborne Computer-2 supercomputer, returning zero errors over the course of 100 
training hours. This demonstrated performance autonomously in far-edge, low-wattage 
computing situations and in hazardous radiological environments where interference 
can cause errors.","Kerman, Mitchell C.",mitchell.kerman@inl.gov,
Energy,,Idaho National Laboratory,"Passive Strain Measurements for 
Experiments in Radiation Environments","This project will develop passive instrumentation to determine permanent strains 
induced by irradiation and extract critical parameters using modeling and simulation as 
well as machine learning algorithms. An irradiation experiment will be conducted that 
will benefit from engineered anisotropic materials and characterize the directional 
deformation in response to neutron radiation. The results of the experiment will be 
incorporated into the model so that the material response can be predicted for future 
uses as a probe material.","Kerman, Mitchell C.",mitchell.kerman@inl.gov,
Energy,,Idaho National Laboratory,"Machine Learning Interatomic 
Potentials for Radiation Damage and 
Physical Properties in Model Fluorite 
Systems","This project will use machine learning interatomic potentials to study the influence of 
radiation damage on physical properties of calcium fluoride and uranium dioxide. 
Electron irradiation experiments and thermal conductivity measurements will be 
performed to validate the effectiveness of the developed potentials. The high 
throughput capability of this method will become an important combinatorial materials 
science tool for developing and qualifying new nuclear fuels.","Kerman, Mitchell C.",mitchell.kerman@inl.gov,
Energy,,Idaho National Laboratory,"Data-driven failure diagnosis and 
prognosis of solid-state ceramic 
membrane reactor under harsh 
conditions using deep learning 
technology with internal voltage sensors","This research will investigate in situ the effects of different components on the 
degradation behavior in a solid-state ceramic membrane reactor by embedding 
sensors that will collect current and impedance data during operation. Artificial 
intelligence will be used to understand the large amounts of data and predict reactor 
failure under harsh operating conditions.","Kerman, Mitchell C.",mitchell.kerman@inl.gov,
Energy,,Idaho National Laboratory,"Tailoring the Properties of Multiphase 
Materials Through the Use of 
Correlative Microscopy and Machine 
Learning","This research uses state-of-the-art machine learning (ML) techniques in a new and 
novel manner to identify and correlate the critical microstructural features in a 
multiphase alloy that exhibits high strength and fracture toughness. Experimental data 
will be used to train a convolutional neural network (CNN) in a semi-supervised 
environment to identify key microstructural features and correlate those features with 
the strength and toughness. The resulting machine learning tool can be trained for 
additional microstructural features, different alloys, and/or target mechanical 
properties.","Kerman, Mitchell C.",mitchell.kerman@inl.gov,
Energy,,Idaho National Laboratory,"Microstructurally-driven Framework for 
Optimization of In-core Materials","This research will develop a methodology that relies on mechanism-informed machine 
learning models, rapid ion irradiation and creep testing techniques, and advanced 
characterization coupled with automated image analysis to enable reactor developers 
to quickly understand the complex linkage between alloy composition, 
thermomechanical processing, the resulting microstructure, and swelling and creep 
behavior. This project will (1) develop and demonstrate a high-potential methodology 
for rapid development of future in-core materials and (2) provide critically important 
information on alloy design for optimized swelling and creep behavior to the advanced 
reactor development community.","Kerman, Mitchell C.",mitchell.kerman@inl.gov,
Energy,,Office of Electricity,"The Grid Resilience and Intelligence 
Platform (GRIP)","AI within GRIP is used to develop metrics that quantify the impact of the anticipated 
weather related extreme events. The platform uses utility data combined with physical 
models, distribution power solver  to infer the potential  grid impacts given a major 
storm.",,"Frank, Gregory",gregory.frank@hq.doe.gov
Energy,,Office of Electricity,"Open-Source High-Fidelity Aggregate 
Composite Load Models of Emerging 
Load Behaviors for Large-Scale 
Analysis (GMLC 0064)","1. Machine learning methods such as cross-correlation, random forest, regression 
tree and transfer learning are used to estimate the load composition data and motor 
protection profiles for different climante regions in the Western US
2. Deep learning algorithm is appplied to calibrate the parameters of WECC 
composite load model to match the responses with detailed feeder model",,"Frank, Gregory",gregory.frank@hq.doe.gov
Energy,,Office of Electricity,"Big Data Synchrophasor Monitoring and 
Analytics for Resiliency Tracking 
(BDSMART)","Explore the use of big data, artificial intelligence (AI), and machine learning technology 
and tools on phasor measurement unit (PMU) data to identify and improve existing 
knowledge, and to discover new insights and tools for better grid operation and 
management.",,"Frank, Gregory",gregory.frank@hq.doe.gov
Energy,,Office of Electricity,"Combinatorial Evaluation of Physical 
Feature Engineering and Deep 
Temporal Modeling  for Synchrophasor 
Data at Scale","Explore the use of big data, artificial intelligence (AI), and machine learning technology 
and tools on phasor measurement unit (PMU) data to identify and improve existing 
knowledge, and to discover new insights and tools for better grid operation and 
management.",,"Frank, Gregory",gregory.frank@hq.doe.gov
Energy,,Office of Electricity,MindSynchro,"Explore the use of big data, artificial intelligence (AI), and machine learning technology 
and tools on phasor measurement unit (PMU) data to identify and improve existing 
knowledge, and to discover new insights and tools for better grid operation and 
management.",,"Frank, Gregory",gregory.frank@hq.doe.gov
Energy,,Office of Electricity,"PMU-Based Data Analytics Using 
Digital Twin Phasor Analytics Software","Explore the use of big data, artificial intelligence (AI), and machine learning technology 
and tools on phasor measurement unit (PMU) data to identify and improve existing 
knowledge, and to discover new insights and tools for better grid operation and 
management.",,"Frank, Gregory",gregory.frank@hq.doe.gov
Energy,,Office of Electricity,"A Robust Event Diagnostic Platform: 
Integrating Tensor Analytics and 
Machine Learning Into Real-time Grid 
Monitoring","Explore the use of big data, artificial intelligence (AI), and machine learning technology 
and tools on phasor measurement unit (PMU) data to identify and improve existing 
knowledge, and to discover new insights and tools for better grid operation and 
management.",,"Frank, Gregory",gregory.frank@hq.doe.gov
Energy,,Office of Electricity,"Discovery of Signatures, Anomalies, 
and Precursors in Synchrophasor Data 
with Matrix Profile and Deep Recurrent 
Neural Networks","Explore the use of big data, artificial intelligence (AI), and machine learning technology 
and tools on phasor measurement unit (PMU) data to identify and improve existing 
knowledge, and to discover new insights and tools for better grid operation and 
management.",,"Frank, Gregory",gregory.frank@hq.doe.gov
Energy,,Office of Electricity,"Machine Learning Guided Operational 
Intelligence","Explore the use of big data, artificial intelligence (AI), and machine learning technology 
and tools on phasor measurement unit (PMU) data to identify and improve existing 
knowledge, and to discover new insights and tools for better grid operation and 
management.",,"Frank, Gregory",gregory.frank@hq.doe.gov
Energy,,Office of Electricity,"Robust Learning of Dynamic 
Interactions for Enhancing Power 
System Resilience","Explore the use of big data, artificial intelligence (AI), and machine learning technology 
and tools on phasor measurement unit (PMU) data to identify and improve existing 
knowledge, and to discover new insights and tools for better grid operation and 
management.",,"Frank, Gregory",gregory.frank@hq.doe.gov
EPA,,,Use of random forest model to predict exposure pathways,"Prioritizing the potential risk posed to human health by chemicals requires tools that can estimate exposure from limited information. In this study, chemical structure and physicochemical properties were used to predict the probability that a chemical might be associated with any of four exposure pathways leading from sources-consumer (near-field), dietary, far-field industrial, and far-field pesticide-to the general population. The balanced accuracies of these source-based exposure pathway models range from 73 to 81%, with the error rate for identifying positive chemicals ranging from 17 to 36%. We then used exposure pathways to organize predictions from 13 different exposure models as well as other predictors of human intake rates. We created a consensus, meta-model using the Systematic Empirical Evaluation of Models framework in which the predictors of exposure were combined by pathway and weighted according to predictive ability for chemical intake rates inferred from human biomonitoring data for 114 chemicals. The consensus model yields an R2 of ∼0.8. We extrapolate to predict relevant pathway(s), median intake rate, and credible interval for 479 926 chemicals, mostly with minimal exposure information. This approach identifies 1880 chemicals for which the median population intake rates may exceed 0.1 mg/kg bodyweight/day, while there is 95% confidence that the median intake rate is below 1 μg/kg BW/day for 474572 compounds.
https://pubmed.ncbi.nlm.nih.gov/30516957/",,,
EPA,,,Records Categorization,The records management technology team is using machine learning to predict the retention schedule for records. The machine learning model will be incorporated into a records management application to help users apply retention schedules when they submit new records.,,,
EPA,,,Enforcement Targeting,"EPA’s Office of Compliance, in partnership with the University of Chicago, built a proof-of-concept to improve enforcement of environmental regulations through facility inspections by the EPA and state partners. The resulting predictive analytics showed a 47% improvement of identifying violations of the Resource Conservation and Recovery Act.",,,
GSA,,,Solicitation Review Tool (SRT),"The SRT intakes SAM.gov data for all Information and Communications Technology (ICT) solicitations. The system then compiles the data into a database to be used by machine learning algorithms. The first of these is a Natural Language Processing model that determines if a solicitation contains compliance language. If a solicitation does not have compliance language, then it is marked as non-compliant. Each agency is asked to review their data and validate the SRT predictions. GSA also conducts random manual reviews monthly.",Operation and Maintenance,,
GSA,,,Acquisition Analytics,Takes Detailed Data on transactions and classifies each transaction within the Government-wide Category Management Taxonomy,Operation and Maintenance,,
GSA,,,City Pairs Program Ticket Forecast and Scenario Analysis Tools,"Takes segment-level City Pair Program air travel purchase data and creates near-term forecasts for the current and upcoming fiscal year by month and at various levels of granularity including DOD vs Civilian, Agency, and Region.",Development and Acquisition,,
GSA,,,Category Taxonomy Refinement Using NLP,Uses token extraction from product descriptions more accurately shape intended markets for Product Service Codes (PSCs).,Operation and Maintenance,,
GSA,,,Key KPI Forecasts for GWCM,"Takes monthly historical data for underlying components used to calculate KPIs and creates near-term forecasts for the upcoming fiscal year. Pilot effort focuses on total agency/category spend (the denominator in multiple KPIs). If the pilot program is successful, the same methodology can be extended to other KPIs.",Implementation,,
GSA,,,Service Desk Generic Ticket Classification,"We are building a model to take generic Service Desk tickets and classify them so that they can be automatically re-routed to the correct team that handles these types of tickets. The process of re-routing generic tickets is currently done manually, so the model will allow us to automate it. The initial model will target the top 5 most common ticket types.",Implementation,,
GSA,,,Service Desk Virtual Agent (Curie),"Virtual agent that uses ML to provide predictive results for chat entries. A natural language chatbot (virtual assistant), we named Curie, as part of a multi-model customer service experience for employee's IT service requests leveraging knowledge-based articles.",Operation and Maintenance,,
GSA,,,Contract Acquisition Lifecycle Intelligence (CALI),"CALI tool is an automated machine learning evaluation tool built to streamline the evaluation of vendor proposals against the solicitation requirements to support the Source Selection process. Once the Contracting Officer (CO) has received vendor proposals for a solicitation and is ready to perform the evaluation process, the CO will initiate evaluation by sending solicitation documents along with all associated vendor proposal documents to the Source Selection module, which will pass all documents to CALI. CALI will process the documents, associated metadata and begin analyzing the proposals in four key areas: format compliance, forms validation, reps & certs compliance, and requirements compliance. The designated evaluation members can review the evaluation results in CALI and submit finalized evaluation results back to the Source Selection module. CALI is currently being trained with sample data from the EULAs under the Multiple Award Schedule (MAS) program.",Implementation,,
GSA,,,Classifying Qualitative Data,"USAGov and USAGov en Español collect large amounts of qualitative data from survey comments, web searches and call center chat transcripts. Comments are grouped together by topic to determine where we need to make product updates/enhancements",Operation and Maintenance,,
GSA,,,Chatbot for Federal Acquisition Community,"The introduction of a chatbot will enable the GSA FAS NCSC (National Customer Support Center) to streamline the customer experience process, and automate providing answers to documented commonly asked questions through public facing knowledge articles. The end goal is this will reduce staffing requirements for NCSC’s live chat programs and allow the NCSC resources to be dedicated to other proactive customer services initiatives. Customers will still have the option to connect to a live agent if they choose by requesting an agent.",Operation and Maintenance,,
GSA,,,Document Workflow / Intelligent Data Capture and Extraction,"GSA is driving towards a more accurate and scalable document workflow platform. GSA seeks to intelligently capture, classify, and transfer critical data from unstructured and structured documents, namely PDF files, to the right process, workflow, or decision engine.",Operation and Maintenance,,
GSA,,,IAE FSD CCAI Virtual Agent,The virtual agent uses manual learning to understand customer needs and provide a response appropriately. Our AI is named SAM and uses natural language.,Operation and Maintenance,,
HHS,AHRQ,,Relevancy Tailoring,Adjusting the ranking of search results so that most relevant results show up at the top of the list,,,
HHS,AHRQ,,Auto-generation Synonyms,"Behind the scenes, adding synonyms to search queries to improve search results",,,
HHS,AHRQ,,Automated Suggestions,Auto-filling queries as they are typed,,,
HHS,AHRQ,,Suggested Related Content,"Show related searches that may provide the user with other related, valuable information",,,
HHS,AHRQ,,Auto Tagging,Suggesting content tags automatically based on a machine-driven evaluation of how existing content is tagged,,,
HHS,AHRQ,,Did you mean,Suggesting spelling corrections and reformatted search queries based on Google Analytics data,,,
HHS,AHRQ,,Chatbot,An interactive interface that can respond to plain language queries in real time using natural language processing,,,
HHS,CDC,NCHS,"ICD-10 Coding of Cause of Death reported on
Death Certificates (MedCoder)","MedCoder ICD-10 cause of death codes to the literal text cause of death description provided by the cause of death
certifier on the death certificate. This includes codes for the underlying and contributing causes of death.",,,
HHS,CDC,NCHS,"Item Nonresponse Detection in Open-text
Response Data","NCHS is developing an item nonresponse detection model, to identify cases of item nonresponse (e.g., gibberish,
uncertain/don’t know, refusals, or high-risk) among open-text responses to help improve survey data and question and
Questionnaire design. The system is a Natural Language Processing (NLP) model pre-trained using Contrastive Learning
and fine-tuned on a custom dataset from survey responses.",,,
HHS,CDC,NCHS,"Sequential Coverage Algorithm (SCA) in Record
Linkage","CDC’s National Center for Health Statistics (NCHS) Data Linkage Program has implemented a supervised machine learning
algorithm, known as the Sequential Coverage Algorithm (SCA) in their linkage programs. The SCA was used to develop
Joining methods (or blocking groups) when working with very large datasets. The SCA method improved the efficiency of
blocking.",,,
HHS,CMS,,Chatbot – Voice,"CMS/OSFLO: To assist the CMS Badging Help Desk, this Chatbot (voice) is an automated phone response for general
badging questions allowing help desk personnel to assist employees and contractors with more detailed/larger issues.",,,
HHS,CMS,,Chatbot – Text,"CMS/OSFLO: To assist the Security team, this Chatbot (text) provides an automated email response for general physical
security questions, allowing the help desk team to assist employees and contractors with more in depth issues.",,,
HHS,CMS,,Feedback Analysis Solution (FAS),"The Feedback Analysis Solution is a system that uses CMS or other publicly available data (such as Regulations.Gov) to
review public comments and/or analyze other information from internal and external stakeholders. The FAS uses Natural
Language Processing (NLP) tools to aggregate, sort and identify duplicates to create efficiencies in the comment review
process. FAS also uses machine learning (ML) tools to identify topics, themes and sentiment outputs for the targeted
Dataset.",,,
HHS,CMS,,Predictive Intelligence - Incident Assignment for Quality Service Center (QSC).,"Predictive Intelligence (PI) is used for incident assignment within the Quality Service Center (QSC). The solution runs on
Quality Service Center (QSC).
incidents created from the ServiceNow Service Portal (https://cmsqualitysupport.servicenowservices.com/sp_ess). The
solution analyzes the short description provided by the end user in order to find key words with previously submitted
incidents and assigns the ticket to the appropriate assignment group. This solution is re-trained with the incident data in
our production instance every 3-6 months based on need.",,,
HHS,CMS,,Reasonable Accommodation RPA Bot,"The Bot pulls HR data related to staffing changes, e.g. promotions, reassignments, change in supervisor, and generates
information for action by Reasonable Accommodation staff to ensure disability reasonable accommodations follow the
Employee.",,,
HHS,CMS,,Rapid Authority to Operate (ATO),"The Rapid ATO System was built using a natural language processing model and pipeline to process system security plans,
to identify unique and commonly used technology components used across Federal Information Security Management
Act (FISMA) systems. Natural language processing (NLP) is a form of machine learning that derives intent or subject out
of blocks of text. In this particular case it was used to identify common blocks of language used in similar ways across
system security plan (SSP) documents. In this way, CMS could identify similar approaches to solving certain technology or
process-related control areas within the Acceptable Risk Safeguards (ARS). The output was used to create a list of
components to develop control description language in a re-usable way, as part of the Blueprint/Rapid ATO effort to
streamline SSP generation for new systems.",,,
HHS,CMS,,Data Lake/Load-Extract-Load-Transform (L-ETL),"CMS is using Security Data Lake to modernize the load-extract-load-transform (L-ETL) pipelines and data tooling. CMS will
be enhancing Agency security to bring together more system, telemetry and program data in one place with a unifying
governance model. Building on top of a modern data platform will provide opportunities to experiment with machine
learning model development against this data--solving any number of problems that require decisions to be made about
inferences over time series data. There is no actual ML/AI work being done here today, rather, we are beginning work on
the scaffolding that will open up these opportunities in 1-2 years time.",,,
HHS,CMS,,Priority Score Model - ranks providers within the Fraud Prevention System using logistic regression based on program integrity guidelines.,"Inputs - Medicare Claims data, Targeted Probe and Educate (TPE) Data, Jurisdiction information
Output - ranks providers within the FPS system using logistic regression based on program integrity guidelines.",,,
HHS,CMS,,"Priority Score Timeliness - forecast the time needed to work on an alert produced by Fraud Prevention System (Random Forest, Decision Tree, Gradient Boost, Generalized Linear Regression)","Inputs - Medicare Claims data, TPE Data, Jurisdiction information
Output - forecast the time needed to work on an alert produced by FPS (Random Forest, Decision Tree, Gradient Boost,
Generalized Linear Regression)",,,
HHS,CMS,,Provider Education 90 Day - reviews claims for provider before and after education for statistical change in their claim submission patterns,"Inputs - Medicare Claims data, TPE Data, Jurisdiction information
Output - reviews claims for provider before and after education for statistical change in their claim submission patterns",,,
HHS,FDA,,Advanced Semantic Search and Indexing of Text for Tobacco Applications (ASSIST4Tobacco),"ASSIST4Tobacco is a novel tool that will use semantic indexing to search tobacco authorization applications. The system
will be based on an AI (artificial intelligence)‐based NLP (Natural Language Processing) model which provides deeper
search capabilities using a language model developed to represent relationships between words and concepts within a
body of text.",,,
HHS,FDA,,"Artificial Intelligence-based Deduplication
Algoirthm for Classfication of Duplicate Reports
in the FDA Adverse Event Reports (FAERS)","The deduplication algorithm is applied to nonpublic data in the FDA Adverse Event Reporting System (FAERS) to identify
duplicate reports. Unstructured data in free text FAERS narratives is processed through a natural language processing
system to extract relevant clinical features. Both structured and unstructured data are then used in a probabilistic record
linkage approach to score pairs of reports by evaluating multiple data fields and applying relative weights per field. The
output of potential duplicate reports is further placed in groups to facilitate identification of FAERS reports during case
series evaluation for safety issues of concern.",,,
HHS,FDA,,Opioid Data Warehouse Term Identification Novel Synthetic Opioid Detection and Evaluation Analytics,"The Term Identification and Novel Synthetic Opioid Detection and Evaluation Analytics use publicly available social media
and forensic chemistry data to identify novel referents to drug products in social media text. It uses the FastText library to
create vector models of each known NSO-related term in a large social media corpus, and provides users with similarity
scores and expected prevalence estimates for lists of terms that could be used to enhance future data gathering efforts.",,,
HHS,HRSA,,Electronic Handbooks (EHBs) AI Chatbot ,AI Chatbot • Successfully developed and deployed HRSA EHBs AI Chatbot using Artificial Solutions Teneo platform for external HRSA EHBs grantees • Built to allow grantees to communicate with the EHBs Chatbot using regular natural conversational expressions • Provides knowledge- and action-based responses through a self-service platform with 24/7 availability • Integrated with existing EHBs application UI and Salesforce for automated ticket creation • Chatbot has the ability to refine and increase the accuracy of its responses as more and more users invoke/use the Chatbot,,,
HHS,HRSA,,BHW Community Need Analysis Platform,"The first use case being developed is for primary care with behavioral health integration which uses a machine learning
based automated clustering engine. The development of this tool allows for BHW to dynamically assess the healthcare
need of a population given a specific use case and relevant datasets. The output of the model will be used as part of the
Notice of Funding Opportunity (NOFO) grant proposal evaluation process.",,,
HHS,NIH,NHLBI,"Leveraging AI/ML for classification and
categorization of scientific concepts","Topical characterization of the research portfolio. Inputs are publications and grants abstracts. These are fed into a text
classification model and concept extraction. The outputs are category labels and list of concepts.",,,
HHS,NIH,NIEHS,"Grant Application Subject-Matter Classification
Tool",Natural language processing of grant applications for the purpose of classification for review assignment,,,
HHS,NIH,NIEHS,Splunk IT System Monitoring Software,"Splunk utilizes machine learning to aggregate system logs from IT infrastructure systems and endpoints for auditing and
monitoring purposes",,,
HHS,NIH,NIEHS,"COVID-19 Pandemic Vulnerability Index
Dashboard","The dashboard creates risk profiles, called PVI scorecards, for every county in the United States, continuously updated
with the latest data that summarize and visualize overall disease risk.",,,
HHS,NIH,NIGMS,"National Institute of General Medical Sciences
(NIGMS) AI Supported Searches, Information
Systems and Tools
System Acronym: NIGMS ASSIST)","NIGMS program staff often need information that is available through IMPAC or QVR to perform their daily tasks. In order
to provide such information, DIMA and IRMB have collaborated to develop functions that utilize artificial intelligence and
natural language processing methods to produce data relevant to the program staff’s mission. These tools are collected
into a single system to make them available to the NIGMS community for use on a day-to-day basis. ASSIST provides a
secure interface supported by Oracle, SQL server and Python analytics. The individual components of ASSIST provide the
following functions:
- FLIP module (Development), provides the ability to identify investigators by PPID from Federal RePORTER based on user
input of investigator PPIDs.
- TPAL module (Production), provides the ability to lookup potential matching program officers, including their
corresponding predicted Program Area Codes, and ICs based on the input of unstructured scientific data.",,,
HHS,NIH,NIGMS,Leveraging AI for Business Process Automation,"NIGMS has developed a method to automate the initial referral of grant applications to the proper scientific expertise
within the Institute using Natural Language Processing and Machine Learning. NIGMS IRMB and DIMA are currently using
this NLP/ML algorithm developed in R statistical software to parse grant applications and to determine Project Officer
candidates for grant assignment. This process was previously fully manual and required a substantial person hour effort.
NIGMS has collaborated with the Electronic Records Administration group to incorporate this technique into the Internal
Referral Module, and the tool is now available to be adapted for broader use across the NIH.",,,
HHS,NIH,NLM,"Pangolin lineage classifications to support
accessing and analysis of SARS-CoV-2 sequence
Data.","The Pango nomenclature, called Pango lineages, is being used by researchers and public health agencies worldwide to
track the transmission and spread of SARS-CoV-2, including variants of concern. The requirements for running the tool
include having conda on a MacOS or Linux system, and the FASTA-formated sequence data. There are 2 methods for
lineage assignment with Pango; within NCBI Virus we use the process which includes PangoLEARN, where a classification
tree is used to group similar sequences.",,,
HHS,NIH,NLM,"Providing MeSH Check Tag of NLM’s Medical
Text Indexer (MTI) ons using Support Vector
Machines (SVM)","Titles and abstracts from MEDLINE Citations are provided through SVM machine learning algorithm provides confidence
scores for a set of MeSH CheckTags to the NLM Medical Text Indexer (MTI) program. These CheckTags are small set of
MeSH Descriptors designed to indicate Species, Sex, and Age in MEDLINE articles.",,,
HHS,NIH,NLM,Determining selection for indexing MEDLINE articles using Neural Network Architecture with a Convolutional Neural Network (CNN),"Using the article title, abstract, journal, publication year, and indexing year of indexed and non-indexed articles that were
submitted to MEDLINE in 2018, methods to automate the selection of indexed articles was researched. A classifier was
developed that combines the predictions of many traditional machine learning algorithms and a Convolutional Neural
Network (CNN). The final classification layer uses a sigmoid activation function to generate a single output value between
zero and one, which can be interpreted as the probability of an article being in-scope for MEDLINE.",,,
HHS,NIH,NLM,MetaMap to identity potential terms for indexing MEDLINE articles,"MetaMap is a widely available program providing access from biomedical text to the concepts in the unified medical
language system (UMLS) Metathesaurus. MetaMap uses NLP to provide a link between the text of biomedical literature
and the knowledge, including synonymy relationships, embedded in the Metathesaurus. The flexible architecture in
which to explore mapping strategies and their application are made available. MTI uses the MetaMap to generate
potential indexing terms.",,,
HHS,NIH,NLM,Best Match: New relevance search for PubMed,"PubMed is a free search engine for biomedical literature accessed by millions of users from around the world each day.
With the rapid growth of biomedical literature, finding and retrieving the most relevant papers for a given query is
increasingly challenging. We have developed Best Match, a new relevance search algorithm for PubMed that leverages
the intelligence of our users and cutting-edge machine-learning technology as an alternative to the traditional date sort
order. The Best Match algorithm is trained with past user searches with dozens of relevance-ranking signals (factors) and
demonstrates state-of-the-art retrieval performance in benchmarking experiments as well as an improved user
experience in real-world testing.",,,
HHS,NIH,NLM,"SingleCite: Improving single citation search in
PubMed","A search that is targeted at finding a specific document in databases is called a Single Citation search, which is particularly
important for scholarly databases, such as PubMed, because it is a typical information need of the users. We have
developed SingleCite, an automated algorithm that establishes a query-document mapping by building a regression
function to predict the probability of a retrieved document being the target based on three variables: the score of the
highest scoring retrieved document, the difference in score between the two top retrieved documents, and the fraction
of a query matched by the candidate citation. SingleCite shows superior performance in benchmarking experiments and
is applied to rescue queries that would fail otherwise.",,,
HHS,NIH,NLM,"Computed Author: author name disambiguation National Institutes of Health (NIH) NLM
for PubMed","PubMed users frequently use author names in queries for retrieving scientific literature. However, author name
ambiguity (different authors share the same name) may lead to irrelevant retrieval results. Thus we have developed a
machine-learning method to score the features for disambiguating a pair of papers with ambiguous names.
Subsequently, agglomerative clustering is employed to collect all papers belong to the same authors from those classified
pairs. Disambiguation performance is evaluated with manual verification of random samples of pairs from clustering
results, with a higher accuracy than other state-of-the-art methods. It has been integrated into PubMed to facilitate
author name searches.",,,
HHS,NIH,NLM,National Library of Medicine NLM-Gene: towards automatic gene indexing in PubMed articles,"Gene indexing is part of the NLM’s MEDLINE citation indexing efforts for improving literature retrieval and information
access. Currently, gene indexing is performed manually by expert indexers. To assist this time-consuming and resource-
intensive process, we have developed NLM-Gene, an automatic tool for finding gene names in the biomedical literature
using advanced natural language processing and deep learning methods. Its performance has been assessed on gold-
standard evaluation datasets and is to be integrated into the production MEDLINE indexing pipeline.  ",,,
HHS,NIH,NLM,National Library of Medicine NLM-Chem: towards automatic chemical indexing in PubMed articles,"Chemical indexing is part of the NLM’s MEDLINE citation indexing efforts for improving literature retrieval and
information access. Currently, chemcial indexing is performed manually by expert indexers. To assist this time-consuming
and resource-intensive process, we have developed NLM-Chem, an automatic tool for finding chemical names in the
biomedical literature using advanced natural language processing and deep learning methods. Its performance has been
assessed on gold-standard evaluation datasets and is to be integrated into the production MEDLINE indexing pipeline.  ",,,
HHS,NIH,OER,Program Class Code (Area of Science) Referral for NIAID,"The REFERRAL GROUP of Referral, Program Analysis Branch (RPAB) is responsible for program assignments for all
research, training, career, and fellowship grant applications submitted to NIAID, from CSR. The NIAID Program Class Code
classification AI project evaluates the projects that are in RAPB and auto assigns these grant applications to the Program
Class Codes. The inputs are comprised of approximately 6,000+ grant applications that are currently manually assigned
by RPAB Staff. The output would be grant applications that are categorized into their respective PCC's.",,,
HHS,NIH,OER,"Research, Condition, and Disease Categorization (RCDC)","RCDC is an electronic budget reporting tool that categorizes projects using AI/NLP. The inputs are grant applications,
R&D contracts, intramural projects, inter agency agreements. The RCDC Fingerprinting process identifies concepts in the
extracted text from the source project, person or publication. The text is normalized, concepts are extracted, concepts
and synonyms are matched to the RCDC thesaurus. A rank is applied based on the frequency of occurrence of the
concepts within the text. Project fingerprints are sourced from the application description text (Title, Abstract and
Specific Aims). Titles and abstracts provide the source of scientific concepts for publications. The system then outputs the
projects into their respective areas of science.",,,
HHS,NIH,OER,Query View Report (QVR) LIKE,"The LIKE feature in QVR makes use of the NIH Research, Condition and Disease Categorization (RCDC) indexing results to
compare scientific terms associated with a project, person or publication and find scientifically similar projects, persons
or publications.",,,
HHS,NIH,OER,Internal Referral Module (IRM) NLP,"The IRM NLP module automatically refers projects to Program Officers once the grant application is received. The system
inputs are grant applications - the title, abstract, specific aims and Public Health Relevance is analyzed to automatically
refer the grant application to the Program Officer who matches a similar background with the science contained in the
applications. This process, is operating at a high accuracy rate and has effectively eliminated the referral bottleneck.",,,
HHS,NIH,OER,NIH Grants Virtual Assistant,Chat Bot to assist users in finding grant related information via OER resources,,,
HHS,NIH,OIG,Grants Analytics Portal,"The Grants Analytics Portal uses AI to enhance HHS OIG staff’s ability to access grants related data quickly and easily by:
quickly navigating directly to the text of relevant findings across thousands of audits, the ability to discover similar
findings, analyze trends, compare data between OPDIVs, and the means to see preliminary assessments of potential
anomalies between grantees.",,,
HHS,NIH,OIG,Text Analytics Portal,"The text analytics portal allows personnel without an analytics background to quickly examine text documents through a
related set of search, topic modeling and entity recognition technologies",,,
HHS,NIH,OPA,Machine learning system to predict translational progress in biomedical research,"Fundamental scientific advances can take decades to translate into improvements in human health. Shortening this
interval would increase the rate at which scientific discoveries lead to successful treatment of human disease. One way to
accomplish this would be to identify which advances in knowledge are most likely to translate into clinical research.
Toward that end, the NIH Office of Portfolio Analysis built a machine learning system that detects whether a paper is
likely to be cited by a future clinical trial or guideline. Despite the noisiness of citation dynamics, as little as 2 years of
postpublication data yield accurate predictions about a paper’s eventual citation by a clinical article (accuracy = 84%, F1
score = 0.56; compared to 19% accuracy by chance). We found that distinct knowledge flow trajectories are linked to
papers that either succeed or fail to influence clinical research. Translational progress in biomedicine can therefore be
assessed and predicted in real time based on information conveyed by the scientific community’s early reaction to a
paper. For more information see the publication describing this system: Hutchins et al 2019
(https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000416)",,,
HHS,NIH,OPA,"Semantic analysis of scientific documents
(word2vec OPA )","The NIH Office of Portfolio Analysis has developed a neural network approach to analysis of scientific content using
dimensionality reduction (word2vec OPA ). This method computationally converts words in scientific texts to numbers and
summarizes documents by their semantic content by learning relationships between words from their context. This
method is adaptable to specific corpora, including grants and scientific articles. For more information see the publication
describing our word2vec approach: Hoppe et al 2019 (https://www.science.org/doi/10.1126/sciadv.aaw7238)",,,
HHS,NIH,OPA,Person-level disambiguation for PubMed authors and NIH grant applicants,"High-quality disambiguation is required to correctly link researchers to their grants and outputs including articles,
patents, and clinical trials. The NIH Office of Portfolio Analysis developed a disambiguation solution that used article level
metadata to assign 24.5M unique papers from the PubMed database to 16.0M unique author names, then used a novel
neural network model trained on ORCID identifiers to determine whether author-publication pairs refer to variant
representations of the same person. For example, our model can determine whether hypothetical records listing Jane
Smith and Jane M. Smith were the same person, or two different people, based on variables that include institutional
affiliation, co-authorship, and article-affiliated Medical Subject Heading (MeSH) terms. For more information see the
publication describing this method: Yu et al 2021
(https://www.biorxiv.org/content/10.1101/2021.02.02.429450v1.full.pdf)",,,
Homeland Security,Customs and Border Protection,,AI Curated Synthetic Data,"AI Curated Synthetic Data creates synthetic data for computer vision to enable more capable and ethical AI when detecting anomalies in complex environments.

Specifically, it creates an emulated X-ray sensor that can produce visually realistic synthetic X-ray scan images similar to real X-ray scan images, and virtual 3D Assets of vehicles and narcotics containers. These images will be used to enhance the development of Anomaly Detection Algorithms for Non-Intrusive Inspection, incorporating AI/ML for the detection of narcotics and other contraband in conveyances and cargo.",Initiation,,
Homeland Security,Customs and Border Protection,,AI for Autonomous Situational Awareness,"The AI for autonomous situational awareness system is intended to use IoT sensor kits to covertly detect and track illicit cross-border traffic in remote locations. 

The system will leverage a motion image/video system enhanced with Artificial Intelligence that is capable of vehicle detection and direction determination. It will also incorporate a motion sensor that, when triggered, wakes up a high-resolution camera to capture a series of pictures, with additional sensors providing confirmation prior to camera capture. 

Images captured will be processed by Artificial Intelligence models to classify objects, determine vehicle direction at intersections, and provide imagery sufficient for re-identification. Ultimately, the systems is intended to create a low footprint, low cost, low power system to provide situational awareness and covert detection.",Development and Acquisition,,
Homeland Security,Customs and Border Protection,,Automated Item of Interest Detection - ICAD,"The software analyzes photographs that are taken by field imaging equipment, which are then fed into the ICAD system for review by USBP agents and personnel. The Matroid software currently processes and annotates images using proprietary software to determine if any of the images contain human subjects.

Matroid is the name of the Video Computer Aided Detection system used by CBP. It uses trained computer vision models that recognize objects, people, and events in any image or video stream. Once a detector is trained, it can monitor streaming video in real time, or efficiently search through pre-recorded video data or images to identify objects, people, and events of interest. 

The intent for the ICAD system is to expand the models used to vehicles, and subjects with long-arm rifles, while excluding items of little or no interest such as animals.",Operation and Maintenance,,
Homeland Security,Customs and Border Protection,,Autonomous Aerostat,"Aerostat capability that uses three tethers instead of the traditional single tether, coupled with advanced weather sensors, analytic capabilities, and powerful winches. The AI/ML model is used to detect the need to launch and land based on weather. It also leverages AI and robotics to autonomously launch and recover the aerostat during inclement weather events without the need for on-site staffing, allowing the aerostat to operate autonomously, saving time and manpower.",Development and Acquisition,,
Homeland Security,Customs and Border Protection,,Autonomous Maritime Awareness,"The Autonomous Maritime Awareness system combines surveillance towers, ocean data solutions, unmanned autonomous surface vehicles (ASV), and AI to autonomously detect, identify, and track items of interest in a maritime environment.

The towers are low-cost, customizable, and relocatable surveillance systems. They are equipped with a suite of radars and day/night camera sensors. The ASVs have been ruggedized for the open ocean and are powered by wind, solar, and/or onboard engine as required, allowing them to operate in an area of responsibility (AOR) for up to 12 months. Their sensor suite includes cameras and radar. 

Both systems use AI/ML to detect and identify objects, determine items of interest (IoI) and autonomously track those items using their sensor suites. Once identified, these systems can send alerts to monitoring agencies for at-sea interdictions of potential targets and/or intel collections.",Development and Acquisition,,
Homeland Security,Customs and Border Protection,,Autonomous Surveillance Towers (Anduril),"Autonomously Detects, Identifies, and Tracks items of interest using Artificial Intelligence integrated with the tower. It does not require a dedicated operator, is rapidly deployable, and is relocatable in less than a day by 2-3 people.

The system features a hybrid command and control capability, hosted in the government cloud, and is accessible via URL by desktop, laptop, tablet, or Smartphone. It is solar powered with battery backup and requires no accompanying physical infrastructure while providing visibility for 1.5 miles (2.4 km) for people, 3 miles (4.8km) for vehicles.

The Lattice system permits autonomous detection, identification, and tracking of Items of Interest (IoIs).  The tower scans constantly and autonomously.  The radar detects and recognizes movement. The camera slews autonomously to the IoI and the system software identifies the object.  The system alerts the user and autonomously tracks the IoI. End users can monitor the system and see near real time photos by logging into the User Interface on any CBP device. ",Operation and Maintenance,,
Homeland Security,Customs and Border Protection,,Data and Entity Resolution,"Automates data unification and entity resolution with a high level of trust at enterprise scale and speed.

Data and Entity Resolution uses Machine Learning modeling to ingest multiple data sources and develop models that associate disparate records to identify probable connections, unique entities, and/or identify commonalities between multiple independently submitted records.

The automation of entity resolution within the models is supported by a tool that enables non-technical end users to continuously train models through a user-friendly interface. ",Operation and Maintenance,,
Homeland Security,Customs and Border Protection,,Entity Resolution,"The third-party global trade data is used to augment and enrich agency’s investigations into entities of interest. It combines data from companies and goods across multiple languages, then provides network analysis to assess trade flows and risks associated with cross-border trade.

This can validate agency-held information or provide better understanding of networks of interest to the agency to better inform investigations that cross borders. AI/ML models help manage the information provided through the software, including behind-the-curtain collection of information, structuring of data, entity resolution, network analysis, risk analysis, and other functions that contribute to the software knowledge graph and frontend that end users interact with.  ",Development and Acquisition,,
Homeland Security,Customs and Border Protection,,Geospatial imagery utilizing annotation,"Leverages a commercial constellation of Synthetic Aperture Radar (SAR) satellites with readily available data, capable of imaging any location on Earth, day, and night, regardless of cloud cover. 

Utilizes AI, including machine vision, object, detection, object recognition, and annotation to detect airframes, military vehicles, and marine vessels, as well as built-in change detection capabilities for disaster response missions.",Development and Acquisition,,
Homeland Security,Customs and Border Protection,,Integrated Digital Environment,"The Integrated Digital Environment provides managers with a better understanding of end user workflows, most and least used applications, and opportunities for improvement. 

The AI/ML model applies to end user activity data (e.g., use of applications, flow between applications) to help CBP identify opportunities for more efficient or effective configuration of interfaces, use of resources, or development and deployment of CBP’s applications.  It tailors analytics and insight generation to allow metrics gathering, usage recording/observation, dashboarding, and workflow experimentations/suggestions to support analysts utilizing the entire suite of agency and open-source data systems. It also customizes existing capabilities to allow the exact automations needed for agency applications and systems, creating an integrated digital environment for greater connectivity and security between applications, and better ability for CBP administrators to manage and optimize use of applications by end users.",Development and Acquisition,,
Homeland Security,Customs and Border Protection,,RVSS Legacy Overhauled System Project (INVNT),"Video Computer Aided Detection (VCAD) (also known as Matroid AI) is software that enables CBP end users to create and share vision detectors. 

VCAD detectors are trained computer vision models that recognize objects, people, and events in any image or video stream. Once a detector is trained, it can monitor streaming video in real time, or efficiently search through pre-recorded video data or images to identify objects, people, and events of interest. 

Users can view detection information via a variety of reports and alert notifications to process and identify important events and trends. Detection data is also available through VCAD's powerful developer Application Programming Interface (API) and language specific clients, so CBP applications can be integrated with the power of computer vision.",Deployment,,
Homeland Security,Customs and Border Protection,,Use of technology to identify proof of life,"The Use of technology to identify proof of life, or ""Liveness Detection,"" uses Artificial Intelligence to reduce fraudulent activity, primarily for use within the CBP One app.

The CBP One app is designed to provide the public with a single portal to a variety of CBP services. It includes different functionality for travelers, importers, brokers, carriers, International Organizations, and other entities under a single consolidated log-in, and uses guided questions to help users determine the correct services, forms, or applications needed.

The Liveness Detection component used by the authentication system for the CBP One app uses the user's mobile device camera in addition to Artificial Intelligence algorithms to determine if the face presented to the app is the person in front of the camera at the time of capture and not a photo, mask, or other spoofing mechanism. Being able to accept submitted data with confidence that the submitting individual is who and where they claim to be is critical to the functionality of the app within the agency environment. ",Development and Acquisition,,
Homeland Security,Customs and Border Protection,,Vessel Detection,"Integrated technologies and analytics enhance maritime detection and the sensor network. Machine-assisted and AI-enhanced detection and tracking allows for improved illicit vessel detection in areas with high volumes of legitimate trade and recreational water vessel traffic by increasing situational awareness and responsiveness to threats.

Vessel Detection allows an agent to set a search area with criteria (e.g., people, drones, vehicles) and transmit that criteria to the sensors.  Images detected by the sensors are auto-recognized using Artificial Intelligence. The AI algorithms filter, detect, and recognize objects and divides them into Items of Interest (IoI) and ""other"" objects. 

Detections of IoI are shared with other detection systems while detections of other objects (e.g., animals) are not shared. IoIs can be tracked and maintained across multiple sensors seamlessly.",Development and Acquisition,,
Homeland Security,Cybersecurity and Infrastructure Security Agency,,Advanced Analytic Enabled Forensic Investigation,"CISA deploys forensic specialists to analyze cyber events at Federal Civilian Executive Branch (FCEB) departments and agencies, as well as other State, Local, Tribal, Territorial, and Critical Infrastructure partners. Forensic analysts can utilize advanced analytic tooling, in the form of Artificial Intelligence implementations to better understand anomalies and potential threats. This tooling allows forensic specialists the capabilities to comb through data in an automated fashion with mathematically and probabilistically based models to ensure high fidelity anomalies are detected in a timely manner. ",Initiation ,,
Homeland Security,Cybersecurity and Infrastructure Security Agency,,Advanced Network Anomaly Alerting,"Threat hunting and Security Operations Center (SOC) analysts are provided terabytes per day of data from the National Cybersecurity Protection System's (NCPS) Einstein sensors. Manually developed detection alerts and automatic correlation via off the shelf tooling are common, but not comprehensive. Many network attacks can be probabilistically determined given sufficient training data and time. Analysts use automated tooling to further refine the alerts they receive and produce additional automated alerts based on aggregated information and backed in subject matter expertise. This tooling allows CISA analysts the capabilities to comb through data in an automated fashion with mathematically and probabilistically based models to ensure high fidelity anomalies are detected in a timely manner. ",Initiation ,,
Homeland Security,Cybersecurity and Infrastructure Security Agency,,AI Security and Robustness,"Frameworks, processes, and testing tools developed to govern the acquisition, development, deployment, and maintenance of AI technologies. Technology integrators within CISA as well as the rest of the federal enterprise use AI-enhanced tools to assure the trustworthy, robust, and secure operation of their AI systems. These tools use Machine Learning and Natural Language Processing to enhance the assessment of AI technology within the agency by speeding up data processing.",Initiation ,,
Homeland Security,Cybersecurity and Infrastructure Security Agency,,AIS Scoring and Feedback,"AIS Automated Scoring & Feedback (AS&F) is uses descriptive analytics from organizational-centric intelligence to support confidence and opinion/reputation classification of indicators of compromise (IOCs). Looking at an indicator AS&F determines if the indicator is present in known-good list by cross-referencing organizational-centric intelligence data of known non-malicious/benign indicators and classifies accordingly if true. If not a known-good, determine if there are sightings of the indicator by cross-referencing organizational-centric intelligence and classify accordingly if true. If there are no sightings for the indicator, determine if this indicator has been verified by an analyst within our organizational-centric intelligence and classify accordingly if true. Lastly if the indicator has not been verified by an analyst, AS&F determines whether there are other reports within our organizational-centric intelligence about this indicator and classifies accordingly. AIS participants can triage against the populated opinion and/or confidence values to identify Indicator objects meeting or exceeding designated criteria and filter out the remaining data. AIS participants may also find value in utilizing the confidence score (if present) and the opinion value to understand whether any difference between the publisher and other organizations exists. Together, these enrichments can help those receiving information from AIS prioritize actioning and investigating Indicator objects.",Operation and Maintenance,,
Homeland Security,Cybersecurity and Infrastructure Security Agency,,Automated Indicator Sharing (AIS) Automated PII Detection,"The Automated PII Detection and Human Review Process incorporates descriptive, predictive, and prescriptive analytics. Automated PII Detection leverages natural language processing (NLP) tasks including named entity recognition (NER) coupled with Privacy guidance thresholds to automatically detect potential PII from within AIS submissions. If submissions are flagged for possible PII, the submission will be queued for human review where the analysts will be provided with the submission and AI-assisted guidance to the specific PII concerns. Within Human Review, analysts are able to confirm/deny proper identification of PII and redact the information (if needed). Privacy experts are also able to review the actions of the system and analysts to ensure proper performance of the entire process along with providing feedback to the system and analysts for process improvements (if needed). The system learns from feedback from the analysts and Privacy experts. Through the incorporation of the automated PII detection, CISA fully compliances with Privacy, Civil Rights and Civil Liberties requirements of CISA 2015 and scaled analyst review of submissions by removing false positives and providing guidance to submission to be reviewed. Through continual audits CISA will maintain integrity and trust in system and human processes.",Operation and Maintenance,,
Homeland Security,Cybersecurity and Infrastructure Security Agency,,Critical Infrastructure Anomaly Alerting,"The Cyber Sentry program provides monitoring of critical infrastructure networks. Within the program, threat hunting analysts require advanced anomaly detection and machine learning capabilities to examine multimodal cyber-physical data on IT and OT networks, including ICS/SCADA. The Critical Infrastructure Anomaly Alerting model provides AI-assistance in processing this information.",Initiation ,,
Homeland Security,Cybersecurity and Infrastructure Security Agency,,Cyber Incident Reporting,Cyber incident handling specialists utilize advanced automation tools to process data received through various threat intelligence and cyber incident channels. These tools leverage Machine Learning and Natural Language Processing to increase the accuracy and relevance of data that is filtered and presented to human analysts and decision-makers. Machine Learning techniques also assist to aggregate the information in reports for presentation and further analysis. This includes data received through covered CIRCIA entities.,Initiation,,
Homeland Security,Cybersecurity and Infrastructure Security Agency,,Cyber Threat Intelligence Feed Correlation,"Cyber Threat Intelligence Feed Correlation uses AI enabled capabilities to provide accelerated correlation across multiple incoming information feeds. This enables more timely enrichment to improve the externally shared information feeds. AI allows the algorithm to use the information items and results to learn most efficient ways to perform the task. Additionally, tailored algorithms could be created to provided sustained surveillance of threat actor TTPs.
",Initiation,,
Homeland Security,Cybersecurity and Infrastructure Security Agency,,Cyber Vulnerability Reporting,"Vulnerability analysts require advanced automation tools to process data received through various  vulnerability reporting channels, as well as aggregate the information for automated sharing. These tools leverage Machine Learning and Natural Language Processing to increase the accuracy and relevance of data that is filtered and presented to human analysts and decision-makers. Machine Learning techniques also assist to aggregate the information in reports for presentation and further analysis. This includes data in the KEV and CVE databases.",Initiation ,,
Homeland Security,Cybersecurity and Infrastructure Security Agency,,Malware Reverse Engineering,"Reverse engineering of malware, and software analysis more broadly, will continue to be a critical activity in support of CISA’s cyber defense mission. Threat Focused Reverse Engineering (TFRE) leverages advanced engineering, formal methods, and deep learning techniques for better cyber threat intelligence. Without scalable, automated tools, it is difficult to disrupt sophisticated adversaries’ malware development lifecycle. New, unique, automated techniques are needed to better target adversaries, augment analysts, and create sophisticated tools for end users. Core tools disrupt the adversary’s development lifecycle by exposing tactics, techniques, and procedures (TTPs). Analysts could spend more time and energy to hunt/takedown threats; adversaries can spend less time operating malware and must commit more resources to reorient. TFRE consists of a broader development pipeline providing tool hardening, enhanced computational abilities, understanding of deployment environments, and other important capabilities.",Initiation ,,
Homeland Security,Cybersecurity and Infrastructure Security Agency,,Operational Activities Explorer,"Duty officers and analysts in CISA's Operations Center use a dashboard powered by artificial intelligence to enable sensemaking of ongoing operational activities. Artificial intelligence uses new near-real-time event data (from open source reporting, partner reporting, CISA regional staff, and cybersecurity sensors) coupled with historical cybersecurity and infrastructure security information and previous operational response activity to recommend courses-of-action and engagement strategies with other government entities and critical infrastructure owners and operators based on potential impacts to the National Critical Functions.",Initiation,,
Homeland Security,Cybersecurity and Infrastructure Security Agency,,Security Information and Event Management (SIEM) Alerting Models,"Threat hunting and Security Operations Center (SOC) analysts are provided terabytes per day of log data. Manually developed detection alerts and automatic correlation in Security Information and Event Management tool are common, but not comprehensive. Many cyber attacks can be probabilistically determined given sufficient training data and time. Analysts  use automated tooling to further refine the alerts they receive and produce additional automated alerts based on aggregated information and curated subject matter expertise. This tooling allows CISA analysts the capabilities to comb through data in an automated fashion with mathematically and probabilistically based models to ensure high fidelity anomalies are detected in a timely manner. ",Initiation ,,
Homeland Security,HQ,,Text Analytics for Survey Responses (TASR),Text Analytics for Survey Responses (TASR) is an application for performing Natural Language Processing (NLP) and text analytics on survey responses. It is currently being applied by DHS OCHCO to analyze and extract significant topics/themes from unstructured text responses to open-ended questions in the quarterly DHS Pulse Surveys. Results of extracted topics/themes are provided to DHS Leadership to better inform agency-wide efforts to meet employees’ basic needs and improve job satisfaction,Operation and Maintenance,,
Homeland Security,"HQ Enforcement, Intelligence and Analysis, Science and Technology",,RelativityOne,"RelativityOne is a document review platform used to gain efficiencies in document review in litigation, FOIA, and other arenas where large-scale document review and production is necessary.",Operation and Maintenance,,
Homeland Security,Immigration and Customs Enforcement,,Normalization Services,"HSI uses Artificial Intelligence to verify, validate, correct, and normalize addresses, phone numbers, names, and ID numbers to streamline the process of correcting data entry errors, point out purposeful misidentification, connect information about a person across HSI datasets, and cut down the number of resource hours needed for investigations. 

Examples of the normalization services provided include: normalizing less well-defined addresses into usable addresses for analysis- (such as those using mile markers instead of a street number); inferring ID type based on user-provided ID value (such as distinguishing a SSN from a DL number without additional context); categorizing name parts while taking into account additional factors (including generational suffixes and multi-part family names); and validating and normalizing phone numbers to the E164 standard, including their identified county of origin.

These services are provided as part of the Repository for Analytics in a Virtualized Environment (RAVEn).  RAVEn is a DHS HSI Innovation Lab project that facilitates large, complex analytical projects to support ICE’s mission to enforce and investigate violations of U.S. criminal, civil, and administrative laws. RAVEn also enables tools used to analyze trends and isolate criminal patterns as HSI mission needs arise. For more information, please read the DHS/ICE/PIA-055 - Privacy Impact Assessment 055 for RAVEn.",Operation and Maintenance,,
Homeland Security,Immigration and Customs Enforcement,,"Machine Translation
(Previously Language Translator)","Systran provides machine translation for over 100 different language combinations.  Currently the Innovation Lab has licenses for translating Chinese, Spanish, Arabic, Farsi, Russian, German, Ukrainian and Filipino to English.  Systran can translate plain text, word documents, and PDFS.  A web-based UI and API endpoint are available.",Operation and Maintenance,,
Homeland Security,Immigration and Customs Enforcement,,Email Analytics ,"The Email Analytics application enables a user to review and analyze email data acquired through legal process.  AI is incorporated to accomplish spam message classification, and named entity recognition (NER) for entity extraction of names, organizations, locations, etc.  It also integrates machine translation capabilities using a commercial product.",Implementation,,
Homeland Security,Immigration and Customs Enforcement,,Mobile Device Analytics,"Mobile Device Analytics (MDA) has been developed to meet the demand on investigators to view and analyze massive amounts of data resulting from court ordered mobile device extractions.  The overarching goal of MDA is to improve the efficacy of agents and analysts in identifying pertinent evidence, relationships, and criminal networks from data extracted from cellular phones. Machine Learning is being developed for object detection (such as firearms, drugs, money, etc.) in photos and videos contained in the data.

This is a DHS HSI Innovation Lab / RAVEn project. The Repository for Analytics in a Virtualized Environment (RAVEn) facilitates large, complex analytical projects to support ICE’s mission to enforce and investigate violations of U.S. criminal, civil, and administrative laws. RAVEn also enables tools used to analyze trends and isolate criminal patterns as HSI mission needs arise. For more information, please read the DHS/ICE/PIA-055 - Privacy Impact Assessment 055 for RAVEn.",Development and Acquisition,,
Homeland Security,Immigration and Customs Enforcement,,Barcode Scanner ,"The Barcode Scanner has been developed to scan and populate detected information into corresponding text fields within the RAVEn GO's Encounter Card. The barcode scanner currently supports MRZ and PDF417 barcode types, frequently found on travel documents (Passport and Passport cards) and US Driver's Licenses. 

This is a DHS HSI Innovation Lab / RAVEn project. The Repository for Analytics in a Virtualized Environment (RAVEn) facilitates large, complex analytical projects to support ICE’s mission to enforce and investigate violations of U.S. criminal, civil, and administrative laws. RAVEn also enables tools used to analyze trends and isolate criminal patterns as HSI mission needs arise. For more information, please read the DHS/ICE/PIA-055 - Privacy Impact Assessment 055 for RAVEn.",Operation and Maintenance,,
Homeland Security,Immigration and Customs Enforcement,,Facial Recognition Service ,"The Facial Recognition Service is used during investigations conducted by HSI agents and analysts for identification of known individuals, as well as extracting faces for further investigations from perpetrators including child exploitation offenses, human rights atrocities, and war criminals.

This is a DHS HSI Innovation Lab / RAVEn project. The Repository for Analytics in a Virtualized Environment (RAVEn) facilitates large, complex analytical projects to support ICE’s mission to enforce and investigate violations of U.S. criminal, civil, and administrative laws. RAVEn also enables tools used to analyze trends and isolate criminal patterns as HSI mission needs arise. For more information, please read the DHS/ICE/PIA-055 - Privacy Impact Assessment 055 for RAVEn.",Operation and Maintenance,,
Homeland Security,United States Citizenship and Immigration Services,,I-485 Family Matching,"I-485 Family Matching is designed to create models to match family members to underlying I-485 petitions. The underlying immigrant petition defines if the I-485 is employment-based or family-based. It also has information about the visa classification and priority date which, when compared against the Department of State’s monthly Visa Bulletin, helps predict visa usage. It is difficult to match an I-485 to its underlying immigrant petition, because the only available field on which to match is the A-number. This number is not always present on the immigrant petition, and name/date of birth matching is not as reliable. The goal of I-485 Family Matching is to leverage AI to more confidently create connections between petitioners and their families based on limited data.

Additionally, it will be able to help identify and group I485s filed by family members, as well as gather up the many ancillary forms they may have pending (such as I765, I131). Similar to immigrant petition matching, it can be difficult to match up I485s filed by family members. In these cases the only similar fields are a common address. Efforts have been made in the past to identify family members by address, but it is effective only to a point. The AI model will help make working with this data more reliable, as well as group individual petitioners, their families, and other helpful associated data together for faster and more accurate processing.",Development and Acquisition,,
Homeland Security,United States Citizenship and Immigration Services,,I-539 approval prediction,"This project attempts to train and build a machine learning throughput analysis model to predict when an I-539 ""Application to Extend or Change Nonimmigrant Status"" case will be approved through eProcessing. Allows for some potential improvement for the approval process via eProcessing channel.",Development and Acquisition,,
Homeland Security,United States Citizenship and Immigration Services,,Identity Match Option (IMO) Process with DBIS Data Marts,"The Identity Match Option (IMO) is used to derive a single identity across multiple systems for each applicant or beneficiary who interacts with USCIS. The IMO aims to aid in person-centric research and analytics. 

USCIS maintains a variety of systems to track specific interactions with individuals – benefits case management, appointment scheduling, background check validation, and customer service inquiries.  Each system captures its own person-centric data attributes (e.g. SSN, A-number, Name, DOB, address, etc.) related to individuals interacting with the agency. The identity derivation process uses standard entity matching algorithms included as part of the IMO product to leverage these individual instances of person-centric data attributes to derive identities. The system is able to account for a variety of data formats and potential data quality issues in the source data. The resulting identities are linked back to the original source records, allowing analysts to see an individual’s comprehensive immigration history with the agency, perform fraud detection, and identify data quality issues requiring resolution.",Operation and Maintenance,,
Homeland Security,United States Citizenship and Immigration Services,,Person-Centric Identity Services A-Number Management Model,"The vision of Person-Centric Identity Services (PCIS) is to be the authoritative source of trusted biographical and biometric information that provides real-time, two-way visibility between services into an individual's comprehensive immigration history and status. The A-Number Management model ingests person-centric datasets from various source systems for model training and evaluation purposes. The dataset includes biographic information (name, date of birth, Alien #, Social Security #, passport #, etc.) as well as biographic information (fingerprint IDs, eye color, hair color, height, weight, etc.) for model training and matching purposes. 

The A-Number Management identifies which records from within our identity database best match search criteria. The model uses machine learning to ensure that search results presented to authorized external partners for external integrations and servicing have a high degree of confidence with the search criteria so that trust in the PCIS entity resolution remains high.

The A-Number Management model plays a critical role in the entity resolution and surfacing of a person and all their associated records. The machine learning models are more capable of resolving ""fuzzy"" matches, and deal with the reality of different data quality.",Operation and Maintenance,,
Homeland Security,United States Citizenship and Immigration Services,,Person-Centric Identity Services Deduplication Model,"The vision of Person-Centric Identity Services (PCIS) is to be the authoritative source of trusted biographical and biometric information that provides real-time, two-way visibility between services into an individual's comprehensive immigration history and status. The de-duplication model, ingests person-centric datasets from various source systems for model training and evaluation purposes. Our dataset includes biographic information (name, date of birth, Alien #, Social Security #, passport #, etc.) as well as biographic information (fingerprint IDs, eye color, hair color, height, weight, etc.) for model training and matching purposes. 

Critical to the success of PCIS is the entity resolution/deduplication of individual records from various systems of records to create a complete picture of a person. Using machine learning, it is able to identify which case management records belong to the same unique individual with a high degree of confidence. This allows PCIS to pull together a full immigration history for an individual without time-consuming research across multiple disparate systems.

The Deduplication model plays a critical role in the entity resolution and surfacing of a person and all their associated records. The ML models are more resilient to fuzzy matches, and deals with the reality of different data fill rates more reliably.",Operation and Maintenance,,
Homeland Security,United States Citizenship and Immigration Services,,Predicted to Naturalize,"The Predicted to Naturalize model predicts when Legal Permanent Residents would be eligible to naturalize, and attempts to provide a current address. This model could potentially be used to send correspondence to USCIS customers of their resident status, and notify others of potential USCIS benefits.",Implementation ,,
Homeland Security,United States Citizenship and Immigration Services,,Sentiment Analysis - Surveys,"The Sentiment Analysis - Surveys system provides a statistical analysis of quantitative results from survey results and then uses Natural Language Processing (NLP) modeling software to assign ""sentiments"" to categories ranging from strongly positive to strongly negative. This allows survey administrators to glean valuable information from employee satisfaction surveys from both quantitative and qualitative data. This capability is currently available on demand.",Operation and Maintenance,,
Homeland Security,United States Citizenship and Immigration Services,,Topic Modeling  on Request For Evidence data sets,"Builds models that identify lists of topics and documents that are related to each topic. Topic Modeling provides methods for automatically organizing, understanding, searching, and summarizing text data. It can help with the following: discovering the hidden themes in the collection. classifying the documents into the discovered themes.",Development and Acquisition,,
Interior,BLM,,Land Use Plan Document and Data Mining and Analysis R&D,"Exploring the potential to identify patterns, rule alignment or conflicts, discovery, and mapping of geo history and/or rules. Inputs included unstructured planning documents. Outputs identify conflicts in resource management planning rules with proposed action locations requiring exclusion, restrictions, or stipluations as defined in the planning documents. ",Planned (not in production),"German, Jesse",jgerman@blm.gov
Interior,BOR,,Data Driven Sub-Seasonal Forecasting of Temperature and Precipitation ,"Reclamation has run 2, year-long prize competitions where particants developed and deployed data driven methods for sub-seasonal (2-6 weeks into future) prediction of temperature and precipitation across the western US. Particpants outperformed benchmark forecasts from NOAA. Reclamation is currently working with Scripps Institute of Oceanography to further refine, evaluate, and pilot implement the most promising methods from these two copmetitions. Improving sub-seasonal forecasts has significant potential to enhance water management outcomes.  ",Development (not in production) ,"Nowak, Kenneth",knowak@usbr.gov
Interior,BOR,,Data Driven Streamflow Forecasting,"Reclamation, along with partners from the CEATI hydropower industry group (e.g. TVA, DOE-PNNL, and others) ran a year-long  evaluation of existing 10-day streamflow foreasting technologies and a companion prize competition open to the public, also focused on 10-day streamflow forecasts. Forecasts were issued every day for a year and verified agains observed flows. Across locations and metrics, the top perfoming foreacst product was a private, AI/ML forecasting company - UpstreamTech. Several competitors from the prize competition also performed strongly; outperforming benchmark forecasts from NOAA. Reclamation is working to further evaluate the UpstreamTech forecast products and also the top performers from the prize competition.  ",Development (not in production) ,"Nowak, Kenneth",knowak@usbr.gov
Interior,BOR,,Seasonal/Temporary Wetland/Floodplain Delineation using Remote Sensing and Deep Learning,"Reclamation was interested in determining if recent advancements in machine learning, specifically convolutional neural network architecture in deep learning, can provide improved seasonal/temporary wetland/floodplain delineation (mapping) when high temporal and spatial resolution remote sensing data is available? If so, then these new mappings could inform the management of protected species and provide critical information to decision-makers during scenario analysis for operations and planning.",Completed,"King, Vanessa",vking@usbr.gov
Interior,BOR,,Improving UAS-derived photogrammetric data and analysis accuracy and confidence for high-resolution data sets using artificial intelligence and machine learning,"UAS derived photogrammetric products contain a large amount of potential information that can be less accurate than required for analysis and time consuming to analyze manually. By formulating a standard reference protocol and applying machine learning/artificial intelligence, this information will be unlocked to provide detailed analysis of Reclamation's assets for better informed decision making.",Proof-of-concept completed,"Klein, Matthew",mklein@usbr.gov
Interior,BOR,,Photogrammetric Data Set Crack Mapping Technology Search ,"This project is exploring a specific application of photogrammetric products to process analysis of crack mapping on Reclamation facilites.  This analysis is time consuming and has typically required rope access or other means to photograph and locate areas that can now be reached with drones or other devices.  By formulating a standard reference protocol and applying machine learning/AI, this information will be used to provide detailed analysis of Reclamation assets for better decision making. ",Proof-of-concept completed,"Klein, Matthew",mklein@usbr.gov
Interior,BOR,,Improved Processing and Analysis of Test and Operating Data from Rotating Machines,"This project is exploring a better method to analyze DC ramp test data from rotating machines. Previous DC ramp test analysis requires engineering expertise to recognize characteristic curves from DC ramp test plots. DC ramp tests produce a plot of voltage vs current for a ramping voltage applied to a rotating machine. By using machine learning/AI tools, such as linear regression, the ramp test plots can be analyzed by computer software, rather than manual engineering analysis, to recognize characteristic curves. The anticipated result will be faster and more reliable analysis of field-performed DC ramp testing.",Investigating/Proof of concept,"Agee, Stephen",sagee@usbr.gov
Interior,BSEE,,Sustained Casing Pressure Identification,"Well casing pressure requests are submitted to BSEE to determine whether a well platform is experiencing a sustained casing pressure (SCP) problem. SCP is usually caused by gas migration from a high-pressured subsurface formation through the leaking cement sheath in one of the well’s casing annuli, but SCP can also be caused by defects in tube connections, downhole accessories, or seals. Because SCP can lead to major safety issues, quickly identifying wells with SCP could greatly mitigate accidents on the well platforms",Planned (not in production),"Boone, Adam",adam.boone@bsee.gov
Interior,BSEE,,Level 1 Report Corrosion Level Classification,"Level 1 surveys obtained from BSEE report the condition of well platforms. The reports include images of well platform components, which can be used to estimate coating condition and structural condition, important factors in the overall condition of the facility. The reports are used to assess the well platforms for safety concerns. The reports are submitted to BSEE and are manually reviewed to determine whether a well platform needs additional audits. Because the manual review process is time-consuming, an automated screening system that can identify parts of the wells that exhibit excess corrosion may greatly reduce report processing time.",Planned (not in production),"Boone, Adam",adam.boone@bsee.gov
Interior,BSEE,,Well Activity Report Classification,Researching use of self-supervised deep neural networks to identify classification systems for significant well event using data from well Activity Reports,Planned (not in production),"Boone, Adam",adam.boone@bsee.gov
Interior,USGS,,"Wildlife Underpass Camera Trap Image Classification, San Diego CA","This software system takes wildlife camera trap images as inputs and outputs the probability of the image belonging to user-specified taxonomic classes based on wildlife species present in each image. (Wildlife camera traps are motion-triggered, time lapse, and other camera systems placed in the field to capture images of wildlife at the location and times where and when the cameras are placed.) The process of humans reviewing, labeling, and QA/QCing labels is labor intensive, time consuming, and costly. Developing AI systems that can perform these tasks within an acceptable level of accuracy can reduce the costs in extracting tabular data from camera-based datasets and increase the volume of data for analysis. The system supports training experiments where model hyperparameters and training dataset characteristics can be varied to find those that are more optimal for training. Training, validation, and testing datasets have human-assigned labels and are used to train and evaluate the models. Once trained, the models can be used to predict classes on unlabeled images. We use a convolutional neural network (CNN) approach based on TensorFlow and training is run on the USGS Tallgrass supercomputer designed for AI/ML workflows.",Planned (not in production),"Tracey, Jeff",jatracey@usgs.gov
Interior,USGS,,Walrus Haulout Camera Trap Image Classification,"This project extends the application of codes developed for wildlife underpass camera trap image classification. Similarly, the system takes walrus haulout camera trap images as inputs and outputs the probability of the image containing walruses and various human disturbances (boats, aircraft, etc.). We will use and further develop the previous system's capability of supporting training experiments. Training, validation, and testing datasets have human-assigned labels and are used to train and evaluate the models. Once trained, the models can be used to predict classes on unlabeled images from ongoing camera monitoring efforts. We use a convolutional neural network (CNN) approach based on TensorFlow and training is run on the USGS Tallgrass supercomputer designed for AI/ML workflows.",Planned (not in production),"Tracey, Jeff",jatracey@usgs.gov
Interior,USGS,,ARMI Amphibian Species ID from Acoustic Data,"The mission of the USGS Amphibian and Reptile Minitoring Initiative (ARMI) is to provide essential scientific information to managers to help arrest or reverse amphibian population declines. Acoustic monitoring of amphibian (anuran) vocalizations are a core technique used by ARMI researchers. Reviewing audio recordings and identifying species vocalizations captured therein is time consuming and labor intensive. For these reasons, many recordings remain unprocessed, preventing valuable data from being available for analysis. Our goal is to train convlutional neural networks (CNNs) that take audio clips that have been converted to sonograms (images) and classify the species generating the vocalizations in the recordings. Our initial prototype project will attempt to develop models that can identify audio clips containing bullfrog (*Lithobates catesbeianus*, which are native in some parts of the US and a destructive invasive species in others) vocalizations. The software will be build using the TensorFlow Python API and training will be performed on the USGS Tallgrass supercomputer.",Planned (not in production),"Tracey, Jeff",jatracey@usgs.gov
Interior,USGS,,Individual Mountain Lion ID from Camera Data,"This system will to take pairs of mountain lion (*Puma concolor*) facial images and output the probability that the images come from the same individual mountain lion. This will allow researches to passively ""mark"" individuals and support population estimation analyses. We will use a ""Siamese"" convolutional neural network architecture that has been used in other facial recognition and motion tracking applications.",Planned (not in production),"Tracey, Jeff",jatracey@usgs.gov
Interior,USGS,,Walrus Object Detection in Drone/Satelite Imagery,"This system, once developed, will input drone imagery and output bounding boxes for individual walruses. If successful, this will allow researchers with Alaska Science Center to count the numbers of walruses in drone imagery to support population reseaarch. The system will use TensorFlow-based convolutional neural networks for object detection trained on the USGS Tallgrass supercomputer.",Planned (not in production),"Tracey, Jeff",jatracey@usgs.gov
Interior,USGS,,PRObability of Streamflow PERmanence,"The PROSPER modeling framework was developed to incorporate sparse streamflow observation data representing wet or dry stream conditions and gridded hydroclimatic explanatory data to predict the annual probability of streamflow permanence at 30-m (PROSPER Pacific Northwest) or 10-m (PROSPER Upper Missouri) resolution. The training data are point observations of wet or dry at locations in the Pacific Northwest or Upper Missouri River basin. The PROSPER models were primarily developed using the FCPGTools (Barnhart, Sando, et al., 2020), R, and USGS HPC resources (Yeti).",In production: more than 1 year,Roy Sando,tsando@usgs.gov
Interior,USGS,,Water Mission Area Drought Prediction Project,"The goal of this project is to develop a method for predicting daily hydrologic drought using machine learning models calibrated on streamflow data (response) and meteorological forcing data. Models will be built at individual gages across CONUS, then transferred to ungaged basins using a 'donor model' approach that identifies which gages are most similar to the ungaged basin and combines the models from those gages for the final prediction. Models will be developed and run on the USGS HPC systems. ",Planned (not in production),Stacey Archfield,sarch@usgs.gov
Interior,USGS,,Water Mission Area Regional Drought Early Warning System,"The goal of this project is to build and test multiple ML models for predicting and forecasting daily hydrologic drought in the Colorado River Basin (CRB). Similar to the project listed in line 9, we use gridded meteorologic forcing data and daily streamflow data in the CRB to build random forest and neural networks (long-short term memory) to determine the best approach to predicting and forecasting hydrologic drought. The project is being developed on AWS and in cooperation with CHS. We are also using the USGS HPC systems.",Planned (not in production),John Hammond,jhammond@usgs.gov
Interior,USGS,,AI system to recognize individual fish and disease,"This study focuses on the development of an AI system to recognize individual fish and their disease status from images. Success of this effort could complement or replace traditional mark-recapture methods used for estimating abundance, survival, and movement, and this could greatly reduce costs to fisheries managers. Likewise, disease detection from images could enable new approaches for assessing status and trends in fish health.",In production: less than 1 year,"Hitt, Nathaniel",nhitt@usgs.gov
Interior,USGS,,River Image SEnsing,"The River Image Sensing (RISE) project is charged with the development of a reliable camera system for integration into the operational streamgage monitoring network of the USGS Water Mission Area. In addition to capturing images and videos, the RISE system will be capable of producing time-series of surface water levels derived from still camera images using AI/ML modeling techniques.",In production: less than 1 year,"Lotspeich, Russ",rlotspei@usgs.gov
Interior,USGS,,Estimating stream flow from images in headwaters,"The goals of this project are to 1) measure how much water flows in small, ungaged stream networks using timelapse images captured by inexpensive and off-the-shelf cameras and 2) provide a web-based platform for making the images, associated climate and other related data as well as the model itself easy to access and explore. Data for training come from user-uploaded imagery and flow data (when available). Database is available for uploading and image viewing here: https://www.usgs.gov/apps/ecosheds/fpe/",In production: less than 1 year,"Letcher, Ben",bletcher@usgs.gov
Interior,USGS,,Economic valuation of fisheries in the Delaware River,"The goal is to link existing hydrological flow data (e.g., USGS stream gages) and models (e.g., USGS Process-Guided Deep Learning Models for flow and temparture) with trout population dynamic models, changes to fish catch, and the economic benefits of recreational fishing. These trout population dynamic models will be developed based on observational data, existing literature estimates, and existing models.",In production: less than 6 months,"Letcher, Ben",bletcher@usgs.gov
Interior,USGS,,Stream physical habitat characterization in the Chesapeake Bay Watershed,"The project objective is to take a large dataset of rapid habitat assessment data collected by multiple jurisdictions in the Chesapeake Bay Watershed, train a predictive model using those data, and use that model to predict stream habitat conditions for all unmeasured stream reaches in the region. The model is able to generate predictions for multiple aspects of physical habitat condition. The model directly connects to EPA's database containing the training data, enabling it to be rapidly updated when new data updates occur.",In production: less than 1 year,"Cashman, Matthew",mcashman@usgs.gov
Interior,USGS,,"Deep Learning for Automated Detection and Classification of Waterfowl, Seabirds, and other Wildlife from Digital Aerial Imagery","Our project includes two stages of AI, the first is a binary detector to automate the detection of wildlife in aerial imagery and the second is a robust classification algorithm to automate the taxonomic classification of wildlife from the binary detector. The input of the first and second stage are manually annotated polygons around targets of interest and their taxonomic classification values from family to species, respectively. We use Tallgrass to develop and train our algorithms, BlackPearl/Caldera to store our large image datasets, a hosted instance of a customized version of the Computer Vision Annotation Tool to gather manually annotated data, and a separate PostgreSQL database to store annotations and image metadata.",In production: less than 1 year,"Landolt, Kyle",klandolt@usgs.gov
Interior,USGS,,Prediction of Regolith Thickness in the Delaware River Basin,This project uses observations of the depth to bedrock reported by private well drillers in the Delaware River Basin to train a Random Forest model to map the thickness of the regolith layer. This data product will support groundwater and hydrologic modeling efforts in the basin.,In production: less than 1 year,"Goodling, Phillip",pgoodling@usgs.gov
Interior,USGS,,ML-Mondays course on applications of deep learning to image analysis,"A course in application of deep learning image segmentation, image classification, and object-in-image detection. Course includes software written in Python using Keras and Tensorflow ML libraries, software documentation, data, website, and slides. See course website https://dbuscombe-usgs.github.io/MLMONDAYS/ for more details",In production: more than 1 year,"Buscombe, Daniel",dbuscombe@contractor.usgs.gov
Interior,USGS,,Coast Train,"Coast Train is a multi-labeler ML-ready dataset of orthomosaic and satellite images of coastal, estuarine, and wetland environments and corresponding thematic label masks. The data consist of spatial and time-series, and contains 1.2 billion labelled pixels, representing over 3.6 million hectares.",In production: less than 6 months,"Buscombe, Daniel",dbuscombe@contractor.usgs.gov
Interior,USGS,,Seabird and Marine Mammal Surveys Near Potential Renewable Energy Sites Offshore Central and Southern California,"The Seabird Studies Team at the Western Ecological Research Center (WERC), with support from the Bureau of Ocean Energy Management (BOEM), completed aerial photographic surveys of the ocean off central and southern California between 2018-2021. Over 800,000 high resolution images of the ocean were collected, with the goal of extracting and counting marine birds and mammals contained within. To process this volume of images machine learning offered the best methodology, but publicly available training data did not exist for this specific purpose. Through a collaboration with Conservation Metrics, Inc. we created a labeled training dataset using Faster RCNN models via active learning and transfer learning. We then evaluated a set of candidate models trained on different label aggregation schemes, selected a final model utilizing YOLOv5 architecture, and ran the final model on the complete image dataset. Images output from the final model classified targets into seven categories: bird, dark bird, dark bird flying, light bird, fish, marine mammal, and other. We are currently reviewing the final model output for false positives and negatives to evaluate performance. Next, we will reclassify model labels to the lowest taxonomic group possible. This manual review is occurring in the USGS cloud environment (Amazon Web Services) utilizing the opensource Computer Vision Annotation Tool (CVAT). Once low taxonomic reclassification is complete, we will generate maps of species distribution and abundance to inform BOEM’s planning in advance of potential offshore wind energy development along the California coast.",Review final model output,"Adams, Josh",josh_adams@usgs.gov
Interior,USGS,,Fouling Identification Neural Network (FINN),"Our product is an end-to-end system that is used to predict and detect sensor (sonde) fouling at USGS stream gages. The system is trained using supervised learning on multiple features derived from archived stream gage data labelled by expert field technicians. The system operated in real-time on Amazon Web Services (AWS), providing predictions every 30 minutes based off of raw data collected from the USGS AQUARIUS database. The system produces values detecting the likelihood that fouling is currently present and likelihood of fouling predicted to occur in the next 24 hours. These values are displayed on a Tableau dashboard that is connected to AWS using Amazon Athena. This dashboard also displays other stream gage network monitoring information from AQUARIUS, like time since a sonde was last visited by a technician.",In production: 1 year,"Katoski, Michelle",mkatoski@usgs.gov
Interior,USGS,,Mapping river bathymetry from remotely sensed data ,"We are using high frequency satellite images from the Planetscope constellation to estimate water depth in river channels. The short time lags between images allows us to average multiple scenes collected on the same day or within a couple of days to improve accuracy. In addition to established depth retrieval methods, we developed a neural network regression approach for this purpose. The training data consist of field measurements of water depth collected as part of other USGS projects on five different rivers. The neural network regression method is implemented in MATLAB using the Deep  Learning Toolbox.",Planned (not in production),"Legleiter, Carl",cjl@usgs.gov
Interior,USGS,,Mapping benthic algae along the Buffalo National River from remotely sensed data,"This study involves using orthophotos acquired from a manned, fixed-wing aircraft and multispectral images from two different satellites to map bottom-attached (benthic) algae along the Buffalo National River in northern Arkansas. The training data for this effort consist of field observations of water depth and percent cover of benthic algae along 8-10 cross sections from two distinct reaches of the Buffalo River. These field data are used to train a bagged trees (aka random forest) classification algorithm to distinguish among four ordinal levels of algal density: none, low, medium, and high.",Planned (not in production),"Legleiter, Carl",cjl@usgs.gov
Interior,USGS,,Characterization of Sub-surface drainage (tile drains) from satellite imagery,"Without knowing how tile-drain extent (sub-surface agricultural drainage) has changed with time, it is difficult to differentiate how streamflow and water quality have changed as a result of spatial extent and characteristics of tile-drain networks.  Our method delineates tile drains in satellite imagery, providing a way to look at historical imagery and to use satellite data to maintain an up-to-date geospatial layer of tile drain extent in basins of interest.  We use panchromatic imagery that is processed using a UNet model that was trained on a library of panchromatic images on which visible tile-drain networks had been traced.  Our workflow uses a combination of python scripting that is encapsulated in a Jupyter notebook; the entire process is open source.  ",We have the final version of the Unet model.  We are working on a manscript to document the model and workflow.  ,"Williamson, Tanja",tnwillia@usgs.gov
Interior,USGS,,Waterfowl Lifehistory and Behavior Classification,"The model we developed provides a highly accurate daily classification of waterfowl behavior into 8 life history states/movement patterns using hourly GPS relocations and, optionally, remotely sensed habitat data.  This will provide waterfowl researchers and managers a tool for real-time capable rapid assessments and notification of important life history events to improve research and management outcomes and reduce project operational costs.  ",Planned (not in production),"Casazza, Michael",mike_casazza@usgs.gov
Interior,USGS,,Spot Elevation OCR from historical topo maps,The goal of this project is to create a database of summit spot elevations from the HTMC labeled for summits in CONUS.,In production: more than 1 year,"Arundel, Samantha",sarundel@usgs.gov
Interior,USGS,,TerrainFeatures detection and recognition,The objective of this project is to use DL tools to extract terrain features. ,In production: more than 1 year,"Arundel, Samantha",sarundel@usgs.gov
Interior,USGS,,The National Landcover database,"NLCD uses AI/ML to develop Landcover across all 50 states. The system includes HPC processes, cloud services, and local resources to create thematic and continuous field classifications. These classifications serve as the base for users and federal agencies across the nation to provide wildlife habitat estimates, urban runoff estimates, population growth, etc. etc.",,"Dewitz, Jon",dewitz@usgs.gov
Interior,USGS,,Artificial Intelligence for Environment & Sustainability (ARIES),"ARIES is an international research project based at the Basque Centre for Climate Change (Bilbao, Spain), to which USGS has been a long-term collaborator. ARIES uses semantics and machine reasoning to enable AI-assisted multidisciplinary, integrated modeling of coupled human-natural systems. See https://aries.integratedmodelling.org/ and https://docs.integratedmodelling.org/technote/ for full details.
ARIES is a full-stack solution for integrated modelling, supporting the production, curation, linking and deployment of scientific artifacts such as datasets, data services, modular model components and distributed computational services. Its purpose is to ensure — by design rather than just intent — that the pool of such artifacts constitutes a seamless knowledge commons, readily actionable (by humans and machines) through a full realization of the linked data paradigm augmented with semantics and powered by artificial intelligence. This design enables automation of a wide range of modeling tasks that would normally require human experts to perform.
ARIES’ underlying software stack, called k.LAB, includes client and server components that support the creation, maintenance and use of a distributed semantic web platform where scientific information can be stored, published, curated and connected. The software is licensed through the Affero General Public License (AGPL) v.3.0.",In production: more than 1 year,"Bagstad, Kenneth",kjbagstad@usgs.gov
Interior,USGS,,Global Inland Fisheries Risk Index,We applied coupled manual and machine learning methods to an expansive literature set for major global inland fisheries to explore opportunities for improving user efficiency for linking anthropogenic drivers of environmental change to direct impacts. This work informs the relative influence of threats in the development of a global inland fisheries assessment using boosted regression trees to derive a spatially-explicit risk index of stressors.,In production: more than 1 year,"Lynch, Abigail",ajlynch@usgs.gov
Interior,USGS,,Fish and Climate Change Database (FiCli),"The Fish and Climate Change Database (FiCli) is a comprehensive database of peer-reviewed literature compiled through an extensive, systematic primary literature review to identify English-language, peer-reviewed journal publications with projected and documented examples of climate change impacts on inland fishes globally. We are currently exploring options to automate certain portions of the review process to increase our efficiency in maintaining and updating the database.",In production: less than 1 year,"Lynch, Abigail",ajlynch@usgs.gov
Interior,USGS,,Evaluating fish movement in restored coastal wetlands using imaging sonar and machine learning models,"Wetland managers are restoring coastal wetland habitats in the Great Lakes, and often seek more information on when and how fish access restored habitats. Terabytes of hydroacoustic data on fish movement need to be analyzed more efficiently, so a collaboration between USGS, USFWS, and the University of Michigan is developing a machine learning model (MLM) that identifies, tracks, and quantifies fish movement. The completed model will read proprietary sonar image files, convert them to a universal file format (i.e., .mp4), place bounding boxes around individual fish detected by the model, and track them across consecutive image frames to determine bi-directional movement. The model uses training data and TensorFlow-based convolutional neural networks for object detection. Post-processing uses sonar geometry to estimate length of individual fish, and output files include labeled videos showing bounding boxes (.avi format), model metrics (.txt format), an enumeration of bi-directional fish movement (i.e., left or right) in .csv format, and individual fish length estimates (.csv format). Model output will allow USGS researchers to estimate fish habitat use and associated community metrics in restored wetland habitats. This information will support USFWS and other management agencies restoring coastal wetland habitats.",In production: less than 1 year,"Kowalski, Kurt",kkowalski@usgs.gov
Interior,USGS,,Fluvial Fish Native Distributions for the Conterminous United States using the NHDPlusV2.1 and Boosted Regression Tree (BRT) Models,"Species distribution models are developed for 271 fluvial fish species in their native ranges of the conterminous United States.  Boosted Regression Tree (BRT) models were used to develop presence/absence predictions for each of the National Hydrography Dataset Plus Version 2.1 stream segments within a species' native range.  Landscape data that describe the natural variation (e.g., slope, precip) and anthropogenic impacts (e.g., stream fragmentation) were summarized to stream segments and used as predictor variables. Native species ranges were used to geographically constrain distribution modeling efforts. R Version 4.0.2 (or newer) with ‘dismo’ and ‘labdsv’ packages are used for modeling.",In production: less than 6 months,"Wieferich, Daniel",dwieferich@usgs.gov
Interior,USGS,,Prediction of Inland Salinity in the Delaware River Basin,"Once developed, the system will input watershed characteristics (soils, land cover), land use (road salt application) and meteorological timeseries, and output predictions of specific conductance (SC) for inland stream reaches in the Delaware River Basin (DRB). The model will be trained using SC sample data from within the DRB. The resulting model will allow for predictions in ungaged locations and time periods, and allow for an evaluation of salinity exposure in these stream reaches. The model will be built using pyTorch on the USGS Tallgrass supercomputer.",Planned (not in production),"Smith, Jared",jsmith@usgs.gov
Interior,USGS,,Prediction of Salt Front Location in the Delaware River Estuary,"We are developing a machine learning model to make predictions of the 250 mg/L isochlor (salt front location) within the Delaware River Estuary. The model will be driven by river discharge into the estuary, tidal forcings, and meterological data from several points throughout the estuary. Model predictions will be compared with a process-based, hydrodynamic model, COAWST. The machine learning model is currently in development, but it will consist of a recurrent neural network architecture built using tools from pyTorch.",Planned (not in production),"Gorski, Galen",ggorski@usgs.gov
Interior,USGS,,Prediction of Water Temperature in the Delaware River Basin,We published a machine learning model to make water temperature predictions at 456 reaches in the Delaware River Basin. The recurrent graph convolutional network (RGCN) was pre-trained with predictions from a coupled process-based model that predicts stream flow and temperature (the Precipitation Runoff Modeling System with the coupled Stream Network Temperature Model or PRMS-SNTemp).,In production: more than 1 year,"Oliver, Samantha",soliver@usgs.gov
Interior,USGS,,Forecasting Water Temperature in the Delaware River Basin ,We developed a process-guided deep learning and data assimilation approach to operationally produce 7-day forecasts of daily maximum stream water temperature downstream of drinking water reservoirs in support of water management decisions. Our process-guided deep learning model was pretrained on output from an integrated stream-reservoir process-based model and used an autoregressive technique and data assimilation to ingest real-time observations of stream temperature to improve near-term forecasts. Our modeling system produced forecasts of daily maximum water temperature with an average root mean squared error (RMSE) from 1.1 to 1.4°C for 1-day-ahead and 1.4 to 1.9°C for 7-day-ahead forecasts across all sites. ,In production: less than 1 year,"Zwart, Jacob",jzwart@usgs.gov
Interior,USGS,,Prediction of Flood Flow Metrics for Minimally Altered Catchments,"Once developed, the system will input watershed characteristics (soils, land cover) and long-term meteorological data, and output predictions of flood flow metrics (magnitude, duration, frequency, volume) for stream reaches. Two models will be trained using gage data from regions surrounding the Delaware River Basin and the Colorado River Basin. The resulting models will allow for estimating flood flow metrics in ungaged reaches, which can be used to inform infrastructure designs along those reaches (e.g., bridges). The current deliverable is predictions for minimally altered catchments, and future years may expend to predictions in altered catchments (e.g., those with dam regulation). The models will be built using various R packages on the USGS Tallgrass supercomputer.",Planned (not in production),"Smith, Jared",jsmith@usgs.gov
Interior,USGS,,Process-Guided Deep Learning Predictions of Lake Water Temperature,"This process-guided deep learning model predicts depth-specific lake temperatures while obeying physical laws using inputs of meteorological drivers. Training consists of two stages. In the first stage, the model is pre-trained using process-based modeling outputs. Then, lake temperature observations are used to finetune the model in a second training stage. The models are trained to simultaneously fit observations and honor conservation of energy. The models were developed using various R and Python packages on the USGS Tallgrass supercomputer. The General Lake Model (GLM version 2) software was used for process-based modeling.",In production: more than 1 year,"Read, Jordan",jread@usgs.gov
Interior,USGS,,Prediction of Lake Water Temperature using Lake Attributes,"Once developed, the system will input lake characteristics (surface area, elevation, and others to be determined) and output predictions of depth-specific lake temperatures. Training data consist of lake temperature observations, meteorological data, and lake characteristics. The models will be developed using various Python packages including PyTorch on the USGS Tallgrass supercomputer.",Planned (not in production),"McAliley, Wallace (Andy)",wmcaliley@usgs.gov
Interior,USGS,,Process-Guided Deep Learning for Dissolved Oxygen Predictions on Stream Networks,"1) this model predicts daily minimum, mean, and maximum dissolved oxygen (DO) concentrations at several stream locations in the Delaware River Basin. The inputs used are meteorological inputs (e.g., precipitation, cloud cover) and static catchment attributes (e.g., basin area). 2) the training data are DO concentrations collected by the USGS and made available via the National Water Information System (NWIS). 3) this work is being done using Python and R. The deep learning models were written via TensorFlow, the data prepartion is in R, and the modeling workflow was scripted via Snakemake.",Planned (not in production),"Sadler, Jeffrey",jsadler@usgs.gov
Interior,USGS,,Multi-task deep learning for daily streamflow and water temperature,"1) This model predicts two interdependent variables, daily average streamflow and daily average stream water temperature, together using multi-task deep learning. A multi-task scaling factor controls the relative contribution of the auxiliary variable’s error to the overall loss during training. Input data include meteorological variables such as rainfall and humidity. 2) The training data were streamflow and water temperature observations. The stream temperature data were collected by the USGS and made available via NWIS. The streamflow observations were also collected by the USGS but collated along with input drivers in the CAMELS dataset. 3) This work was done using Python. The deep learning models were written via TensorFlow and the modeling workflow was scripted via Snakemake.",Planned (not in production),"Sadler, Jeffrey",jsadler@usgs.gov
Interior,USGS,,Predicting Water Temperature Dynamics of Unmonitored Lakes With Meta‐Transfer Learning,"The approach compares the transfer of different model types from well-observed to unobserved lake systems. Process-based models, neural networks, and process-guided neural networks are trained on well observed lakes (source lakes) and then is used to make predictions in unobserved lakes (target lakes). The performance of each of those transfers is used to train a meta-model that uses lake characteristics (e.g., depth, area) to predict which source lakes will be good candidates for transfer to target lakes. The process-guided deep learning models were able to transfer better than process-based and pure machine learning approaches. ",In production: more than 1 year,"Read, Jordan",jread@usgs.gov
Interior,USGS,,Process-guided deep learning for predicting stream temperature in out-of-bound conditions,"1) This work uses meteorological drivers to predict network wide daily average stream temperature in the Delaware River Basin. 2) The training data are water temperature observations available through NWIS and collected by the UGSS. 3) The work compares the performance of two deep learning achictectures, both of which incorporate process guidance through pretraining on process-based modelling outputs.  For each architecture, we're testing the ability of the model to generalize outside the bounds of its training data in order to better understand the limitations of each modelling approach for accurately predicting stream temperature under changing climate and precipitation regimes.",Planned (not in production),"Topp, Simon",stopp@usgs.gov
Interior,USGS,,Process guidance for learning groundwater influence on stream temperature predictions,"1) This work uses meteorological drivers to predict network wide daily average stream temperature in the Delaware River Basin. 2) The training data are water temperature observations available through NWIS and collected by the UGSS. 3) The work focuses on developing a custom loss function that helps deep learning models learn to account for groundwater influence on stream temperature.  Specifically, it uses the phase lag and amplitude dampening effect of groundwater to identify reaches likely influenced by shallow and deep groundwater inputs. ",Planned (not in production),"Barclay, Janet",jbarclay@usgs.gov
Interior,USGS,,Explainable AI and interpretable machine learning,"This work focuses on developing expertise and resources for Explainable AI (XAI) within WMA PUMP Projects.  The inputs are various models developed for predicting stream temperature, discharge, dissolved oxygen, and other characteristics.  The outputs are interpretable metrics to help understand why models are making the predictions they are and what physical processes are getting captured with the model architectures.",Planned (not in production),"Topp, Simon",stopp@usgs.gov
Interior,USGS,,AI applications to mapping surface water,"The research is investigating the use of hand annotated hydrography from one region to train an artificial neural net (ANN) to identify where surface water is likely to be in other areas. The input training data includes lidar, radar, and other remotely sensed data along with modeled surface flow to inform the model. The work is using open-source tools in a high-performance computing environment. ",Planned (not in production),"Shavers, Ethan",eshavers@usgs.gov
Interior,USGS,,Where’s the Rock: Using Neural Networks to Improve Land Cover Classification,"While machine learning techniques have been increasingly applied to land cover classification problems, these techniques have not focused on separating exposed bare rock from soil covered areas. Therefore, we are using a neural network to differentiate exposed bare rock (rock) from soil cover (other). We started with a training dataset by mapping exposed rock at 20 test sites across the Sierra Nevada Mountains (California, USA) using USDA’s 0.6 m National Aerial Inventory Program (NAIP) orthoimagery. These initial sites were used to train and test the original CNN and now NASA's DELTA toolkit, which is being run on the USGS high-performance computing facilities. The goal is to generate a machine learning approach to classify bare rock in NAIP orthoimagery, starting with the Sierras, in order to provide a more accurate map of soil vs. rock-covered areas for use in landslide hazard mapping, quantifying soil carbon storage, calculating water fluxes, etc.",In production: more than 1 year,"Cerovski-Darriau, Corina",ccerovski-darriau@usgs.gov
Interior,USGS,,Data–driven prospectivity modelling of sediment–hosted Zn–Pb mineral systems and their critical raw materials,"Regional data (magnetics and their derivatives, gravity and their derivatives, black shales, terrane boundaries, LAB depths, permissive geology, paleo-latitude etc.) is loaded into Uber's H3 cube. Clastic Dominated (CD) and  Mississippi Valley Type (MVT) deposits are used to train a Weights of Evidence model and two different Gradient-Boosting Machine models. After training occured the result was a prospecticvity map for CD and MVT deposits in the three countries. ",In production: less than 1 year,Coyan Joshua,jcoyan@usgs.gov
Interior,USGS,,"Updating Real-time Earthquake Shaking, Ground Failure, and Impact products with remote sensing and ground truth observations","A breakthrough for rapid post-earthquake ground failure (GF) and loss modeling and reporting has been achieved with initial Bayesian updating of our global loss and GF models with ground-truth observations. Empirical models suffer from limited performance due to the complex, event-specific causal effects underlying the cascading processes of earthquake-triggered hazards and impacts. In contrast, satellite imagery-based impact assessments (e.g., NASA’s Damage Proxy Maps, or DPMs), while spatially accurate, lack the specificity as to what physical process caused those image changes. We present the first rapid seismic multi-hazard and damage updating framework based on variational Bayesian causal inference and remotely sensed DPMs. This machine learning framework enables accurate and high-resolution multi-hazard and damage estimates by jointly inferring shaking and secondary hazards and resulting building damage and quantifying their causal dependencies from imagery and prior loss and GF models. The underlying physical causal dependencies are modeled using a multi-layer causal Bayesian network. Initial results are impressive, showing that our framework significantly improves the GF prediction abilities. It also reveals the event-specific causal dependencies among ground shaking, GF, building damage, and other environmental factors. We expect improved PAGER products to more rapidly evolve to accurate and thus more actionable images, maps, and products. ",Planned (not in production),David Wald,wald@usgs.gov
Interior,USGS,,Using Artificial Neural Networks to Improve Earthquake Ground-Motion Models,"The ML model provides estimates of peak ground-motion from earthquakes given the location, magnitude, and local geological structure at a site of interest. The training data is a compilation of about 12,000 peak ground-motions recorded at seismic stations for moderate to large earthquakes. I constructed the ML model in Python using Keras with TensorFlow.",Planned (not in production),"Aagaard, Brad",baagaard@usgs.gov
Interior,USGS,,Leveraging Deep Learning to Improve Earthquake Monitoring,"The USGS National Earthquake Information Center monitors global earthquakes 24/7, rapidly detecting, characterizing, and publically desimating earthquake information. In order to improve the perfomance of their event characterization system, the NEIC has trained AI models to characterize earthquake source information using small portions of waveform data. These models improve autotmatic phase picking, classify phase types, and estimate source-station distances. The outcome of these models is improved automatic earthquake detections. The training dataset used in these models leverages the long standing reveiwed earthquake catalog produced by the NEIC combined with archived continous waveform recordings, many of which are USGS opperated stations. These tools have been developed primarily leveraging Python, Keras, and Tensorflow. ",In production: less than 1 year,"Yeck, William",wyeck@usgs.gov
Interior,USGS,,Using Gradient Boosting Method and Feature Selection to Reduce Aleatory Uncertainty of Earthquake Ground-Motion Models,"We develop ground-motion models for peak ground acceleration and peak ground velocity using a gradient boosting method (GBM). In total 128 GBM-based ground-motion models are developed for estimating PGA and PGV, respectively, using varying subsets of explanatory variables. We select eight GBM-based ground-motion models that have the lowest root mean squared error (rmse) for the cross-validation datasets among models with the same number of explanatory variables. The secondary variables, in order of importance, that contribute to the model accuracy are: VS30, Ztor, Ry, Rx, Rake, Zhyp, and Dip. By considering the tradeoff between the model accuracy and model complexity (number of explanatory variables), we find an optimal model to predict PGA and PGV uses four explanatory variables: M, Rjb, VS30, and Ztor. The variability decomposition results suggest that the reduction of total variability is mostly due to the reduction of inter-event variability, likely because more source parameters than site or path parameters are included as explanatory variables. ",Planned (not in production),"Cochran, Elizabeth",ecochran@usgs.gov
Interior,USGS,,Application of machine learning to  ground motion-based earthquake early warning,"We use initial observations of an earthquake on seismic stations close to an earthquake to predict what the peak ground shaking will be across a region. The initial test dataset are waveforms from the USGS-collected, large-n seismic array in an area of induced seismicity in Oklahoma. Future datasets will include seismic data from the California Seismic Integrated Network (primarily supported by USGS) and possible the Japanese Meterological Agency. Currently running Python-based codes on a desktop, plan to move to AWS or similar. ",Planned (not in production),"Cochran, Elizabeth",ecochran@usgs.gov
Interior,USGS,,A machine learning approach to developing ground motion models from simulated ground motions,"We use a machine learning approach to build a ground motion model (GMM) from a synthetic database of ground motions extracted from the Southern California CyberShake study. An artificial neural network is used to find the optimal weights that best fit the target data (without overfitting), with input parameters chosen to match that of state-of-the-art GMMs. We validate our synthetic-based GMM with empirically based GMMs derived from the globally based Next Generation Attenuation West2 data set, finding near-zero median residuals and similar amplitude and trends (with period) of total variability. Additionally, we find that the artificial neural network GMM has similar bias and variability to empirical GMMs from records of the recent Mw7.1 Ridgecrest event, which neither GMM has included in its formulation. As simulations continue to better model broadband ground motions, machine learning provides a way to utilize the vast amount of synthetically generated data and guide future parameterization of GMMs. ",In production: more than 1 year,"Withers, Kyle",kwithers@usgs.gov
Interior,USGS,,Integrating machine learning phase pickers into the Southern California Seismic Network earthquake catalog,"We evaluate the readiness of machine-learning models for automatic earthquake detection and phase picking to enhance the Southern California Seismic Network earthquake catalog, with the end-goal of using these models in routine seismic network operations. We first test a model called Generalized Phase Detection (GPD), trained on millions of manually-picked P- and S- arrival times from Southern California earthquakes and examples of noisy time series data.  Inputs are continuous seismic data time series, with 3 components (north, east, vertical), at hundreds of seismic stations located in southern California.  Outputs are arrival times of P and S seismic waves, with associated probabilities between 0 and 1, with a threshold probability applied for detection; these arrival times are fed into existing software to estimate earthquake locations, origin times, and magnitudes.  Custom software is written in Python with the model implemented in the PyTorch library.  We are also developing a cloud-native software architecture that takes real-time seismic data as input (~15 seconds at a time) and applies the GPD model within Amazon Web Services.",Planned (not in production),"Yoon, Clara",cyoon@usgs.gov
Interior,USGS,,Understanding the 2020-2021 Puerto Rico Earthquake sequence with deep learning approaches,"We enhance the earthquake catalog for the 2020-2021 southwestern Puerto Rico earthquake sequence with a variety of deep learning approaches to understand its complex fault system, triggering mechanisms, and long-lived vigorous nature of the aftershock sequence.  We use an existing deep learning model for earthquake detection and phase picking called EQTransformer, which was trained on a global data set of earthquake waveforms called STEAD, using the TensorFlow library.  We also apply deep learning methods for earthquake location (EikoNet and HypoSVI), trained on a known velocity model with a physics informed neural network using the PyTorch library, which then allows grid-free rapid seismic wave travel time calculation between any 2 locations within a 3D volume .  These machine learning methods for automatic earthquake detection, phase-picking, and location, which are all available as open-source Python codes, help increase the number of small earthquake observations and improve earthquake depth estimates, thus offering more detailed information about active faults and physical processes in this earthquake sequence.",Planned (not in production),"Yoon, Clara",cyoon@usgs.gov
Justice,Drug Enforcement Administration,,Drug Signature Program Algorithms,"DEA's Special Testing and Research 
Laboratory utilizes AI/ML techniques and 
has developed a robust statistical 
methodology including multi-variate 
statistical analysis tools to automatically 
classify the geographical region of origin 
of samples selected for DEA's Heroin and 
Cocaine signature programs. The system 
provides for detection of anomalies and 
low confidence results.",In production: more than 1 year,"Bourque, Monique",monique.bourque@usdoj.gov
Justice,Federal Bureau of Investigation,,Complaint Lead Value Probability,"Threat Intake Processing System (TIPS) 
database uses artificial intelligence (AI) 
algorithms to accurately identify, 
prioritize, and process actionable tips 
in a timely manner. The AI used in this 
case helps to triage immediate threats 
in order to help FBI field offices and 
law enforcement respond to the most 
serious threats first.  Based on the 
algorithm score, highest priority tips 
are first in the queue for human 
review.",In production: more than 1 year,"Bourque, Monique",monique.bourque@usdoj.gov
Justice,Justice Management Division,,Intelligent Records Consolidation Tool,"The Office of Records Management Policy 
uses an AI and Natural Language Processing 
(NLP) tool to assess the similarity of records 
schedules across all Department records 
schedules. The tool provides clusters of 
similar items to significantly reduce the time 
that the Records Manager spends manually 
reviewing schedules for possible 
consolidation. An AI powered dashboard 
provides recommendations for schedule 
consolidation and review, while also 
providing the Records Manager with the 
ability to review by cluster or by individual 
record. The solution's technical approach 
has applicability with other domains that 
require text similarity analysis.",In production: more than 1 year,"Bourque, Monique",monique.bourque@usdoj.gov
Justice,Tax Division,,Privileged Material Identification,"The application scans documents and 
looks for attorney/client privileged 
information. It does this based on 
keyword input by the system 
operator.",In production: less than 6 months,"Bourque, Monique",monique.bourque@usdoj.gov
Labor,,,Form Recognizer for Benefits Forms,Custom machine learning model to extract data from complex forms to tag data entries to field headers. The input is a document or scanned image of the form and the output is a JSON response with key/value pairs extracted by running the form against the custom trained model.,Operation and Maintenance,,
Labor,,,Language Translation,Language translation of published documents and website using natural language processing models.,Implementation,,
Labor,,,Audio Transcription,Transcription of speech to text for records keeping using natural language processing models.,Operation and Maintenance,,
Labor,,,Text to Speech Conversion,Text to speech (Neural) for more realistic human sounding applications using natural language processing models.,Operation and Maintenance,,
Labor,,,Claims Document Processing,To identify if physician’s note contains causal language by training custom natural language processing models.,Implementation,,
Labor,,,Website Chatbot Assistant,"The chatbot helps the end user with basic information about the program, information on who to contact, or seeking petition case status.",Implementation,,
Labor,,,Data Ingestion of Payroll Forms,Custom machine learning model to extract data from complex forms to tag data entries to field headers. The input is a document or scanned image of the form and the output is a JSON response with key/value pairs extracted by running the form against the custom trained model.,Initiation,,
Labor,,,Hololens,AI used by Inspectors to visually inspect high and unsafe areas from a safe location.,Operation and Maintenance,,
Labor,,,DOL Intranet Website Chatbot Assistant,"Conversational chatbot on DOL intranet websites to help answer common procurement questions, as well as specific contract questions.",Initiation,,
Labor,,,Official Document Validation,AI detection of mismatched addresses and garbled text in official letters sent to benefits recipients.,Implementation,,
Labor,,,Electronic Records Management,"Meeting NARA metadata standards for (permanent) federal documents by using AI to identify data within the document, and also using NLP to classify and summarize documents.",Initiation,,
Labor,,,Call Recording Analysis,Automatic analysis of recorded calls made to Benefits Advisors in the DOL Interactive Voice Repsonse (IVR) center.,Initiation,,
Labor,,,Automatic Document Processing,Automatic processing of continuation of benefits form to extract pre-defined selection boxes.,Implementation,,
Labor,,,Automatic Data Processing Workflow with Form Recognizer,Automatic processing of current complex worflow to extract required data.,Initiation,,
Labor,,,Case Recording summarization,Using an open source large language model to summarize publicly available case recording documents which are void of personal identifiable information (PII) or any other sensitive information. This is not hosted in the DOL technical environment and is reviewed by human note takers.,Development and Acquisition,,
Labor,,,OEWS Occupation Autocoder,The input is state submitted response files that include occupation title and sometimes job description of the surveyed units. The autocoder reads the job title and assigns up to two 6-digit Standard Occupational Classification (SOC) codes along with their probabilities as recommendations for human coders. Codes above a certain threshold are appended to the submitted response file and sent back to states to assist them with their SOC code assignment.,Operation and Maintenance,,
Labor,,,Scanner Data Product Classification,"BLS receives bulk data from some corporations related to the cost of goods they sell and services they provide. Consumer Price Index (CPI) staff have hand-coded a segment of the items in these data into Entry Level Item (ELI) codes. To accept and make use of these bulk data transfers at scale, BLS has begun to use machine learning to label data with ELI codes. The machine learning model takes as input word frequency counts from item descriptions. Logistic regression is then used to estimate the probability of each item being classified in each ELI category based on the word frequency categorizations. The highest probability category is selected for inclusion in the data. Any selected classifications that do not meet a certain probability threshold are flagged for human review.",Operation and Maintenance,,
Labor,,,Expenditure Classification Autocoder,Custom machine learning model to assign a reported expense description from Consumer Expenditure Diary Survey respondents to expense classification categories known as item codes.,Development and Acquisition,,
SSA,SSA,"Office of Analytics, Review, and Oversight ",Modernized Development Worksheet (MDW),This process uses AI to review textual data that is part of claim development tasks so it can be categorized into workload topics using natural language processing to facilitate faster technician review.,,,
SSA,SSA,"Office of Analytics, Review, and Oversight ",Anomalous iClaim Predictive Model,The anomalous iClaim predictive model is a machine learning model that identifies high-risk iClaims. These claims are then sent to Operations for further review before additional action is taken to adjudicate the claims. ,,,
SSA,SSA,"Office of Analytics, Review, and Oversight ",Pre-Effectuation Review / Targeted Denial Review Models,These review models use machine learning to identify cases with greatest likelihood of disability eligibility determination error and refer them for quality review checks.  ,,,
SSA,SSA,"Office of Analytics, Review, and Oversight ",Rep Payee Misuse Model,This model uses machine learning to estimate the probability of resource misuse by representative payees and flag the cases for a technician to examine.,,,
SSA,SSA,"Office of Analytics, Review, and Oversight ",CDR Model,This model uses machine learning techniques to identify disability cases with the greatest likelihood of medical improvement and flag them for a coninuing disability review.,,,
SSA,SSA,"Office of Analytics, Review, and Oversight ",SSI Redetermination Model,This model uses machine learning to identify supplemental security income cases with highest expected overpayments due to changes in financial eligibility and flag them for technician review.  ,,,
SSA,SSA,"Office of Analytics, Review, and Oversight ",Medicare Part D Subsidy Model,This model uses machine learning to identify cases most likely to have incorrect Medicare Part D subsidies and flag them for technician review.,,,
SSA,SSA,"Office of Analytics, Review, and Oversight ",PATH Model,This model uses machine learning to identify cases likely to receive an allowance at the hearing level and refer them to administrative law judges or senior adjudicators for prioritized review.,,,
SSA,SSA,"Office of Analytics, Review, and Oversight; Office of Hearing Operations, Office of Disability Systems",Insight,"Insight is decision support software used by hearings and appeals-level Disability Program adjudicators to help maximize the quality, speed, and consistency of their decision making.  Insight analyzes the free text of disability decisions and other case data to offer adjudicators real-time alerts on potential quality issues and case-specific reference information within a web application.  It also offers adjudicators a series of interactive tools to help streamline their work.  Adjudicators can leverage these features to speed their work and fix issues before the case moves forward (e.g. to another reviewing employee or to the claimant).  Insight�s features are powered by several natural language processing and artificial intelligence packages and techniques.",,,
SSA,SSA,"Office of Disability Determinations, Office of Disability Information Systems",Intelligent Medical Language Analysis Generation (IMAGEN),"IMAGEN is an IT Modernization Disability Analytics & Disability Decision Support (ADDS) Product that will provide new tools and services to visualize, search and more easily identify relevant clinical content in medical records.  These tools and services will improve the efficiency and consistency of disability determinations and decisions and provide a foundation for machine-based decisional guidance. IMAGEN will transform text to data and enable disability adjudicators to leverage various machine learning technologies like Natural Language Processing (NLP) and predictive analytics and will support other high-priority agency initiatives such as fraud prevention and detection.",,,
SSA,SSA,"Office of Disability Information Systems,  Office of Hearing Operations, Office of Appellate Operations",Duplicate Identification Process (DIP),"Duplicate Identification Process's (DIP's) objective is to help the user to�identify and flag�and mark duplicates�more efficiently, reducing the amount�of time spent to review�cases for�hearings.�DIP uses artificial�intelligence software in the form of image recognition technology to accurately�identify duplicates consistent with SSA�policy.?",,,
SSA,SSA,"Office of Disability Information Systems,  Office of Hearing Operations, Office of Appellate Operations",Handwriting recognition from forms,AI performs OCR against handwritten entries on specific standard forms submitted by clients. This use case is in support of an Robtic Process Automation effort as well as a standalone use.,,,
SSA,SSA,Office of Retirement of Disability Programs ,Quick Disability Determinations Process,"The Quick Disability Determinations (QDD) process uses a computer-based predictive model to screen initial applications to identify cases where a favorable disability determination is highly likely and medical evidence is readily available. The Agency bases the QDD model�s predictive scores on historical data from application forms completed by millions of applicants. By identifying QDD cases early in the process, the Social Security Administration can prioritize this workload and expedite case processing.  The Agency routinely refines the QDD model to reflect the characteristics of the recent applicant population and optimize its ability to identify strong candidates for expedited processing. ",,,
SSA,SSA,Office of Systems,Mobile Wage Reporting (MOBWR) ,Mobile Wage Reporting uses AI to extract text/data from scanned images/documents represeting pay stubs or payroll information to enable faster processing.,,,
State,,A,Federal Procurement Data System (FPDS) Auto-Populate Bot,"A/LM collaborated with A/OPE to develop a bot to automate the data entry in the Federal Procurement Data System (FPDS), reducing the burden on post’s procurement staff and driving improved compliance on DATA Act reporting. This bot is now used to update ~300 FPDS awards per week.  A/LM also partnered with WHA to develop a bot to automate closeout reminders for federal assistance grants nearing the end of the period of performance and begin developing bots to automate receiving report validation and customer service inbox monitoring.",,,
State,,A,Product Service Code Automation ML Model,"A/LM developed a machine learning model to scan unstructured, user entered procurement data such as Requisition Title and Line Descriptions to automatically detect the commodity and services types being purchased for enhanced procurement categorization.",,,
State,,A,Tailored Integration Logistics Management System (ILMS) User Analytics,A/LM plans to use available ILMS transactional data and planned transactions to develop tailored user experiences and analytics to meet the specifics needs of the user at that moment. By mining real system actions and clicks we can extract more meaningful information about our users to simplify their interactions with the system and reduce time to complete their daily actions.,,,
State,,A,Supply Chain Fraud and Risk Models,"A/LM plans to expand current risk analytics through development of AI/ML models for detecting anomalous activity within the Integrated Logistics Management System (ILMS) that could be potential fraud or malfeasance. The models will expand upon existing risk models and focus on key supply chain functions such as: Asset Management, Procure-to-Pay, and Fleet Management.",,,
State,,A,Tailored Integration Logistics Management System (ILMS) Automated User Support Bot,ILMS developed and deployed an automated support desk assistant using ServiceNow Virtual Agent to simplify support desk interactions for ILMS customers and to deflect easily resolved issues from higher cost support desk agents.,,,
State,,A,Within Grade Increase Automation,"A Natural Language Processing (NLP) model is used in coordination with Intelligent Character Recognition (ICR) to identify and extract values from the JF-62 form for within grade increase payroll actions. Robotic Process Automation (RPA) is then used to validate the data against existing reports, then create a formatted file for approval and processing.",,,
State,,A,Verified Imagery Pilot Project,"The Bureau of Conflict and Stabilization Operations ran a pilot project to test how the use of a technology service, Sealr, could verify the delivery of foreign assistance to conflict-affected areas where neither U.S. Department of State nor our implementing partner could go.  Sealr uses blockchain encryption to secure photographs taken on smartphones from digital tampering.  It also uses artificial intelligence to detect spoofs, like taking a picture of a picture of something.  Sealr also has some image recognition capabilities.  The pilot demonstrated technology like Sealr can be used as a way to strengthen remote monitoring of foreign assistance to dangerous or otherwise inaccessible areas.",,,
State,,A,Conflict Forecasting,"CSO/AA is developing a suite of conflict and instability forecasting models that use open-source political, social, and economic datasets to predict conflict outcomes including interstate war, mass mobilization, and mass killings. The use of AI is confined to statistical models including machine learning techniques including tree-based methods, neural networks, and clustering approaches.",,,
State,,CGFS,Automatic Detection of Authentic Material,The Foreign Service Institute School of Language Studies is developing a tool for automated discovery of authentic native language texts classified for both topic and Interagency Language Roundtable (ILR) proficiency level to support foreign language curriculum and language testing kit development.,,,
State,,CSO,Automated Burning Detection,The Village Monitoring System program uses AI and machine learning to conduct daily scans of moderate resolution commercial satellite imagery to identify anomalies using the near-infrared band.,,,
State,,CSO,Automated Damage Assessments,"The Conflict Observatory program uses AI and machine learning on moderate and high-resolution commerical satellite imagery to document a variety of war crimes and other abuses in Ukraine, including automated damage assessments of a variety of buildings, including critical infrastructure, hospitals, schools, crop storage facilities.",,,
State,,CSO,ServiceNow AI-Powered Virtual Agent (Chatbot),IRM’s BMP Systems is planning to incorporate ServiceNow’s Virtual Agent into our existing applications to connect users with support and data requests. The Artificial Intelligence (AI) is provided by ServiceNow as part of their Platform as a Service (PaaS).,,,
State,,CSO,Apptio,Working Capital Fund (IRM/WCF) uses Apptio to bill bureaus for consolidated services run from the WCF. Cost models are built in Apptio so bureaus can budget for the service costs in future FYs. Apptio has the capability to extrapolate future values using several available formulas.,,,
State,,F,NLP for Foreign Assistance Appropriations Analysis,Natural language processing application for F/RA to streamline the extraction of earmarks and directives from the annual appropriations bill. Before NLP this was an entirely manual process.,,,
State,,FSI,eRecords M/L Metadata Enrichment,"The Department’s central eRecords archive leverages machine learning models to add additional metadata to assist with record discovery and review. This includes models for entity extraction, sentiment analysis, classification and identifying document types.",,,
State,,GPA,Facebook Ad Test Optimization System,GPA’s production media collection and analysis system that pulls data from half a dozen different open and commercial media clips services to give an up-to-date global picture of media coverage around the world.,,,
State,,GPA,Global Audience Segmentation Framework,A prototype system that collects and analyzes the daily media clips reports from about 70 different Embassy Public Affairs Sections.,,,
State,,GPA,Machine-Learning Assisted Measurement and Evaluation of Public Outreach,"GPA’s production system for collecting, analyzing, and summarizing the global digital content footprint of the Department.",,,
State,,GPA,GPATools and GPAIX,GPA’s production system for testing potential messages at scale across segmented foreign sub-audiences to determine effective outreach to target audiences.,,,
State,,IRM,AI Capabilities Embedded in SMART,"Models have been embedded in the backend of the SMART system on OpenNet to perform entity extraction of objects within cables, sentiment analysis of cables, keyword extraction of topics identified within cables, and historical data analysis to recommend addressees and passlines to users when composing cables.",,,
State,,IRM,Crisis Campaign Cable Analytics,Use optical character recognition and natural language processing on Department cables in order to evaluate gaps and trends in crisis training and bolster post preparedness for crisis events.,,,
State,,IRM,Fast Text Word Builder,Fast Text is an AI approach to identifying similar terms and phrases based off a root word. This support A&R’s capability to build robust search queries for data collection.,,,
State,,IRM,Behavioral Analytics for Online Surveys Test (Makor Analytics),"GEC executes a Technology Testbed to rapidly test emerging technology applications against foreign disinformation and propaganda challenges. GEC works with interagency and foreign partners to understand operational threats they’re facing and identifies technological countermeasures to apply against the operational challenge via short-duration tests of promising technologies. Makor Analytics is an AI quantitative research company that helps clients understand their audience’s perceptions and feelings helping to mitigate some of the limitations in traditional survey research. Makor Analytics’ proprietary behavioral analytics technology was developed to uncover true convictions and subtle emotions adding additional insights into traditional online survey results. The desired outcome of the pilot is a report analyzing the survey responses using behavioral analytics to provide target audience sentiment insights and subsequent recommendations. By leveraging AI behavioral analytics, the pilot aims to provide additional information beyond self-reported data that reflects sentiment analysis in the country of interest.",,,
State,,M/SS,Crisis Campaign Cable Analytics,Use optical character recognition and natural language processing on Department cables in order to evaluate gaps and trends in crisis training and bolster post preparedness for crisis events.,,,
State,,PM,NLP to pull key information from unstructured text,Use NLP to extract information such as country names and agreement dates from dozens of pages of unstructured pdf document,,,
State,,PM,K-Means clustering into tiers,Cluster countries into tiers based off data collected from open source and bureau data using k-means clustering,,,
State,,R,Optical Character Recognition – text extraction,Extract text from images using standard python libraries; inputs have been websites to collect data,,,
State,,R,Topic Modeling,Cluster text into themes based on frequency of used words in documents; has been applied to digital media articles as well as social media posts; performed using available Python libraries,,,
State,,R,forecasting,"using statistical models, projecting expected outcome into the future; this has been applied to COVID cases as well as violent events in relation to tweets",,,
State,,R,Deepfake Detector,"Deep learning model that takes in an image containing a person’s face and classifies the image as either being real (contains a real person’s face) or fake (synthetically generated face, a deepfake often created using Generative Adversarial Networks).",,,
State,,R,SentiBERTIQ,"GEC A&R uses deep contextual AI of text to identify and extract subjective information within the source material. This sentiment model was trained by fine-tuning a multilingual, BERT model leveraging word embeddings across 2.2 million labeled tweets spanning English, Spanish, Arabic, and traditional and simplified Chinese. The tool will assign a sentiment to each text document and output a CSV containing the sentiment and confidence interval for user review.",,,
State,,R,TOPIQ,"GEC A&R’s TOPIQ tool automatically classifies text into topics for analyst review and interpretation. The tool uses Latent Dirichlet Allocation (LDA), a natural language processing technique that uncovers a specified number of topics from a collection of documents, and then assigns the probability that each document belongs to a topic.",,,
State,,R,Text Similarity,GEC A&R’s Text Similarity capability identified different texts that are identical or nearly identical by calculating cosine similarity between each text. Texts are then grouped if they share high cosine similarity and then available for analysts to review further.,,,
State,,R,Image Clustering,"Uses a pretrained deep learning model to generate image embeddings, then uses hierarchical clustering to identify similar images.",,,
State,,R,Louvain Community Detection,"Takes in a social network and clusters nodes together into “communities” (i.e., similar nodes are grouped together)",,,
State,,R,Fast Text Word Builder,Fast Text is an AI approach to identifying similar terms and phrases based off a root word. This support A&R’s capability to build robust search queries for data collection.,,,
State,,R,Behavioral Analytics for Online Surveys Test (Makor Analytics),"GEC executes a Technology Testbed to rapidly test emerging technology applications against foreign disinformation and propaganda challenges. GEC works with interagency and foreign partners to understand operational threats they’re facing and identifies technological countermeasures to apply against the operational challenge via short-duration tests of promising technologies. Makor Analytics is an AI quantitative research company that helps clients understand their audience’s perceptions and feelings helping to mitigate some of the limitations in traditional survey research. Makor Analytics’ proprietary behavioral analytics technology was developed to uncover true convictions and subtle emotions adding additional insights into traditional online survey results. The desired outcome of the pilot is a report analyzing the survey responses using behavioral analytics to provide target audience sentiment insights and subsequent recommendations. By leveraging AI behavioral analytics, the pilot aims to provide additional information beyond self-reported data that reflects sentiment analysis in the country of interest.",,,
Transportation,Federal Aviation Administration,ANG,Determining Surface Winds with Machine Learning Software,Successfully demonstrated use of an AI capability to analyze camera images of a wind sock to produce highly accurate surface wind speed and direction information in remote areas that don’t have a weather observing sensor.,Planned (not in production),,
Transportation,Federal Aviation Administration,ANG,Remote Oceanic Meteorological Information Operations (ROMIO),"ROMIO is an operational demonstration to evaluate the feasibility to uplink convective weather information to aircraft operating over the ocean and remote regions. Capability converted weather satellite data, lightning and weather prediction model data into areas of thunderstorm activity and cloud top heights. AI is used to improve the accuracy of the output based on previous activity compared to ground truth data.",Planned (not in production),,
Transportation,Federal Aviation Administration,ATO,Surface Report Classifiier (SCM/Auto-Class),"SCM classifies surface incident reports by event type, such as Runway Incursion, Runway Excursion, Taxiway Incursion/Excursion and categorizes runway incursions further by severity type (Category A, B, C, D, E)",In production: more than 1 year,,
Transportation,Federal Aviation Administration,Aviation Safety (AVS),Course Deviation Identification for Multiple Airport Route Separation (MARS),"The Multiple Airport Route Separation (MARS) program is developing a safety case for reduced separation standards between Performance Based Navigation (PBN) routes in terminal airspace. These new standards may enable deconfliction of airports in high-demand metropolitan areas, including the Northeast Corridor (NEC), North Texas, and Southern California. To build necessary collision risk models for the safety case, several models are needed, including one that describes the behavior of aircraft that fail to navigate the procedure correctly. These events are very rare and difficult to identify with standard data sources. Prior work has used Machine Learning to filter incident data to identify similar events on departure procedures.",Planned (not in production),,
Transportation,Federal Aviation Administration,AVS,JASC Code classification in Safety Difficulty Reports (SDR),"AVS identified a need to derive the joint aircraft system codes (JASC) chapter codes from the narrative description within service difficulty reports (SDR), a form of safety event reporting from aircraft operators. A team of graduate students at George Mason University collaborated with AVS employees to apply Natural Language Processing (NLP) and Machine Learning to predict JASC codes. This method can be used to check SDR entries to ensure the correct codes were provided or to assign a code when one was not.",Planned (not in production),,
Transportation,Federal Aviation Administration,AVS,Regulatory Compliance Mapping Tool,"The AVS International office is required to identify means of compliance to ICAO Standards and Recommended Practices (SARPs).  Both SARPs and means of compliance evidence are text paragraphs scattered across thousands of pages of documents.  AOV identified a need to find each SARP, evaluate the text of many FAA Orders, and suggest evidence of compliance based upon the evaluation of the text.  The base dataset used by RCMT is the documents’ texts deconstructed into paragraphs.  RCMT processes all the documents’ paragraphs run through Natural Language Processing (NLP) (this process has an AI aspect) to extract the meaning (semantics) of the text.    RCMT then employs a recommender system (also using some AI technology) to take the texts augmented by the texts’ meaning to establish candidate matches between the ICAO SARPs and FAA text that provides means of compliance.",Planned (not in production),,
Transportation,Federal Aviation Administration,NextGen (ANG),Offshore Precipitation Capability (OPC),"OPC leverages data from several sources such as weather radar, lightning networks, satellite and numerical models to produce a radar-like depiction of precipitation. The algorithm then applies machine learning techniques based on years of satellite and model data to improve the accuracy of the location and intensity of the precipitation areas.",In production: more than 1 year,,
Transportation,Federal Railroad Administration,"Office of Research, Development and Technology",Automatic Track Change Detection Demonstration and Analysis,"Description: DeepCNet-based neural network to identify and classify track-related  features (e.g., track components, such as fasteners and ties) for ""change detection"" applications.
Input: Line-scan images from rail-bound inspection systems
Output: Notification of changes from status quo or between different inspections based on geolocation.",In production: more than 1 year,,
Transportation,Federal Railroad Administration,"Office of Research, Development and Technology",Crushed Aggregate Gradation Evaluation System,"Description: Deep learning computer vision algorithms aimed at analyzing aggregate particle size grading.
Input: Images of ballast cross sections
Output: Ballast fouling index",In production: less than 1 year,,
Transportation,Federal Railroad Administration,"Office of Research, Development and Technology",Development of Predictive Analytics Using Autonomous Track Geometry Measurement System (ATGMS) Data,"Description: Leveraging large volumes of these recursive track geometry measurements to develop and implement automated machine-learning-based processes for analyzing, predicting, and reporting track locations of concern, including those with significant rates of degradation.
Input: Track geometry measurements and exceptions
Output: Inspection report that includes the trending of track geometry measures and time to failure (i.e., maintenance and safety limits).",In production: more than 1 year,,
Transportation,Federal Transportation Administration,ATO,Automated Delay detection using voice processing,"In order to get a full accounting of delay, automated voice detection of ATC and aircraft interaction is required.  Many delay events, such as vectoring, are not currently reported/detected/accounted for and voice detection would enable automated detection.",In production: less than 1 year,,
Transportation,National Highway Traffic Safety Administration,NSR Human Injury Research Division,Machine Learning for Occupant Safety Research,"Description: Utilize deep learning models for predicting head kinematics directly from crash videos. The utilization of deep learning techniques enables the extraction of 3D kinematics from 2D views, offering a viable alternative for calculating head kinematics in the absence of sensors or when sensor availability is inadequate, and when high-quality sensor data is absent
Input:  Vehicle crash videos
Output: Angular velocity - injury prediction",Planned (not in production),,
Transportation,National Highway Traffic Safety Administration,NSR Human Injury Research Division,Machine Learning for Occupant Safety Research,"Description: Utilize deep learning for predicting crash parameters, Delta-V (change in velocity) and PDOF (principal direction of force), directly from real-world crash images. Delta-V and PDOF are two most important parameters affecting injury outcome. Deep learning models can help predict both Delta-V and PDOF, without the need to run WinSmash software for Delta-V computation, and without requiring estimations by crash examiners.  Moreover, with deep learning models, the Delta-V and PDOF can be obtained within milliseconds, providing rapid results for improved efficiency""
Input:  Real world crash images
Output:  Delta-V & PDOF",Planned (not in production),,
Transportation,Pipeline and Hazardous Materials Safety Administration (PHMSA,PHMSA Office of Chief Counsel (PHC),PHMSA Rule Making,"Artificial Intelligence Support for Rulemaking - Using ChatGPT to support the rulemanking processes to provide significant efficiencies, reduction of effort, or the ability to scale efforts for unusual levels of public scrutiny or interest (e.g. comments on a rulemaking).    ChatGPT will be used to provide: 
1.  Sentiment Analysis – Is the comment positive / negative or neutral towards the proposed rule.
2.  Relevance Analysis – Whether the particular comment posted is relevant to the proposed rule
3.  Synopsis of the posted comment.
4.  Cataloging of comments.
5.  Identification of duplicate comments.",Planned (not in production),,
Treasury,,,"Inventory Item 
Replenishment MLR 
Modeling POC - Phase 1","Build and evaluate a multiple linear regression model to predict to 
determine if the replenishment of an inventory item was received before 
or after the need by date to predict the likelihood that an item will be 
received on time in the future.",Planned (not in production),,
Treasury,,,"Inventory Item 
Replenishment MLR 
Modeling POC - Phase 2","Built and evaluate a multiple linear regression model to predict to 
determine if the replenishment of an inventory item would receive by the 
standard Need By time of 128 days set for all inventory items.",Planned (not in production),,
Treasury,,,Collection Chat Bot,"The Natural Language Understanding (NLU) model will be located inside 
the eGain intent engine. This NLU will take customer typed text input aka 
– Utterances.  It will map the utterance to a specific intent and return the 
appropriate knowledge article.",In production: less than six months,,
Treasury,,,Collection Voice Bot,"The NLU model will be located inside the Automated Collections IVR (ACI) 
main menu. This NLU will take customer speech input aka – Utterances.  It 
will map the utterance to a specific intent and direct the taxpayer down to 
a certain call path.",In production: less than six months,,
Treasury,,,"Evaluate Multilingual BERT 
for Software Translation 
Use Case Evaluations","Project is evaluating the cost-effectiveness of training a multi-lingual BERT 
model on IRS corpora and using the model as means to evaluate software 
translation output of IRS content. The framework is leveraging COMET, 
ROGUE, and BLEU measures. Furthermore, the product will also be 
assessed for English-Only and Spanish-Only content content classification.",In production: less than six months,,
Treasury,,,"Large Corporate 
Compliance","Large Corporate Compliance is a machine learning model for classifying 
corporate taxes.",In production: more than one year,,
Treasury,,,"Large Partnership 
Compliance","Large Partnership Compliance is a machine learning model for stratifying 
Partnership data and score risk of potential non-compliance.",In production: more than one year,,
Treasury,,,"LB&I Text Analytics 
(including Appeals Case 
Management)","Trained text extraction and tax domain-specific BERT models (called 
TaxBERT) using about 190,000 documents including Internal Revenue 
Code, Internal Revenue Manual, and PDFs from irs.gov, Revenue Rulings, 
Private Letter Rulings, Revenue Procedures, Treasury Decisions, and other 
legal tax-related documents. The extracted text was decomposed into 21 
million sentences with 1 million unique tokens. Further filtering 
refinement resulted in 11 million unique sentences and 31 thousand unique vocabulary tokens which are then used to train domain-specific  NLP models which can be used for targeted analytics.",In production: less than six months,,
Treasury,,,"Line Anomaly 
Recommender","This use case seeks to identify a workload selection model that uses two 
recommender system models to measure overall compliance risk and 
identify anomalous tax returns and line-item values. The delivered pipeline 
capabilities can supplement the core case selection model processes by 
providing additional insight to IRS LB&I reviewers through the use of 
advanced deep learning techniques for anomaly detection.",In production: less than six months,,
Treasury,,,NRP Redesign,"Deploy innovative active learning methods to provide a lower opportunity 
cost method of estimating a compliance baseline to support tax gap 
estimation, improper papyments reporting, development and validation of 
workload identfication and selection models, and inform policy analysis.  
System inputs require existing NRP data which provide an acceptable level 
of precision and quality for an acceptable level of data quality output.",In production: less than one year,,
Treasury,,,"Projected Contract Award 
Date Web App","Projected contract award dates are generated with a machine learning 
model that statistically predicts when procurement requests will become 
signed contracts. Input data includes funding information, date / time of 
year, and individual Contract Specialist workload. The model outputs 
projected contract award timeframes for specific procurement requests.  
'When will a contract be signed?' is a key question for the IRS and 
generally for the federal government. This tool gives insight about when 
each request is likely to turn into a contract. The tool provides a technique 
other federal agencies can implement, potentially affecting $600 billion in 
government contracts. Weblink: https://www.irs.gov/newsroom/irs-
announces-use-of-projected-contract-award-date-web-app-that-predicts-
when-contracts-will-be-signed",In production: less than one year,,
Treasury,,,SBSE Issue Recommender,"Developed an AI-based recommender for detecting potential non-
compliance issues which makes training returns selection more efficient and scalable, which has been applied to the process for selecting training  returns and field work. ",In production: less than 6 months,,
USAID,USAID,"USAID/Bureau for Development, Democracy, and Innovation (DDI)",Media Early Warning System (MEWS),To detect narratives and trends in social media alterations of images and video in order to find and counteract malign narratives,Initiation,"Vachhani, Ankit",AI@usaid.gov
USAID,USAID,"USAID/Bureau for Development, Democracy, and Innovation (DDI)",Gender differentiated credit scoring,"University of California, Berkeley, is building a machine learning model to conduct gender differentiated credit scoring for customers of Rappicard in Mexico. They will compare this ML model to Rappi's ""status quo"" model to determine whether a gender differentiated model leads to greater access to credit for women.",Initiation,"Vachhani, Ankit",AI@usaid.gov
USAID,USAID,"USAID/Bureau for Development, Democracy, and Innovation (DDI)",Machine Learning for Peace,"Objective 1 under the Illuminating New Solutions and Programmatic Innovations for Resilient Spaces’ (INSPIRES). Includes program activities
and website - https://web.sas.upenn.edu/mlp-devlab/",Development and Acquisition,"Vachhani, Ankit",AI@usaid.gov
USAID,USAID,"USAID/Bureau for Development, Democracy, and Innovation (DDI)",Long-term impacts of land-use/land-cover dynamics on surface water quality in Botswana’s reservoirs using satellite data and artificial intelligence methods: Case study of the Botswana’s Limpopo River Basin (1984-2019),"For water supply, semi-arid Botswana relies on the reservoirs within the Botswana’s LRB. Reservoirs are particularly susceptible to the negative impacts of land-use and land-cover (LULC) activities and runoff because of their complex dynamics, relatively longer water residence times, and their role as an integrating sink for pollutants from their drainage basins. Despite these interrelationships and significance in regional and global economic stability, land and water (L-W) are often treated in “silos”. To understand the complex L-W nexus within the LRB, this study will use data-driven artificial intelligence for quantitative determination of the relationships between LULC change, together with socioeconomic development indicators and climate change, and their impacts on water quality and availability within the basin, both for 1984-2019 and to predict future scenarios (2020-2050). To advance data acquisition for LULC analysis and climate change, the study utilizes optical Earth-observation and meteorological satellite data. To provide near real-time and cost-effective approach for continuous monitoring of reservoir water quality within the basin, the study will develop empirical models for water quality estimation and water quality index mapping using 35-years of in-situ water quality measurements and water spectral observations using drone-borne spectrometer and optical satellite imagery through regression modeling and geospatial methods.",Development and Acquisition,"Vachhani, Ankit",AI@usaid.gov
USAID,USAID,"USAID/Bureau for Development, Democracy, and Innovation (DDI)",Morogoro youth empowerment through establishment of social innovation (YEESI) lab for problem-centered training in machine vision,"The project proposes to establish a social innovation lab for a machine vision program that will be used by youth in the Morogoro region of Tanzania. There are young people in the area who have studied information technologies and allied sciences, and while most of them can write computer programs, they cannot solve machine vision problems. This project aims to increase awareness among the youth of Morogoro and nearby regions to address machine vision problems in agriculture. Machine vision is a new and understudied practice in Tanzania; hence, this project will contribute to efforts in the creation of scientific societies that address the most pressing problems faced by more than 80% of Tanzania’s population who engage in farming. The main agricultural problems can be classified into five categories, as explained below: (1) Disease Detection and Classification: The project will develop experts who will solve problems in disease identification using machine vision for most of the diseases in crops and livestock, which are misdiagnosed by farmers. (2) Weed Classification: The project will develop algorithms that accurately identify weeds and contribute to the growing scientific database for automatic weed detection. (3) Pest Detection and Classification: Appropriate tools using machine vision for Integrated Pest Management (IPM) are needed in Tanzania, as IPM has been hindered due to a lack of extension officers to train farmers on mitigation and identification of pests in agriculture. (4) Crop Seedlings Stand Count and Yield Estimation: Use of machine vision and drones instead of scouting manually to estimate stand counts would provide appropriate mitigation strategies for replanting that would be beneficial to commercial farmers. Also of importance are algorithms to sort and estimate yield by counting the fruits and to estimate the amount of other agricultural products. (5) Crop Vigor Estimation: Most farmers apply inputs evenly across the farm because they cannot predetermine crop vigor. Accurate estimation of crop health would help farmers to mitigate the problems earlier and improve crop performance and avoid failure. Algorithms to determine crop vigor developed in this project will contribute to the improvement of the methods to estimate crop performance earlier.",Development and Acquisition,"Vachhani, Ankit",AI@usaid.gov
USAID,USAID,"USAID/Bureau for Development, Democracy, and Innovation (DDI)",Project Vikela,Use AI to detect illegal rhino horn in airplane luggage X-Ray scanners,Operation and Maintenance,"Vachhani, Ankit",AI@usaid.gov
USAID,USAID,USAID/Bureau for Global Health (GH),Using ML for predicting treatment interruption among PLHIV in Nigeria,"Using data from USAID funded Strengthening Integrated Delivery of HIV/AIDS Services (SIDHAS) project in Nigeria we trained and tested an algorithm that can be used for predicting the probability that someone newly initiated on ART will interrupt treatment. The algorithm has been successfully integrated into the Lafiya Management Information System (LAMIS), the individual-level client level electronic medical record system. Each week the outputs, for each new patient is shared with staff at the health facilities and those at high risk are provided with more intensive follow up support to reduce the risk of treatment interruption. We also conducted a qualitative assessment among to health care workers at the facilities to determine their perception of ML and determine what additional support are required for institutionalizing ML into their routine work.   ",Development and Acquisition,"Vachhani, Ankit",AI@usaid.gov
USAID,USAID,USAID/Bureau for Global Health (GH),Breakthrough RESEARCH’s Social Media Listening,"Social media listening draws on machine learning to synthesize and organize the vast quantities of data shared over social media platforms. Breakthrough RESEARCH carried out social listening on 12,301 social media posts in Nigeria to explore how gender-related online conversations manifest themselves and whether they have changed in the last five years. Using Crimson Hexagon’s machine learning algorithm, “Brightview,” publicly available social media content originating in the countries of interest was scraped by the algorithm, for posts relevant to RH/FP and youth. The resulting social media posts were then classified by topic, using language detected in the content. This provided a dataset categorizing conversations into overarching topics, allowing analyses to uncover key trends in topic specific conversation volume, insights about misinformation, attitudes and social norms, and more. The machine learning algorithm was able to identify relevant social media content. The 12,301 social media posts were qualitatively assessed and categorized, allowing researchers to monitor and track social media conversations far more expansively than allowed by research methods more traditionally used in public health and SBC programs.",Operation and Maintenance,"Vachhani, Ankit",AI@usaid.gov
USAID,USAID,USAID/Bureau for Global Health (GH),Serbia: AI predictions for the utilization of hospital beds ,"AI technology was used to predict bed occupancy at hospitals with MoH data from 2019, with an overall median error by department around 20%. This was a proof-of-concept model developed at the request of the Institute of Public Health (IPH) Batut to understand how AI can work and the value add. CHISU was asked to subsequently focus on a different use case (waiting list optimization for scheduled imaging diagnostics services, specifically CT and MRI), which is considered higher priority to demonstrate the implementation of the national AI strategy and the effect of AI in data use for decision making by the government, and will be addressed in the 2023-4.",Implementation,"Vachhani, Ankit",AI@usaid.gov
USAID,USAID,USAID/Bureau for Global Health (GH),Mali: AI predictions for the optimization of the allocation of the distribution of COVID-19 vaccines  ,AI technology was used to develop a pandemic preparedness AI model to support allocation of COVID-19 vaccines based on a multi-tiered strategy for target populations: 1) hotspots for COVID-19 positive cases and 2) pregnant/breastfeeding women using DHIS2 data. This was a proof-of-concept model.,Implementation,"Vachhani, Ankit",AI@usaid.gov
USAID,USAID,USAID/Bureau for Global Health (GH),Indonesia: AI predictions for improving forecasts for TB drugs,AI technology will be used to develop a forecasting AI model for TB sensitive drugs to inform more accurate annual quantification exercises for the MoH linked to their national data integration platform SatuSehat,Initiation,"Vachhani, Ankit",AI@usaid.gov
USAID,USAID,USAID/Bureau for Latin America and the Caribbean,NASA SERVIR - Bias Correcting Historical GEOGloWS ECMWF Streamflow Service (GESS) data using Machine Learning (ML) Techniques,"GEOGloWS ECMWF Streamflow Service (GESS) helps to organize the international community engaged in the hydrologic sciences, observations, and their application to forecasting and provides a forum for government-to-government collaboration, and engagement with the academic and private sectors to achieve the delivery of actionable water information. Since the formal creation of the initiative in 2017, the most significant element of GEOGloWS has been the application of Earth Observations (EO) to create a system that forecasts flow on every river of the world while also providing a 40-year simulated historical flow.

This application uses Long Short Term Memory (LSTM) Model with the time series of discharge data to bias correct the globally available GESS discharge information locally.",Development and Acquisition,"Vachhani, Ankit",AI@usaid.gov
USAID,USAID,USAID/Bureau for Latin America and the Caribbean,"NASA SERVIR - Using artificial intelligence to forecast harmful algae blooms in Lake Atitlán, Guatemala","This application uses machine learning with Earth observations and weather-modeled data to forecast daily algal blooms in Lake Atitlán, Guatemala. The forecasting system is being used by Lake Authorities, such as the  Authority for Sustainable Management of the Lake Atitlan Basin and its surroundings (AMSCLAE),  to inform their Harmful Algal Blooms Alert System. This work is also supported by National Geographic and Microsoft through their Artificial Intelligence (AI) for Innovation grants. ",Implementation,"Vachhani, Ankit",AI@usaid.gov
USAID,USAID,USAID/Bureau for Latin America and the Caribbean,NASA SERVIR - Mapping urban vulnerability using AI techniques,"This activity will improve urban vulnerability assessment in key population centers, particularly by co-creating replicable methods to use satellite imagery to map informal settlements. ",Initiation,"Vachhani, Ankit",AI@usaid.gov
VA,,,Artificial Intelligence physical therapy app,This app is a physical therapy support tool.  It is a data source agnostic tool which takes input from a variety of wearable sensors and then analyzes the data to give feedback to the physical therapist in an explainable format. ,,,
VA,,,Artificial intelligence coach in cardiac surgery,"The artificial intelligence coach in cardiac surgery infers misalignment in team members’ mental models during complex healthcare task execution. Of interest are safety-critical domains (e.g., aviation, healthcare), where lack of shared mental models can lead to preventable errors and harm. Identifying model misalignment provides a building block for enabling computer-assisted interventions to improve teamwork and augment human cognition in the operating room.",,,
VA,,,AI Cure,AICURE is a phone app that monitors adherence to orally prescribed medications during clinical or pharmaceutical sponsor  drug studies.,,,
VA,,,Acute kidney injury (AKI),"This project, a collaboration with Google DeepMind, focuses on detecting acute kidney injury (AKI), ranging from minor loss of kidney function to complete kidney failure. The artificial intelligence can also detect AKI that may be the result of another illness.",,,
VA,,,Assessing lung function in health and disease,Health professionals can use this artificial intelligence to determine predictors of normal and abnormal lung function and sleep parameters.,,,
VA,,,Automated eye movement analysis and diagnostic prediction of neurological disease,"Artificial intelligence  recursively analyzes previously collected data to both improve the quality and accuracy of automated algorithms, as well as to screen for markers of neurological disease (e.g. traumatic brain injury, Parkinson's, stroke, etc).",,,
VA,,,Automatic speech transcription engines to aid scoring neuropsychological tests.,Automated speech transcription engines analyze the cognitive decline of older VA patients. Digitally recorded speech responses are transcribed using multiple artificial intelligence-based speech-to-text engines. The transcriptions are fused together to reduce or obviate the need for manual transcription of patient speech in order to score the neuropsychological tests.,,,
VA,,,CuraPatient,"CuraPatient is a remote tool that allows patients to better manage their conditions without having to see a provider.  Driven by artificial intelligence, it allows patients to create a profile to track their health, enroll in programs, manage insurance, and schedule appointments.",,,
VA,,,Digital command center,The Digital Command Center seeks to consolidate all data in a medical center and apply predictive prescriptive analytics to allow leaders to better optimize hospital performance.  ,,,
VA,,,Disentangling dementia patterns using artificial intelligence on brain imaging and electrophysiological data,This collaborative effort focuses on developing a deep learning framework to predict the various patterns of dementia seen on MRI and EEG and explore the use of these imaging modalities as biomarkers for various dementias and epilepsy disorders.  The VA is performing retrospective chart review to achieve this.,,,
VA,,,Machine learning (ML) for enhanced diagnostic error detection and ML classification of protein electrophoresis text,"Researchers are performing chart review to collect true/false positive annotations and construct a vector embedding of patient records, followed by similarity-based retrieval of unlabeled records ""near"" the labeled ones (semi-supervised approach). The aim is to use machine learning as a filter, after the rules-based retrieval, to improve specificity. Embedding inputs will be selected high-value structured data pertinent to stroke risk and possibly selected prior text notes.",,,
VA,,,Behavidence,Behavidence is a mental health tracking app. Veterans download the app onto their phone and it compares their phone usage to that of a digital phenotype that represents people with confirmed diagnosis of mental health conditions. ,,,
VA,,,Machine learning tools to predict outcomes of hospitalized VA patients,"This is an IRB-approved study which aims to examine machine learning approaches to predict health outcomes of VA patients.  It will focus on the prediction of Alzheimer's disease, rehospitalization, and Chlostridioides difficile infection.",,,
VA,,,Nediser reports QA,"Nediser is a continuously trained artificial intelligence “radiology resident” that assists radiologists in confirming the X-ray properties in their radiology reports.  Nediser can select normal templates, detect hardware, evaluate patella alignment and leg length and angle discrepancy, and measure Cobb angles.",,,
VA,,,Precision medicine PTSD and suicidality diagnostic and predictive tool,"This model interprets various real time inputs in a diagnostic and predictive capacity in order to forewarn episodes of PTSD and suicidality, support early and accurate diagnosis of the same, and gain a better understanding of the short and long term effects of stress, especially in extreme situations, as it relates to the onset of PTSD.",,,
VA,,,Prediction of Veterans' Suicidal Ideation following Transition from Military Service,Machine learning is used to identify predictors of veterans' suicidal ideation. The relevant data come from a web-based survey of veterans’ experiences within three months of separation and every six months after for the first three years after leaving military service.,,,
VA,,,PredictMod,PredictMod uses artificial intelligence to determine if predictions can be made about diabetes based on the gut microbiome.,,,
VA,,,Predictor profiles of OUD and overdose,Machine learning prediction models evaluate the interactions of known and novel risk factors for opioid use disorder (OUD) and overdose in Post-9/11 Veterans. Several machine learning classification-tree modeling approaches are used to develop predictor profiles of OUD and overdose. ,,,
VA,,,Provider directory data accuracy and system of record alignment,"AI is used to add value as a transactor for intelligent identity resolution and linking.  AI also has a domain cache function that can be used for both Clinical Decision Support and for intelligent state reconstruction over time and real-time discrepancy detection.  As a synchronizer, AI can perform intelligent propagation and semi-automated discrepancy resolution.  AI adapters can be used for inference via OWL and logic programming.  Lastly, AI has long term storage (“black box flight recorder”) for virtually limitless machine learning and BI applications.",,,
VA,,,Seizure detection from EEG and video,Machine learning algorithms use EEG and video data from a VHA epilepsy monitoring unit in order to automatically identify seizures without human intervention.,,,
VA,,,SoKat Suicidial Ideation Detection Engine,The SoKat Suicide Ideation Engine (SSIE) uses natural language processing (NLP) to improve identification of Veteran suicide ideation (SI) from survey data collected by the Office of Mental Health (OMH) Veteran Crisis Line (VCL) support team (VSignals).,,,
VA,,,Using machine learning to predict perfusionists’ critical decision-making during cardiac surgery,"A machine learning approach is used to build predictive models of perfusionists’ decision-making during critical situations that occur in the cardiopulmonary bypass phase of cardiac surgery. Results may inform future development of computerized clinical decision support tools to be embedded into the operating room, improving patient safety and surgical outcomes.",,,
VA,,,Gait signatures in patients with peripheral artery disease,Machine learning is used to improve treatment of functional problems in patients with peripheral artery disease (PAD). Previously collected biomechanics data is used to identify representative gait signatures of PAD to 1) determine the gait signatures of patients with PAD and 2) the ability of limb acceleration measurements to identify and model the meaningful biomechanics measures from PAD data.,,,
VA,,,Medication Safety (MedSafe) Clinical Decision Support (CDS),"Using VA electronic clinical data, the Medication Safety (MedSafe) Clinical Decision Support (CDS) system analyzes current clinical management for diabetes, hypertension, and chronic kidney disease, and makes patient-specific, evidence-based recommendations to primary care providers.  The system uses knowledge bases that encode clinical practice guideline recommendations and an automated execution engine to examine multiple comorbidities, laboratory test results, medications, and history of adverse drug events in evaluating patient clinical status and generating patient-specific recommendations",,,
VA,,,"Prediction of health outcomes, including suicide death, opioid overdose, and decompensated outcomes of chronic diseases.","Using electronic health records (EHR) (both structured and unstructured data) as  inputs, this tool outputs deep phenotypes and predictions of health outcomes including suicide death, opioid overdose, and decompensated outcomes of chronic diseases.",,,
VA,,,VA-DoE Suicide Exemplar Project,The VA-DoE Suicide Exemplar project is currently utilizing artificial intelligence to improve VA's ability to identify Veterans at risk for suicide through three closely related projects that all involve collaborations with the Department of Energy.,,,
VA,,,Machine learning models to predict disease progression among veterans with hepatitis C virus,A machine learning model is used to predict disease progression among veterans with hepatitis C virus.,,,
VA,,,Prediction of biologic response to thiopurines,"Using CPRS and CDW data, artificial intelligence is used to predict biologic response to thiopurines among Veterans with irritable bowel disease.",,,
VA,,,Predicting hospitalization and corticosteroid use as a surrogate for IBD flares ,"This work examines data from 20,368 Veterans Health Administration (VHA) patients with an irritable bowel disease (IBD) diagnosis between 2002 and 2009. Longitudinal labs and associated predictors were used in random forest models to predict hospitalizations and steroid usage as a surrogate for IBD Flares.",,,
VA,,,Predicting corticosteroid free endoscopic remission with Vedolizumab in ulcerative colitis,This work uses random forest modeling on a cohort of 594 patients with Vedolizumab to predict the outcome of corticosteroid-free biologic remission at week 52 on the testing cohort. Models were constructed using baseline data or data through week 6 of VDZ therapy.,,,
VA,,,Use of machine learning to predict surgery in Crohn’s disease,"Machine learning analyzes patient demographics, medication use, and longitudinal laboratory values collected between 2001 and 2015 from adult patients in the Veterans Integrated Service Networks (VISN) 10 cohort. The data was used for analysis in prediction of Crohn’s disease and to model future surgical outcomes within 1 year.",,,
VA,,,Reinforcement learning evaluation of treatment policies for patients with hepatitis C virus,A machine learning model is used to predict disease progression among veterans with hepatitis C virus.,,,
VA,,,Predicting hepatocellular carcinoma in patients with hepatitis C,This prognostic study used data on patients with hepatitis C virus (HCV)-related cirrhosis in the national Veterans Health Administration who had at least 3 years of follow-up after the diagnosis of cirrhosis. The data was used to examine whether deep learning recurrent neural network (RNN) models that use raw longitudinal data extracted directly from electronic health records outperform conventional regression models in predicting the risk of developing hepatocellular carcinoma (HCC).,,,
VA,,,Computer-aided detection and classification of colorectal polyps,This study is investigating the use of artificial intelligence models for improving clinical management of colorectal polyps. The models receive video frames from colonoscopy video streams and analyze them in real time in order to (1) detect whether a polyp is in the frame and (2) predict the polyp's malignant potential.,,,
VA,,,GI Genius (Medtronic),The Medtronic GI Genius aids in detection of colon polyps through artificial intelligence.,,,
VA,,,Extraction of family medical history from patient records,This pilot project uses TIU documentation on African American Veterans aged 45-50 to extract family medical history data and identify Veterans who are are at risk of prostate cancer but have not undergone prostate cancer screening.,,,
VA,,,VA /IRB approved research study for finding colon polyps,This IRB approved research study uses  a randomized trial for finding colon polyps with artifical intelligence.,,,
VA,,,Interpretation/triage of eye images,"Artificial intelligence supports triage of eye patients cared for through telehealth, interprets eye images, and assesses health risks based on retina photos. The goal is to improve diagnosis of a variety of conditions, including glaucoma, macular degeneration, and diabetic retinopathy.",,,
VA,,,Screening for esophageal adenocarcinoma,National VHA administrative data is used to adapt tools that use electronic health records to predict the risk for esophageal adenocarcinoma.,,,
VA,,,Social determinants of health extractor,"AI is used with clinical notes to identify social determinants of health (SDOH) information. The extracted SDOH variables can be used during associated health related analysis to determine, among other factors, whether SDOH can be a contributor to disease risks or healthcare inequality.",,,
